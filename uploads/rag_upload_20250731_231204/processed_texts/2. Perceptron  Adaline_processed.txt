2. Perceptron & Adaline COMP3314 Machine Learning COMP 3314 2 Outline In this chapter we will implement two of the first published machine learning algorithms for classification Perceptron Adaptive Linear Neurons (Adaline) This will lay the groundwork for using more powerful classifiers with the scikit-learn library We will Build an intuition for machine learning algorithms Use pandas, NumPy, and Matplotlib to read in, process, and visualize data Implement linear classification algorithms in Python COMP 3314 3 Outline Neuron Artificial Neuron History Definition Perceptron Perceptron Learning Rule Linearly Separable Implementation Adaptive Linear Neuron (Adaline) Implementation Feature Scaling Stochastic Gradient Descent / Mini-Batch Learning COMP 3314 4 Neuron Neurons are interconnected nerve cells in the brain that are involved in the processing and transmitting of chemical and electrical signals COMP 3314 5 Artificial Neuron - History McCulloch and Pitts described the first artificial neuron in 1943 (aka MCP neuron) as a simple logic gate with binary outputs Signals arrive at the dendrites, are integrated into the cell body, and, if the accumulated signal exceeds a certain threshold, an output signal is generated at the axon A few years later, in 1958, Rosenblatt published the first concept of the perceptron learning rule based on the MCP neuron Automatically learn the optimal weight coefficients that are then multiplied with the input features COMP 3314 6 Artificial Neuron - Definition Consider a binary classification task where we refer to our two classes as 1 (positive class) and -1 (negative class) We can define a decision function where is a threshold and z is the net input of the input values x and the corresponding weight vector w COMP 3314 7 Artificial Neuron - Definition For simplicity, we can bring the threshold to the left side of the equation and define and where w = - and x = 1 0 0 w is called the bias unit 0 COMP 3314 8 Artificial Neuron - Definition COMP 3314 9 Perceptron Learning Rule Rosenblatt's perceptron rule can be summarized by the following steps a. Initialize the weights b. For each training sample x(i) i. Compute the output value , i.e. the class label predicted by unit step function i. Update the weights The value of w is calculated as follows j Where is the learning rate, y(i) is the true class label of the ith training sample, and (i) is the predicted class label COMP 3314 10 Weight Update All weights in the weight vector are updated simultaneously, which means that we don't recompute the (i) before all of the weights w are j updated I.e., for a two-dimensional dataset, we would write the update as COMP 3314 11 Weight Update In the two scenarios where the perceptron predicts the class label correctly, the weights remain unchanged However, in the case of a wrong prediction, the weights are being pushed towards the direction of the positive or negative target class COMP 3314 12 Weight Update Note that the weight update is proportional the value of x (i) j COMP 3314 13 Linearly Separable It was shown that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small If the two classes can't be separated by a linear decision boundary, we can set a maximum number of passes over the training dataset (epochs) and/or a threshold for the number of tolerated misclassifications The perceptron would never stop updating the weights otherwise COMP 3314 14 Linearly Separable vs. Not Linearly Separable COMP 3314 15 Perceptron COMP 3314 16 Iris Dataset - Loading Code is here COMP 3314 17 Iris Dataset - Preprocessing and Plotting COMP 3314 18 Perceptron - Implementation COMP 3314 19 Perceptron - Training COMP 3314 20 Perceptron - Plotting Decision Region COMP 3314 21 Outline Neuron Artificial Neuron History Definition Perceptron Perceptron Learning Rule Linearly Separable Implementation Adaptive Linear Neuron (Adaline) Implementation Feature Scaling Stochastic Gradient Descent / Mini-Batch Learning COMP 3314 22 Adaptive Linear Neuron ADAptive LInear NEuron: Adaline Improvement on Perceptron algorithm Published in 1960 by Bernard Widrow and Ted Hoff In Adaline the weights are updated based on a linear activation function While the linear activation function is used for learning the weights, we still use a threshold function to make the final prediction COMP 3314 23 Adaptive Linear Neuron COMP 3314 24 Objective Function One of the key ingredients of supervised ML algorithms is the objective function that is to be optimized E.g., cost function that we want to minimize In Adaline the cost function J is defined as the sum of squared errors (SSE) between the calculated and true class label It can be shown that J is differentiable and convex It will be easy to minimize (using e.g. gradient descent) COMP 3314 25 Gradient Descent A generic optimization algorithm Capable of finding optimal solutions to a wide range of problems Main idea Tweak parameters iteratively in order to minimize a cost function Measures the local gradient of the error function with regard to the parameter vector and goes in the direction of descending gradient Once the gradient is zero, it has reached a minimum COMP 3314 26 Weight Update Update the weights by taking a step in the opposite direction of the gradient J(w) of our cost function J(w) The weight change w is defined as the negative gradient multiplied by the learning rate COMP 3314 27 COMP 3314 28 Weight Update To compute the gradient of the cost function, we compute the partial derivative of the cost function with respect to each weight w j The update of weight w can then be written as j Batch gradient descent Note that the weight update is calculated based on all samples in the training set (instead of updating the weights incrementally after each sample) COMP 3314 29 Learn from the Author The LMS algorithm and ADALINE. Part I - The LMS algorithm Bernard Widrow and Ted Hoff published a paper introducing Adaline in 1959 Bernard Widrow has a youtube channel He has a video talking about Adaline :-) COMP 3314 30 Adaline - Implementation Code is here COMP 3314 31 COMP 3314 32 Learning Rate Important hyperparameter of Gradient Descent If too small Requires excessive many iterations to converge Takes a long time If too large Might jump across the valley and end up on the other side Possibly even higher up than before Algorithm may diverge, with larger and larger values COMP 3314 33 COMP 3314 34 Gradient Descent Pitfalls J(w) w COMP 3314 35 Feature Scaling Many ML algorithms require feature scaling for w 2 optimal performance E.g., gradient descent converges more quickly if our data follows a standard distribution w 1 Standardization A feature scaling method w After standardization, feature have 2 a mean value of 0 a standard deviation of 1 w 1 COMP 3314 36 Standardization For instance, to standardize the jth feature, we can simply subtract the sample mean from every training sample and divide it by its standard j deviation j Here, x is a vector consisting of the jth feature values of all training j samples n This standardization technique is applied to each feature j in our dataset COMP 3314 37 COMP 3314 38 Stochastic Gradient Descent Imagine we have a very large dataset with millions of data points Not uncommon in ML applications Running batch gradient descent can be computationally costly in such scenarios since we need to reevaluate the whole training dataset each time we take one step Stochastic Gradient Descent is a popular alternative Instead of updating the weights based on the sum of the accumulated errors over all samples x(i) we update all the weights incremental for each training sample COMP 3314 39 Stochastic Gradient Descent Typically reaches convergences faster because of the more frequent weight updates Can escape shallow local minima more readily if we are working with nonlinear cost functions It is important to present it training data in a random order In addition shuffle the training set for every epoch to prevent cycles Another advantage of stochastic gradient descent is that we can use it for online learning In online learning, our model is trained on the fly as new training data arrives COMP 3314 40 w 2 w 1 COMP 3314 41 Mini-Batch Learning A compromise between batch gradient descent and stochastic gradient descent Apply batch gradient descent to smaller subsets of the training data E.g., 32 samples at a time The advantage over batch gradient descent is that convergence is reached faster via mini-batches because of the more frequent weight updates Furthermore, mini-batch learning allows us to replace the for loop over the training samples in stochastic gradient descent with vectorized operations COMP 3314 42 COMP 3314 43 COMP 3314 44 COMP 3314 45 w 2 w 1 COMP 3314 46 Summary Gained a good understanding of the basic concepts of linear classifiers for supervised learning Implemented Perceptron Adaline Efficient training via a vectorized implementation of gradient descent and online learning via stochastic gradient descent COMP 3314 47 References Most materials in this chapter are based on Book Code COMP 3314 48 References Some materials in this chapter are based on Book Code COMP 3314 49 Exercise 1 Suppose the features in your training set have very different scales What can you do about it? Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model? Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough? Suppose you use Batch Gradient Descent and you plot the validation error at every epoch If you notice that the validation error consistently goes up, what is likely going on? How can you fix this? Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?