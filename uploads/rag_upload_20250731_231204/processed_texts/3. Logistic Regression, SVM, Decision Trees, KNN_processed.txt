3. Logistic Regression, SVM, Decision Trees, KNN COMP3314 Machine Learning COMP 3314 2 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 3 Scikit-learn Library In the previous chapter we implement the perceptron rule in Python ourselves We will now use the scikit-learn library and train a perceptron model similar to the one we implemented in the previous chapter This will serve as an intro to the scikit-learn library COMP 3314 4 Code - PerceptronSkLearn.ipynb Available here on CoLab COMP 3314 5 Iris Dataset Conveniently, the Iris dataset is already available via scikit-learn We will only use two features (petal length and petal width) from the Iris dataset for visualization purposes Assign all flower samples to the feature matrix X and the corresponding class labels of the flower species to the vector y COMP 3314 6 Training and Testing To evaluate how well a trained model performs on unseen data, we will further split the dataset into separate training and test datasets Note that the train_test_split function shuffles the training sets internally and performs stratification before splitting Otherwise, all class 0 and class 1 samples would have ended up in the training set, and the test set would consist of 45 samples from class 2 COMP 3314 7 Stratification We took advantage of the built-in support for stratification In this context, stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset COMP 3314 8 Multiclass Classification n (number of classes) > 2 Some classification algorithms naturally permit n > 2 Others are by nature binary algorithms OvA or One-versus-Rest (OvR) Train one classifier per class, where the particular class is treated as the positive class Samples from all other classes are considered negative classes If we were to classify a new data sample, we would use our n classifiers, and assign the class label with the highest confidence to the particular sample COMP 3314 9 Feature Scaling Recall that many machine learning and optimization algorithms also require feature scaling for optimal performance Here, we will standardize the features using the StandardScaler class from scikit-learn's preprocessing module COMP 3314 10 Training We can now train a perceptron model Most algorithms in scikit-learn already support multiclass classification by default via the One-versus-Rest (OvR) method, which allows us to feed the three flower classes to the perceptron all at once COMP 3314 11 Scikit-Learn Help The Perceptron, as well as other scikit-learn functions and classes, often have additional parameters that we omit for clarity You can read more about those parameters using the help function COMP 3314 12 Scikit-Learn Help The Perceptron, as well as other scikit-learn functions and classes, often have additional parameters that we omit for clarity You can read more about those parameters using the help function COMP 3314 13 Testing Having trained a model in scikit-learn, we can make predictions via the predict method on the test data COMP 3314 14 Testing - Accuracy The scikit-learn library also implements a large variety of different performance metrics that are available via the metrics module We can calculate the classification accuracy as follows Alternatively, each classifier in scikit-learn has a score method COMP 3314 15 Decision Regions Plotting Finally, we can use our plot_decision_regions function to plot the decision regions of our newly trained perceptron model Visualize how well it separates the different flower samples However, let's add a small modification to highlight the samples from the test dataset via small circles COMP 3314 16 Decision Regions Plotting COMP 3314 17 Decision Regions Plotting COMP 3314 18 Perceptron The three flower classes cannot be perfectly separated by a linear decision boundary Recall that the perceptron algorithm never converges on datasets that aren't perfectly linearly separable In the following sections, we will look at more powerful linear classifiers that converge to a cost minimum even if the classes are not perfectly linearly separable COMP 3314 19 Logistic Regression - Intuition Widely used algorithm for binary classification For classification, not regression The logit function is defined as the logarithm of odds, i.e., It takes as input values in the range 0 to 1 and transforms them to values over the entire real-number range COMP 3314 20 Logit Function COMP 3314 21 Inverse of Logit (Sigmoid) Function COMP 3314 22 Logistic Regression - Intuition The output of the logit function can be used to express a linear relationship between feature values and the log-odds p ( y = 1| x ) is the conditional probability that a particular sample belongs to class 1 given its features x COMP 3314 23 Adaline vs. Logistic Regression COMP 3314 24 Output of Sigmoid Function The output of the sigmoid function is interpreted as the probability of a particular sample belonging to class 1 (z) = P ( y = 1| x ; w ), given its features x parameterized by the weights w E.g, let ( z ) = 0.8 for a particular flower sample. I.e., the probability that this sample is an Iris-versicolor flower is 80 % The probability that this flower is an Iris-setosa flower can be calculated as P ( y = 0 | x ; w ) = 1 P ( y = 1| x ; w ) = 0.2 The predicted probability can then simply be converted into a binary outcome via a threshold function COMP 3314 25 Learning w Recall that we defined the sum-squared-error cost function as follows We minimized this function in order to learn the weights w for our Adaline classification model For logistic regression, we define the likelihood L that we want to maximize as COMP 3314 26 Learning w In practice, it is easier to maximize the (natural) log of this equation, which is called the log-likelihood function Let's rewrite the log-likelihood as a cost function that can be minimized using gradient descent COMP 3314 27 Learning w Let us take a look at the cost that we calculate for one single training sample We can see that the first term is zero if y = 0, and the second term is zero if y = 1 COMP 3314 28 Plotting our new J COMP 3314 29 From Adaline to Logistic Regression If we were to implement logistic regression ourselves, we could simply substitute the cost function J in our Adaline implementation from the previous chapter COMP 3314 30 Code - LogisticRegression.ipynb Available here on CoLab COMP 3314 31 COMP 3314 32 COMP 3314 33 Logistic Regression with scikit-learn Scikit-learn's implementation of logistic regression also supports multi-class settings off the shelf (OvR by default) COMP 3314 34 Prediction The probability that training examples belong to a certain class can be computed using the predict_proba method For example, we can predict the probabilities of the first three samples in the test set as follows Notice that for each row the columns sum all up to one COMP 3314 35 Prediction We can get the predicted class labels by identifying the largest column in each row, for example, using NumPy's argmax function The returned class indices correspond to Iris-virginica, Iris-setosa, and Iris-setosa This is just a manual approach to calling the predict method directly COMP 3314 36 COMP 3314 37 Overfitting and Underfitting A model may perform well on training data but does not generalize well to unseen data (test data) Two main categories of things that can go wrong Overfitting (high variance) Could be caused by having too many parameters that lead to a model that is too complex Underfitting (high bias) Model is not complex enough to capture the pattern in the training data well COMP 3314 38 COMP 3314 39 Regularization One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization Useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting Requires feature scaling such as standardization Need to ensure that all our features are on comparable scales COMP 3314 40 L2 Regularization The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values A common form of regularization is so-called L2 regularization (sometimes also called L2 shrinkage or weight decay), which can be written as follows is the so-called regularization parameter COMP 3314 41 Regularization - Updated Cost Function The cost function for logistic regression can be regularized by adding a simple regularization term, which will shrink the weights during model training Via the regularization parameter , we can then control how well we fit the training data while keeping the weights small By increasing the value of , we increase the regularization strength COMP 3314 42 C The parameter C that is implemented for the LogisticRegression class in scikit-learn is directly related to the regularization parameter , which is its inverse Decreasing the value of the inverse regularization parameter C means that we are increasing the regularization strength COMP 3314 43 Regularization Lets plot the L2-regularization path for the two weight coefficients We fit ten logistic regression models with different values for C For the purposes of illustration, consider only class 1 COMP 3314 44 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 45 Support Vector Machine (SVM) Powerful and widely used learning algorithm Can be considered an extension of the perceptron Perceptron: Minimized misclassification errors SVM: Maximize margin The margin is the distance between the separating hyperplane (decision boundary) and the training samples that are closest to this hyperplane, which are the so-called support vectors Original idea based on paper from 1963 by Vladimir Vapnik Extended by Vladimir Vapnik in 1992 and 1995 SVMs are used to solve various real-world problems Text categorization, classification of images, handwritten character recognition, Protein classification, ... COMP 3314 46 Which Hyperplane ? Sample from positive class Sample from negative class X 2 Potential decision boundary X 1 COMP 3314 47 SVM: Maximize Margin Sample from positive class Sample from negative class X 2 Decision boundary X 1 COMP 3314 48 Large Margin Classification Fitting the widest possible street is called large margin classification Notice that adding more training instances off the street will not affect the decision boundary at all It is fully determined (or supported) by samples located on the edge of the street These instances are called the support vectors COMP 3314 49 SVM: Decision Rule Sample from positive class X Sample from negative class 2 Use scalar projection: w x c Decision Rule: wTx + w 0 then positive class 0 x w X 1 COMP 3314 50 SVM: More Constraints Sample from positive class x pos X Sample from negative class x 2 neg wTx + w 1 pos 0 wTx + w -1 neg 0 y(i) (wTx + w ) 1 pos 0 y(i) (wTx + w ) 1 x neg 0 w y(i) (wTx + w ) -1 0 0 X 1 COMP 3314 51 SVM: More Constraints Sample from positive class x pos X Sample from negative class x 2 neg Constraint from previous slide: y(i) (wTx + w ) -1 0 0 Additional constraint: y(i) (wTx + w ) -1 = 0 0 at the gutter i.e. y(i) (wTx + w ) -1 = 0 + 0 y(i) (wTx + w ) -1 = 0 - 0 X 1 COMP 3314 52 SVM: Margin X 2 x - x What is the width of the street, + - x + i.e., the margin? x - X 1 COMP 3314 53 SVM: Margin X 2 We can take the scalar projection with the unit vector w / ||w|| Width of the street: x - x w (x - x )T w / ||w|| + - + - X 1 COMP 3314 54 SVM: Margin Width of the street: (x - x )T w / ||w|| X + - 2 = (wTx - wTx ) / ||w|| + - = (1-w + 1+w ) / ||w|| 0 0 = 2 / ||w|| x - x w + - Recall that we had the constraints: y(i) (wTx + w ) -1 = 0 + 0 y(i) (wTx + w ) -1 = 0 - 0 i.e., wTx = 1 - w + 0 X 1 wTx = -1 - w - 0 COMP 3314 55 SVM: Optimization The objective function of the SVM is the maximization of the margin 2 / ||w|| Equivalently we can minimize the reciprocal term ||w|| COMP 3314 56 SVM: Optimization For mathematical convenience we solve the following arg min w subject to the constraints This can be minimized efficiently by quadratic programming More details can be found here The Nature of Statistical Learning Theory Vladimir Vapnik, 2000 A Tutorial on Support Vector Machines for Pattern Recognition Data Mining and Knowledge Discovery, 1998 It can be shown that the search space is convex The optimization will not get stuck in a local minima COMP 3314 57 Scaling SVMs are sensitive to the feature scales COMP 3314 58 Hard vs. Soft If we strictly impose that all instances must be off the street and on the right side, this is called hard margin classification There are two main issues with hard margin classification First, it only works if the data is linearly separable Second, it is sensitive to outliers COMP 3314 59 Hard vs. Soft To avoid these issues, use a more flexible model The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations I.e., instances that end up in the middle of the street or even on the wrong side This is called soft margin classification. COMP 3314 60 Slack Variables A slack variable can be introduced Positive (or zero) Allows for convergence in the presence of misclassifications For nonlinearly separable data Soft margin COMP 3314 61 If Data is not Linearly Separable Introduce Penalty How to penalize? arg min Idea: ||w||2 + C (# mistakes) w Not all mistakes are equally bad. Use margin to penalize the mistakes. COMP 3314 62 If Data is not Linearly Separable Introduce Penalty Introduce slack variable (i) If point x(i) is on the wrong side then get penalty (i) (i) arg min Idea v2: (j) w, (i) Under the following constraints y(i) (wTx(i) + w ) 1- (i) 0 COMP 3314 63 Slack Penalty C New optimization problem: arg min w, (i) Via the variable C, we can control the penalty for misclassification Large values of C correspond to large error penalties, whereas we are less strict about misclassification errors if we choose smaller values for C This concept is related to regularization Decreasing the value of C increases the bias and lowers the variance of the model COMP 3314 64 Hinge Loss Function The preceding formulation of the SVM cost function was in the so called QP form An equivalent formulation uses the hinge loss function max( 0, 1 - y(i) ( wTx(i) + w ) ) 0 The hinge loss function returns 0 if the point is correctly classified If the point is on the wrong side of the margin, the function's value is proportional to the distance from the margin COMP 3314 65 SVM Natural Form SVM in the natural form: arg min w,w 0 Hinge Loss vs. 0\1 Loss y t l a n e p z = y( wTx + w ) 0 1 0 COMP 3314 66 Code - SVM.ipynb Available here on CoLab COMP 3314 67 COMP 3314 68 Logistic Regression vs. SVM Linear logistic regression and linear SVMs often yield similar results Logistic regression tries to maximize the conditional likelihoods of the training data, which makes it more prone to outliers than SVMs, which mostly care about the points that are closest to the decision boundary (support vectors) On the other hand, logistic regression has the advantage that it is a simpler model and can be implemented more easily Furthermore, logistic regression models can be easily updated, which is attractive when working with streaming data COMP 3314 69 Alternative Scikit-learn Implementations We used the LIBLINEAR library (via scikit-learn library's Perceptron and LogisticRegression classes) in the previous section A highly optimized C/C++ library developed at NTU Similarly, the SVC makes use of LIBSVM, which is an equivalent C/C++ library specialized for SVMs The above libraries are very fast, however, if your dataset cannot fit into memory you may use an alternative implementation available in SGDClassifier Supports online learning via the partial_fit method COMP 3314 70 Kernel SVM SVM can be kernelized to solve nonlinear classification problems Lets create a small data set of linearly inseparable data COMP 3314 71 Kernel SVM - Mapping Function Basic idea Deal with linearly inseparable data by creating nonlinear combinations of the original features by projecting them onto a higher-dimensional space where it becomes linearly separable Example We can transform a 2D dataset onto a new 3D feature space where the classes become separable via the following projection COMP 3314 72 Kernel SVM One problem with this mapping approach is that the construction of the new features is computationally very expensive, especially if we are dealing with high-dimensional data This is where the so-called kernel trick comes into play In practice all we need is to replace the dot product In order to save the expensive step of calculating the dot product between two points explicitly, we define a so-called kernel function On of the most widely used kernels is the Gaussian kernel (aka radial basis function) where is a free parameter COMP 3314 73 COMP 3314 74 Gamma The gamma parameter can be understood as a cut-off parameter for the Gaussian If we increase the value for gamma, we increase the influence or reach of the training samples, which leads to a tighter and bumpier decision boundary To get a better intuition for gamma, let us apply an RBF kernel SVM to our Iris flower dataset COMP 3314 75 COMP 3314 76 COMP 3314 77 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 78 Decision Tree Learning Decision tree classifiers are attractive models if we care about interpretability This model breaks down our data by making a decision based on asking a series of questions Example of decision tree COMP 3314 79 Decision Tree Learning Model learns to ask a series of questions E.g.: Is sepal width 2.8 cm? Prediction is based on answers to questions What questions to ask? Split data on feature that results in largest Information Gain (IG) Iterative repeat splitting until leaves are pure I.e., samples all belong to same class This can result in a very deep tree with many nodes, which can easily lead to overfitting Thus, we typically want to prune the tree by setting a limit for the maximal depth of the tree COMP 3314 80 Decision Tree Learning - Intuition Training data: Color Diameter Label Green 3 Apple Yellow 3 Apple Red 1 Grape Red 1 Grape Yellow 3 Lemon features COMP 3314 81 Decision Tree Learning - Intuition Color Diam Label Green 3 Apple Yellow 3 Apple Is diameter >= 3 ? Red 1 Grape Color Diam Label Red 1 Grape Green 3 Apple mixed Yellow 3 Lemon F T Yellow 3 Apple Yellow 3 Lemon Color Diam Label unmixed Is color == Yellow ? Red 1 Grape Red 1 Grape Predict grape 100% F T Color Diam Label Color Diam Label Yellow 3 Apple Green 3 Apple Yellow 3 Lemon Predict apple 50% Predict apple 100% Predict lemon 50% COMP 3314 82 Which questions to ask and when? Quantify how much a question helps to unmix the labels 1. Quantify the amount of uncertainty at a single node E.g. Gini Impurity 2. How much does a question reduce the uncertainty We aim to maximize Information Gain COMP 3314 83 1. Gini Impurity Chance of being incorrect if you randomly assign a label to an example in the same set p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 84 1. Gini Impurity Color Diam Label Green 3 Apple Gini Impurity: Yellow 3 Apple Is diameter >= 3 ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Color Diam Label Gini Impurity: Green 3 Apple I G = 0 R R e e d d 1 1 G G r r a a p p e e Yellow 3 Apple I G = 0.44 Yellow 3 Lemon p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 85 1. Gini Impurity Color Diam Label Green 3 Apple Gini Impurity: Yellow 3 Apple Is color == green ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Color Diam Label Gini Impurity: Yellow 3 Apple I = 0 Green 3 Apple G Red 1 Grape I = 0.625 G Red 1 Grape Yellow 3 Lemon p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 86 1. Gini Impurity Color Diam Label Green 3 Apple Gini Impurity: Yellow 3 Apple Is color == yellow ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Green 3 Apple Color Diam Label Gini Impurity: I = 0.44 Red 1 Grape Yellow 3 Apple I = 0.5 G Yellow 3 Lemon G Red 1 Grape p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 87 Which questions to ask and when? Quantify how much a question helps to unmix the labels 1. Quantify the amount of uncertainty at a single node E.g. Gini Impurity 2. How much does a question reduce the uncertainty We aim to maximize Information Gain COMP 3314 88 2. Information Gain Information Gain: Color Diam Label IG = 0.376 Green 3 Apple Gini Impurity: Yellow 3 Apple Is diameter >= 3 ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Color Diam Label Gini Impurity: Green 3 Apple I G = 0 R R e e d d 1 1 G G r r a a p p e e Yellow 3 Apple I G = 0.44 Yellow 3 Lemon COMP 3314 89 2. Information Gain - Task Information Gain: Color Diam Label IG = ? Green 3 Apple Gini Impurity: Yellow 3 Apple Color == green ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Gini Impurity: Gini Impurity: I = ? I = ? G G COMP 3314 90 2. Information Gain Information Gain: Color Diam Label IG = 0.176 Green 3 Apple Gini Impurity: Yellow 3 Apple Is color == yellow ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Green 3 Apple Color Diam Label Gini Impurity: I = 0.44 Red 1 Grape Yellow 3 Apple I = 0.5 G Yellow 3 Lemon G Red 1 Grape COMP 3314 91 Example Information Gain: Color Diam Label IG = 0.376 Green 3 Apple Gini Impurity: Yellow 3 Apple Is diameter >= 3 ? I = 0.64 Red 1 Grape Color Diam Label G Gini Impurity: Red 1 Grape Green 3 Apple Yellow 3 Lemon F T Yellow 3 Apple I = 0.44 G Yellow 3 Lemon Gini Impurity: Color Diam Label Information Gain: Is color == Yellow ? I = 0 Red 1 Grape G IG = 0.11 Red 1 Grape F T Color Diam Label Color Diam Label Yellow 3 Apple Green 3 Apple Yellow 3 Lemon Gini Impurity: Gini Impurity: I = 0 I = 0.5 G G COMP 3314 92 Maximizing Information Gain Split nodes at most informative features In our tree learning algorithm we optimize an objective function E.g., maximize information gain at each split f is the feature to perform the split D and D are the dataset of the parent and jth child node p j I is our impurity measure N is the total number of samples at the parent node, and p N is the number of samples in the jth child node. j COMP 3314 93 Binary Decision Tree For binary decision trees each parent node is split into two child nodes D and D left right The following impurity measures or splitting criteria are commonly used in binary decision trees Gini impurity ( I ), G Entropy ( I ), and H Classification error ( I ) E COMP 3314 94 Entropy p( i | t ) is the proportion of the samples that belong to class i for a particular node t The entropy is therefore 0 if all samples at a node belong to the same class, and the entropy is maximal if we have a uniform class distribution The entropy criterion attempts to maximize the mutual information in the tree p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 95 Entropy For example, in a binary class setting ( c = 2 ) The entropy is 0 if p ( i = 1| t ) = 1 or p ( i = 0 | t ) = 0 If the classes are distributed uniformly with p ( i = 1| t ) = 0.5 and p ( i = 0 | t ) = 0.5 , the entropy is 1 p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 96 Gini The Gini impurity can be understood as a criterion to minimize the probability of misclassification Similar to entropy, the Gini impurity is maximal if the classes are perfectly mixed, for example, in a binary class setting ( c = 2 ) p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 97 Classification Error Another impurity measure is the classification error This is a useful criterion for pruning but not recommended for growing a decision tree, since it is less sensitive to changes in the class probabilities of the nodes p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 98 Example IG E p( i | t ) is the proportion of the samples that belong to class i for a particular node t Consider the two possible splitting scenarios shown in the figure Lets calculate the information gain using the classification error I E as a splitting criterion I (D ) = 0.5 E p Case A: Case B: I (D ) = 0.25 I (D ) = 0.33 E left E left I (D ) = 0.25 I (D ) = 0 E right E right IG = 0.25 IG = 0.25 E E COMP 3314 99 Example IG G p( i | t ) is the proportion of the samples that belong to class i for a particular node t Consider the two possible splitting scenarios shown in the figure Lets calculate the information gain using the gini impurity I as a G splitting criterion I (D ) = 0.5 G p Case A: Case B: I (D ) = 0.375 I (D ) = 0.44 G left G left I (D ) = 0.375 I (D ) = 0 G right G right IG = 0.125 IG = 0.17 G G COMP 3314 100 Example IG H p( i | t ) is the proportion of the samples that belong to class i for a particular node t Consider the two possible splitting scenarios shown in the figure Lets calculate the information gain using the entropy I as a H splitting criterion I (D ) = 1 H p Case A: Case B: I (D ) = 0.81 I (D ) = 0.92 H left H left I (D ) = 0.81 I (D ) = 0 H right H right IG = 0.19 IG = 0.31 H H COMP 3314 101 Code - DecisionTrees.ipynb Available here on CoLab COMP 3314 102 COMP 3314 103 Building a Decision Tree Decision trees can build complex decision boundaries by dividing the feature space into rectangles However, we have to be careful since the deeper the decision tree, the more complex the decision boundary becomes, which can easily result in overfitting Using scikit-learn, we will now train a decision tree with a maximum depth of 4, using Gini Impurity as a criterion for impurity Although feature scaling may be desired for visualization purposes, note that feature scaling is not a requirement for decision tree algorithms COMP 3314 104 COMP 3314 105 Visualizing Decision Tree The following code will create an image of our decision tree in PNG format For this to work you may have to install sudo apt-get install graphviz pip install pydotplus COMP 3314 106 Random Forests A random forest can be considered as an ensemble of decision trees The idea behind a random forest is to average multiple (deep) decision trees that individually suffer from high variance, to build a more robust model that has a better generalization performance and is less susceptible to overfitting COMP 3314 107 Random Forests Creation 1. Draw a random bootstrap sample of size n (i.e., randomly choose n samples from the training set with replacement) 2. Grow a decision tree from the bootstrap sample. At each node: a. Randomly select d features without replacement b. Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain 3. Repeat steps 1. and 2. k times 4. Aggregate the prediction by each tree to assign the class label by majority vote COMP 3314 108 Random Forests Intuition COMP 3314 109 Bootstrap Sample Size Decreasing the size of the bootstrap samples (i.e., decreasing n) increases the diversity among the individual trees The probability that a particular training sample is included in the bootstrap sample is lower This increases the randomness of the random forest Helps to reduce the effect of overfitting However, smaller bootstrap samples typically result in a lower overall performance of the random forest, a small gap between training and test performance, but a low test performance overall Conversely, increasing the size of the bootstrap sample may increase the degree of overfitting The bootstrap samples, and consequently the individual decision trees, become more similar to each other, they learn to fit the original training dataset more closely COMP 3314 110 COMP 3314 111 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 112 KNN Algorithm The k-nearest neighbor (KNN) classifier is fairly straightforward and can be summarized by the following steps Choose the number of k and a distance metric Find the k-nearest neighbors of the sample that we want to classify Assign the class label by majority vote COMP 3314 113 KNN The main advantage of such a memory-based approach is that the classifier immediately adapts as we collect new training data However, the downside is that the computational complexity for classifying new samples grows linearly with the number of samples in the training dataset in the worst-case scenario Unless the dataset has very few dimensions (features) and the algorithm has been implemented using efficient data structures such as KD-trees Furthermore, we can't discard training samples since no training step is involved Storage space can become a challenge if we are working with large datasets COMP 3314 114 Code - KNN.ipynb Available here on CoLab COMP 3314 115 COMP 3314 116 KNN The right choice of k is crucial to find a good balance between overfitting and underfitting We also have to make sure that we choose a distance metric that is appropriate for the features in the dataset Often, a simple Euclidean distance measure is used for real- value samples, for example, the flowers in our Iris dataset, which have features measured in centimeters. However, if we are using a Euclidean distance measure, it is also important to standardize the data so that each feature contributes equally to the distance COMP 3314 117 KNN The Minkowski distance that we used in the previous code is just a generalization of the Euclidean and Manhattan distance, which can be written as follows It becomes the Euclidean distance if we set the parameter p=2 or the Manhattan distance at p=1 Many other distance metrics are available in scikit-learn and can be provided to the metric parameter COMP 3314 118 Curse of Dimensionality KNN is susceptible to overfitting due to the curse of dimensionality A phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed- size training dataset Even the closest neighbors being too far away in a high- dimensional space to give a good estimate We can use feature selection and dimensionality reduction techniques to help us avoid the curse of dimensionality COMP 3314 119 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 120 References Most materials in this chapter are based on Book Code COMP 3314 121 References Some materials in this chapter are based on Book Code COMP 3314 122 References 16. Learning: Support Vector Machines MIT 6.034 Artificial Intelligence, Fall 2010 View the complete course: http://ocw.mit.edu/6-034F10 Instructor: Patrick Winston In this lecture, we explore support vector machines in some mathematical detail. We use Lagrange multipliers to maximize the width of the street given certain constraints. If needed, we transform vectors into another space, using a kernel function. License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu COMP 3314 123 References 7 4 Soft Margin SVMs 9 46 COMP 3314 124 References CS229 Lecture notes COMP 3314 125 References Introduction to Machine Learning, Third Edition Ethem Alpaydin COMP 3314 126 References Logistic Regression: From Introductory to Advanced Concepts and Applications Scott Menard, 2010 COMP 3314 127 References Lets Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8 Hey everyone! Glad to be back! Decision Tree classifiers are intuitive, interpretable, and one of my favorite supervised learning algorithms. In this episode, Ill walk you through writing a Decision Tree classifier from scratch, in pure Python. Ill introduce concepts including Decision Tree Learning, Gini Impurity, and Information Gain. Then, well code it all up. Understanding how to accomplish this was helpful to me when I studied Machine Learning for the first time, and I hope it will prove useful to you as well. You can find the code from this video here: https://goo.gl/UdZoNr https://goo.gl/ZpWYzt Books! Hands-On Machine Learning with Scikit-Learn and TensorFlow https://goo.gl/kM0anQ Follow Josh on Twitter: https://twitter.com/random_forests Check out more Machine Learning Recipes here: https://goo.gl/KewA03 Subscribe to the Google Developers channel: http://goo.gl/mQyv5L COMP 3314 128 References Random Forest Algorithm - Random Forest Explained | Random Forest in Machine Learning | Simplilearn This Random Forest Algorithm tutorial will explain how Random Forest algorithm works in Machine Learning. By the end of this video, you will be able to understand what is Machine Learning, what is Classification problem, applications of Random Forest, why we need Random Forest, how it works with simple examples and how to implement Random Forest algorithm in Python. Below are the topics covered in this Machine Learning tutorial: 1. What is Machine Learning? 2. Applications of Random Forest 3. What is Classification? 4. Why Random Forest? 5. Random Forest and Decision Tree 6. Use case - Iris Flower Analysis Subscribe to our channel for more Machine Learning Tutorials: https://www.youtube.com/user/Simplilearn?sub_confirmation=1 You can also go through the Slides here: https://goo.gl/K8T4tW Machine Learning Articles: https://www.simplilearn.com/what-is-artificial-intelligence-and-why-ai-certification-article?utm_campaign=Random-Forest-Tutorial-eM4uJ6XGnSMutm_medium=Tutorialsutm_source=youtube To gain in-depth knowledge of Machine Learning, check our Machine Learning certification training course: https://www.simplilearn.com/big-data-and-analytics/machine-learning-certification-training-course?utm_campaign=Random-Forest-Tutorial-eM4uJ6XGnSMutm_medium=Tutorialsutm_source=youtube #MachineLearningAlgorithms #Datasciencecourse #DataScience #SimplilearnMachineLearning #MachineLearningCourse - - - - - - - - About Simplilearn Machine Learning course: A form of artificial intelligence, Machine Learning is revolutionizing the world of computing as well as all peoples digital interactions. COMP 3314 129 Exercise 1 What is the fundamental idea behind Support Vector Machines? What is a support vector? Why is it important to scale the inputs when using SVMs? Can an SVM classifier output a confidence score when it classifies an instance? What about a probability? Say youve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease (gamma)? What about C? COMP 3314 130 Exercise 2 Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus- the-rest to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach? COMP 3314 131 Exercise 3 What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances? Is a nodes Gini impurity generally lower or greater than its parents? Is it generally lower/greater, or always lower/greater? If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth? If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features? If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances? COMP 3314 132 Exercise 4 Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set Hint: the KNeighborsClassifier works quite well for this task; you just need to find good hyperparameter values (try a grid search on the weights and n_neighbors hyperparameters) COMP 3314 133 Exercise 5 Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel. Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set Finally, train your best model on this expanded training set and measure its accuracy on the test set You should observe that your model performs even better now This technique of artificially growing the training set is called data augmentation or training set expansion COMP 3314 134 Exercise 6 Tackle the Titanic dataset A great place to start is on Kaggle