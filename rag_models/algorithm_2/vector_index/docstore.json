{"docstore/metadata": {"c0c8a4f5-faaa-42e1-8a90-be209966240a": {"doc_hash": "b95f2d304e422db36d0e0b5ced3626ba70fdff7411bff8f39e0e3c6449639a48"}, "4e5a3d23-fdbe-46da-bc93-26420e7fd497": {"doc_hash": "81581aea8eb4174aa8cff463b38ca37500af00ce72f59e468672f5ab1d6d5e79"}, "09dba5a8-fbbc-4862-a3eb-f62ded418ca6": {"doc_hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf"}, "4124fd20-c144-428a-8738-868f0ef176f5": {"doc_hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796"}, "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98": {"doc_hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871"}, "31962845-0db9-4121-bb9f-207b3124a377": {"doc_hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3"}, "3580b79e-60eb-4ff8-934a-6fb88ed3f7d1": {"doc_hash": "37996f8cac2130f8c3973c21a0a71b2b1e8b41ae4121db0121f61e434653cb96", "ref_doc_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a"}, "ac4288de-4ae1-406a-843f-556d79f70eda": {"doc_hash": "47042e5cade547c80f5667b38c31dc0770790b924fdc2034e52e2b483c17634f", "ref_doc_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a"}, "4a65184b-61e3-40ea-8469-78edfce52058": {"doc_hash": "14f79a8689c86e6813270b7eb2e14cb0c05e4c983fe0654ef2aa372af47978b4", "ref_doc_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a"}, "b6b71c3a-3348-4eed-9980-80a7cc5f62b1": {"doc_hash": "18f22db0fcf50440f0c0839546e5411c50d328555718e89274c6683b2c2aa115", "ref_doc_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a"}, "7e99ba01-97b2-4d14-a087-34bac1484462": {"doc_hash": "4e413853428fc3b3679eaeec5a943cb8dc192a94d10c96ebe6df36203c4be4d4", "ref_doc_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a"}, "000e238b-cdef-4f71-afc6-089fa50b5344": {"doc_hash": "996aed87cc28f0f52788e448a9a81c053928d81d77577835c3856bfaa20f96f1", "ref_doc_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a"}, "752d5d12-a8a2-4903-b01c-58a9a497dfa9": {"doc_hash": "c81d730b6315a6c435ff2501ddfbb8ea16796fad21723536daa806f34697812f", "ref_doc_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a"}, "6ebf05b2-2d84-458e-8dec-de18db07aa85": {"doc_hash": "c19bfa424484df30a66416382df4df523ac4955d0a9f06e6b1f684c8c479f9f6", "ref_doc_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497"}, "758a31d6-2e97-40c5-9f7e-49a8982d281e": {"doc_hash": "4a2c26532877a1911e63fa9268a4513db45b166c93419a29fc112901bbb31f2f", "ref_doc_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497"}, "4bc7672c-0368-4024-aa7f-118a7f2d2863": {"doc_hash": "14479273b4b3035586693bfc9211c0dfb35e74d25eb3f1a139844a0ed138e51d", "ref_doc_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497"}, "7748a3cd-1d4f-4351-8f16-c5c00121d55e": {"doc_hash": "3d74b54b916534a1ca0e788eb130a1a5cd089f7a770860cd562c65ab805cd765", "ref_doc_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497"}, "4b7e4051-b29c-4887-910c-ea04d7e76cc9": {"doc_hash": "9f004ad9fca911528170245a5f5545a440cf785d6141bfe199eeacf9d8991227", "ref_doc_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497"}, "8b1d711b-6070-407c-8a5b-7a8c29924ea1": {"doc_hash": "a50447b8dc7ecdbbd1dd2f5b7b8e40b542fc87ae774596a53bcc23797a096f83", "ref_doc_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497"}, "5c82e40b-96d6-451a-9d56-237856160d46": {"doc_hash": "d7aef4744004c1d460bee43cd49bdce8bab1947ecfae8817bb8a2b321e1efcea", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "57df3278-0dc0-4316-a930-eb3cacbc305d": {"doc_hash": "93f0f6605b36f46e72723acaeccecaaa0992489bce7e1916d7f3d0c87c2fec65", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "2d5e5554-f33d-45d3-91ec-5d3c8d9fde21": {"doc_hash": "a4d7f689273e7c2550106bde6c3c92eff9e02ac1ada72ad63d62709d8261aa96", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "f03ce5e3-7352-46a6-91b4-f6605f93d2f2": {"doc_hash": "6ba439d11837e6e4e1da844c1ff5338c5eae3ccf7027817520842a588a3fc6af", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "d4f198b5-e43a-4b21-a880-969a86eff993": {"doc_hash": "d92d38b3161f43ebf89ef43ef13418f0e1d723ec273a1e1bfb8529ca39feccad", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "6f469c08-5923-4b97-9183-b507e2147d68": {"doc_hash": "425828ac2f922efcd3609d1a9e2058675433bc1f7bc71bda0a72ecd7fa124a49", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "cb4f7723-5bab-48a7-a8a4-bc2d144ae98f": {"doc_hash": "140781b6b9189523d59364baa050705b0eefe5c026e4610774ff7559e55d7052", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "98331acb-520d-4c88-bf33-ba12ed2d1c05": {"doc_hash": "f0da2e8d0d2e2ece43cb540e81d4f036b3190d3b066e01ad62fe75762b83d1e1", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "c733f247-b28b-4d88-872c-4448c08c915d": {"doc_hash": "e077c9ead13a01292a26ec6c38e146965d5d7563355e0fdb49c5921c1ef2da18", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "a644b85b-2c37-4a87-9732-ef9c73655f74": {"doc_hash": "16a3edb6a7792165351543878be0f7baf8496fbe14a171d7c614915710e7c6e2", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "40428c82-f883-4bc5-8b8e-d1da59b86d88": {"doc_hash": "b48d3c65319ca4a394eb0348e87ef4fb5f4a8ee8665c6cbe4b9b728b75d44641", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "0a47bc34-2be0-4c6e-bf1f-5c510fe57264": {"doc_hash": "2d662bd9fedee0c7a889828d96fb778fb35c1d0b3301f47bd4bf1abe8f954332", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "6c23c418-30ba-4b6d-944c-668c5cf67b3a": {"doc_hash": "dd6a44c243a4bfaa8623101f3a1b8ceef509b7b386b13c472e75724b3f4e8b3c", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "a8cd049f-1543-4814-b063-820f58eb10dd": {"doc_hash": "4ac7bb83e7279077619bbd481be315f0404277bd5239762794f568b93405527a", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "e38f3751-920f-426a-8cc3-d78c98dd4c6c": {"doc_hash": "9f95c8df11611acad04426138e9f16373874861af89576b835036885ff82c907", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "8a29d838-d68b-43f6-8366-91d0710aa648": {"doc_hash": "414e66a54f56b95428d882f90b70b221ea18e56381461159d510678db2658943", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "5df967b8-ef3b-42f8-8c37-3754c86d6820": {"doc_hash": "02fab0b8a059f8572c1744c82bca180a73c4aaeb8d5f03bdb88730d686b6e38a", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "84e2322f-2668-4411-ba38-df18b0bf3e3a": {"doc_hash": "8bee269efeb7a29fa6ce8a678470e5985d626427f431096a95595e3f58e989f3", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "3ae28f98-82a7-46fa-a829-ce3bffbfd8f3": {"doc_hash": "665fba9e1f0d8d34d0776c6e108012bb90d4c49d8120615078d9ea42339a32a3", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "3085e786-0751-4570-b0ca-670512804737": {"doc_hash": "d0b19d7fd8c46dc25d5788da1c651604b846ffff11da83884ac7ce91fddf6f74", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "2eb7d8ee-aef3-490e-bc0a-c46f1aced4c2": {"doc_hash": "b8fd0a6d42d33cbea017532f73eff259a877628306d0ea16583859a8f951b4e4", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "3048a5e6-d0e0-41e5-8d39-193e2047a0aa": {"doc_hash": "39b1a3b7d3a13a079b560ec4eeb86ae014463e91aa3250384de938bb4226ce20", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "bbf2f6fb-61ed-4de5-a565-37ec30a96995": {"doc_hash": "2415dcbdbae9b46d3dcf8ffab8fc3ec467e470f96aec4e3f49bd8ac8630d2add", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "f560cf60-9321-4303-9ae6-4081cd5e245b": {"doc_hash": "b299fde2434136c4d5e8a02f4c4d0c521e79954b4e08d6c2c02caf11f76a8d1c", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "8238366b-4349-4ab0-a06f-f75b88fc9f58": {"doc_hash": "d08b8fed299b193a69d0187087e815981a40675dad9fc05b5c9f48b9bc0ba061", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "69f7e520-1fcf-4a66-9cf1-d858caf4bea4": {"doc_hash": "9cf14efd38dab0e29fe6f7a88f187a1e31c67c32ccb37a058793135ecf6e4146", "ref_doc_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6"}, "e21fdec2-4b97-46b1-955b-fe51e72cbe6a": {"doc_hash": "3169819d409daa69b30403e8c0bafc837704ff8b24ce727885b328b500dd51e9", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "bfeb49b9-09a2-4016-9be8-33b2c9f93861": {"doc_hash": "5071f53eb10487498581720989bbf0071b523d0772d55f2c47898f85b56d34f6", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "a6aea7b9-1e40-4ef6-bd5e-e3d843535173": {"doc_hash": "2575a985da25c46451da29469028da526867f4745cb5ce8de7e58fca888365bd", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "5517e0a7-37ad-44ed-98f4-ff0144280a1c": {"doc_hash": "f528432bdc9bfeab41da394af4202236da6e094d7726283f43e6df253f2d06e0", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "1b289722-756e-4aca-aa0e-af027f5ee4df": {"doc_hash": "ab7089e4f1859b478a1dacfa3cfd4a743c841aa9607f86ed31009bfe5aba46c4", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "a275f3c3-b6b9-4b37-9c23-83fa65d9612e": {"doc_hash": "64977fe58c29df6272e5274ec872179cb5b6821fc997178ad081cff70a6645b0", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "25f52715-070a-4579-8a33-9edfd7aed672": {"doc_hash": "16d616f20d7162300bc09c32fea7ae59d1ce4797996bc548866c3dcfd2bd4885", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "ffbab969-e6b3-495a-8490-62113458bede": {"doc_hash": "683ee20af32b0e3822fa2f34ad524d7c1108e4bec7eec05ffde5e15b5e2080e4", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "fab40c41-ceb5-4e01-97a7-75e7b5c86466": {"doc_hash": "4edb01a11cbd05b342231ba187b9342b8533e5dda1106f6ce2243d271f3854c0", "ref_doc_id": "4124fd20-c144-428a-8738-868f0ef176f5"}, "e6e27028-bf1f-41ed-83d4-fc9ab35ae216": {"doc_hash": "501cab8e30e6aaeeac96edd3bf2900c977f6a930cd4786764b18ea1e32faad71", "ref_doc_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98"}, "1c2617c9-d271-4bf3-84fc-0ff48804c94f": {"doc_hash": "690408c14ce633bab9374b894051bc52b6c7a20525bbdffa38b53b272c30adda", "ref_doc_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98"}, "eaeb3e6d-1c89-47ba-9f2f-7eab8c725ebe": {"doc_hash": "3535c57d4526adcf2662bd0897697c03c8b7aab2b94c5f3e5504e8a2fa377cce", "ref_doc_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98"}, "0af44f70-2875-4b9c-af12-2b1700a58d73": {"doc_hash": "8eecb617943ff4e61e07dba0ae9aec7dd29d531da59a7a16feaed354f180e297", "ref_doc_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98"}, "a026ba11-6d60-41c1-9768-38f26eb85285": {"doc_hash": "828c07594b568b62809aa1b446b786513e53e852138c8dabc1d392dbb5e4d183", "ref_doc_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98"}, "e75dbde7-bf8e-410f-b68f-b48e7e60ef4e": {"doc_hash": "04ab43faf83a7c77a03cb7c1a41c9945b5d357e180cf96437069d2ec85da7afc", "ref_doc_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98"}, "9c18d836-3d89-4c63-9b02-7a19e60a10cd": {"doc_hash": "8212ea5f58144cc8181a1c1a9e20a261c222d0052c8be6ecbf7dc64a57b04435", "ref_doc_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98"}, "c9eb35d2-36b5-4b39-aab3-7c1ee47e3dcf": {"doc_hash": "af6191a90f355407d5869e2fc5ea4165ef56894d451a7a482544c5189f883a9f", "ref_doc_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98"}, "9619dc67-af98-4be6-8edf-e51e4f176eec": {"doc_hash": "e5e404e03a616713386b159558f52756eb6499f8d4c917d53edb0886721f021c", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "cff0234c-2352-4748-addd-12807a80182c": {"doc_hash": "208e76ac219169817975c115b6f2c24f767685e3d30f6e9f220d63104a268a3b", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "d7caa5dc-66e2-4d56-bb5a-f385774974df": {"doc_hash": "144251449ef4571725eb6d87ff81a0144a6def9a02e79edd12667cfe283e27a4", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "51373707-bf06-41cc-8adc-321ed0c1d8ed": {"doc_hash": "b0e98838bed827be6b4c28020f3d3fed3689562e2198ac7d025717afd05c5b2e", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "77ec4bcf-444a-4b5c-a11f-5a33c4a044d9": {"doc_hash": "64fcd3a6afe38775b78602ca78aac73770deb4a9d00b88194ed5d2242b0697d6", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "35caf5fe-203b-437f-91bd-948c412a4acc": {"doc_hash": "5802b884d9d8ae4f98a14eb724185bed9f834ff966872bd62dbbf590c17667f0", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "970af8ef-055f-49d6-bd23-7a99d93beaa0": {"doc_hash": "3f30c0af025922dd151579c0ed81441ba13314593955c649bd3ea7588883eae6", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "25a8efbd-8d2f-47bc-b787-e0bf9dab0dde": {"doc_hash": "6650a1001b23e019d8aafb54da8973e4150d1d21380143bc54c100e27986d999", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "c26c21e4-a57e-4805-95b6-fdffe23146d9": {"doc_hash": "8da6eaa42c6c5daff32d11694b1620b96348577b904e390e4d65817c63b0f48a", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}, "934451c0-1236-4018-870b-9da6a31f1ff2": {"doc_hash": "ab9cb2ce21ffb2295a2d51d08c117ffb0cf192986cfb0e7286748d1bfafb296a", "ref_doc_id": "31962845-0db9-4121-bb9f-207b3124a377"}}, "docstore/ref_doc_info": {"c0c8a4f5-faaa-42e1-8a90-be209966240a": {"node_ids": ["3580b79e-60eb-4ff8-934a-6fb88ed3f7d1", "ac4288de-4ae1-406a-843f-556d79f70eda", "4a65184b-61e3-40ea-8469-78edfce52058", "b6b71c3a-3348-4eed-9980-80a7cc5f62b1", "7e99ba01-97b2-4d14-a087-34bac1484462", "000e238b-cdef-4f71-afc6-089fa50b5344", "752d5d12-a8a2-4903-b01c-58a9a497dfa9"], "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}}, "4e5a3d23-fdbe-46da-bc93-26420e7fd497": {"node_ids": ["6ebf05b2-2d84-458e-8dec-de18db07aa85", "758a31d6-2e97-40c5-9f7e-49a8982d281e", "4bc7672c-0368-4024-aa7f-118a7f2d2863", "7748a3cd-1d4f-4351-8f16-c5c00121d55e", "4b7e4051-b29c-4887-910c-ea04d7e76cc9", "8b1d711b-6070-407c-8a5b-7a8c29924ea1"], "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}}, "09dba5a8-fbbc-4862-a3eb-f62ded418ca6": {"node_ids": ["5c82e40b-96d6-451a-9d56-237856160d46", "57df3278-0dc0-4316-a930-eb3cacbc305d", "2d5e5554-f33d-45d3-91ec-5d3c8d9fde21", "f03ce5e3-7352-46a6-91b4-f6605f93d2f2", "d4f198b5-e43a-4b21-a880-969a86eff993", "6f469c08-5923-4b97-9183-b507e2147d68", "cb4f7723-5bab-48a7-a8a4-bc2d144ae98f", "98331acb-520d-4c88-bf33-ba12ed2d1c05", "c733f247-b28b-4d88-872c-4448c08c915d", "a644b85b-2c37-4a87-9732-ef9c73655f74", "40428c82-f883-4bc5-8b8e-d1da59b86d88", "0a47bc34-2be0-4c6e-bf1f-5c510fe57264", "6c23c418-30ba-4b6d-944c-668c5cf67b3a", "a8cd049f-1543-4814-b063-820f58eb10dd", "e38f3751-920f-426a-8cc3-d78c98dd4c6c", "8a29d838-d68b-43f6-8366-91d0710aa648", "5df967b8-ef3b-42f8-8c37-3754c86d6820", "84e2322f-2668-4411-ba38-df18b0bf3e3a", "3ae28f98-82a7-46fa-a829-ce3bffbfd8f3", "3085e786-0751-4570-b0ca-670512804737", "2eb7d8ee-aef3-490e-bc0a-c46f1aced4c2", "3048a5e6-d0e0-41e5-8d39-193e2047a0aa", "bbf2f6fb-61ed-4de5-a565-37ec30a96995", "f560cf60-9321-4303-9ae6-4081cd5e245b", "8238366b-4349-4ab0-a06f-f75b88fc9f58", "69f7e520-1fcf-4a66-9cf1-d858caf4bea4"], "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}}, "4124fd20-c144-428a-8738-868f0ef176f5": {"node_ids": ["e21fdec2-4b97-46b1-955b-fe51e72cbe6a", "bfeb49b9-09a2-4016-9be8-33b2c9f93861", "a6aea7b9-1e40-4ef6-bd5e-e3d843535173", "5517e0a7-37ad-44ed-98f4-ff0144280a1c", "1b289722-756e-4aca-aa0e-af027f5ee4df", "a275f3c3-b6b9-4b37-9c23-83fa65d9612e", "25f52715-070a-4579-8a33-9edfd7aed672", "ffbab969-e6b3-495a-8490-62113458bede", "fab40c41-ceb5-4e01-97a7-75e7b5c86466"], "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}}, "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98": {"node_ids": ["e6e27028-bf1f-41ed-83d4-fc9ab35ae216", "1c2617c9-d271-4bf3-84fc-0ff48804c94f", "eaeb3e6d-1c89-47ba-9f2f-7eab8c725ebe", "0af44f70-2875-4b9c-af12-2b1700a58d73", "a026ba11-6d60-41c1-9768-38f26eb85285", "e75dbde7-bf8e-410f-b68f-b48e7e60ef4e", "9c18d836-3d89-4c63-9b02-7a19e60a10cd", "c9eb35d2-36b5-4b39-aab3-7c1ee47e3dcf"], "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}}, "31962845-0db9-4121-bb9f-207b3124a377": {"node_ids": ["9619dc67-af98-4be6-8edf-e51e4f176eec", "cff0234c-2352-4748-addd-12807a80182c", "d7caa5dc-66e2-4d56-bb5a-f385774974df", "51373707-bf06-41cc-8adc-321ed0c1d8ed", "77ec4bcf-444a-4b5c-a11f-5a33c4a044d9", "35caf5fe-203b-437f-91bd-948c412a4acc", "970af8ef-055f-49d6-bd23-7a99d93beaa0", "25a8efbd-8d2f-47bc-b787-e0bf9dab0dde", "c26c21e4-a57e-4805-95b6-fdffe23146d9", "934451c0-1236-4018-870b-9da6a31f1ff2"], "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}}}, "docstore/data": {"3580b79e-60eb-4ff8-934a-6fb88ed3f7d1": {"__data__": {"id_": "3580b79e-60eb-4ff8-934a-6fb88ed3f7d1", "embedding": null, "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a", "node_type": "4", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "b95f2d304e422db36d0e0b5ced3626ba70fdff7411bff8f39e0e3c6449639a48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac4288de-4ae1-406a-843f-556d79f70eda", "node_type": "1", "metadata": {}, "hash": "c0fb7affca6de4bb34997a026ce1cde18e5d8282e4b586d9347db6a1b2e70f60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Introduction COMP3314 Machine Learning COMP 3314 2 What is AI? ML? DL? COMP 3314 3 History of AI COMP 3314 4 ChatGPT (November 2022) OpenAI GPT-4o & o1 (May & September 2024) Apple Intelligence (September 2024) COMP 3314 7 Large Language Models A Survey of Large Language Models, Zhao et al. arXiv:2303.18223. COMP 3314 8 Multimodal Large Language Models Still developing rapidly A Survey on Multimodal Large Language Models, Yin et al. arXiv:2306.13549 COMP 3314 9 AlphaGo COMP 3314 10 AlphaFold COMP 3314 11 Multi-Agent Hide and Seek COMP 3314 12 DALLE 2 COMP 3314 13 GPT4 COMP 3314 14 Apple Intelligence COMP 3314 15 Outline Motivation Terminology and Notation Types of ML Roadmap Supervised Learning Preprocessing Classification Learning Regression Evaluation and Prediction Reinforcement Learning Python Chess Installation Unsupervised Learning Linear Algebra Review Clustering References Dimensionality Reduction COMP 3314 16 Motivation Nowadays large amount of structured and unstructured data is available ML algorithms can turn this data into knowledge Powerful open source libraries available to do this In this course you will understand how these algorithms work You will also learn how to utilize them to make predictions COMP 3314 17 Motivation ML algorithms are self-learning Automatically derive knowledge from data to make predictions No need for humans to manually derive rules ML offers a more efficient alternative for capturing the knowledge in data to gradually improve the performance of predictive models ML becomes increasingly relevant in CS research More importantly Plays an ever greater role in our everyday lives COMP 3314 18 How do you use machine learning everyday?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac4288de-4ae1-406a-843f-556d79f70eda": {"__data__": {"id_": "ac4288de-4ae1-406a-843f-556d79f70eda", "embedding": null, "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a", "node_type": "4", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "b95f2d304e422db36d0e0b5ced3626ba70fdff7411bff8f39e0e3c6449639a48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3580b79e-60eb-4ff8-934a-6fb88ed3f7d1", "node_type": "1", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "37996f8cac2130f8c3973c21a0a71b2b1e8b41ae4121db0121f61e434653cb96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a65184b-61e3-40ea-8469-78edfce52058", "node_type": "1", "metadata": {}, "hash": "1a8caf9205c83a1b0365d16e2d11aebbb3ca3fd89e6ac3933f4f043a679e5496", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "COMP 3314 19 Examples of Machine Learning Basket analysis Credit scoring Medical diagnosis Biometrics Object recognition Service recommendations Understanding human learning COMP 3314 20 Machine Learning Definition Subfield of Artificial Intelligence (AI) Arthur Samuel (1959) Field of study that gives computers the ability to lean without being explicitly programmed Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E COMP 3314 21 Types of Machine Learning In the following we will consider three types of machine learning COMP 3314 22 Supervised Learning Learn from labeled training data Make predictions about unseen / future data Supervised refers to a set of samples where the desired output signals (labels) are already known COMP 3314 23 Supervised Learning: Classification vs. Regression Two subcategories of supervised learning Classification A supervised learning task with discrete class labels E.g., spam email classifier Regression Outcome is a continuous value E.g., student exam score prediction COMP 3314 24 Supervised Learning - Classification Goal: Predict class labels of new instances, based on past observations Class labels are discrete, unordered values Two subcategories of classifiers: Binary classification Only two possible class labels can be assigned E.g., spam vs. non-spam emails Multiclass classification Any fixed number > 2 of class labels can be assigned E.g., handwritten character recognition MNIST dataset COMP 3314 25 Classification - Example Given 30 training samples 15 labeled as negative class 15 labeled as positive class Let each sample have 2 dimensions Classifier will learn the decision boundary Represented as a dashed line Able to separate the two classes COMP 3314 26 Regression Prediction of continuous outcome The term regression was devised by Francis Galton in his article Regression towards Mediocrity in 1886 Example: Predicting the exam scores given time spent studying COMP 3314 27 Regression - Example Given Predictor variable x Response variable y I.e., 1D data set Fit a line to it minimizing the distance between sample points and the fitted line Average squared distance is most commonly used Use the line to predict outcome of new data COMP 3314 28 Quiz Consider the following supervised ML tasks.", "mimetype": "text/plain", "start_char_idx": 1701, "end_char_idx": 4117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a65184b-61e3-40ea-8469-78edfce52058": {"__data__": {"id_": "4a65184b-61e3-40ea-8469-78edfce52058", "embedding": null, "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a", "node_type": "4", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "b95f2d304e422db36d0e0b5ced3626ba70fdff7411bff8f39e0e3c6449639a48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac4288de-4ae1-406a-843f-556d79f70eda", "node_type": "1", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "47042e5cade547c80f5667b38c31dc0770790b924fdc2034e52e2b483c17634f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6b71c3a-3348-4eed-9980-80a7cc5f62b1", "node_type": "1", "metadata": {}, "hash": "9ccda294d14cc5cc42de49cce5ee4898db4f7c9b5b6bee4f0e42258bd71a8e28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Label each task with Classification Task or Regression Task a. You are working for an investment bank and your task is to predict investors sentiment for certain stocks by analyzing popular online investments forums b. You are working for a property agency and your task is to predict the housing price for a property based on past data that the agency has available in their database c. Your task is to analyze a video stream of the western harbour tunnel and count how many Tesla pass by every day COMP 3314 29 Reinforcement Learning The system (aka agent) improves its performance based on interactions with an environment Trial-and-Error approach Learning by doing The agent receives feedback (reward) from the environment This reward is not the correct ground truth It is a sample experience Extensive interaction with the environment allows agent to learn a series of actions that maximizes this reward COMP 3314 30 Reinforcement Learning - Example: Chess Agent decides upon a series of moves depending on state of board Environment is the board Reward can be defined as win or lose at the end of the game Outcome of each move results in different state of the environment Removing an opponent's chess piece from the board or threatening the queen is associated with a positive event Losing a chess piece to the opponent is associated with a negative event Note: Not every turn results in the removal of a chess piece Reinforcement learning is concerned with learning the series of steps by maximizing a reward based on immediate and delayed feedback COMP 3314 31 Reinforcement Learning - Example Learning to walk COMP 3314 32 Unsupervised Learning Unlabeled data / data of unknown structure Explores the structure of data Extract meaningful information without guidance of known outcome variable / reward function Examples Clustering Dimensionality reduction COMP 3314 33 Clustering Exploratory data analysis technique Organizes information into meaningful subgroups (clusters) without having any knowledge of group memberships Each cluster defines a group of objects that share a certain degree of similarity but are more dissimilar to objects in other clusters COMP 3314 34 Clustering - Example COMP 3314 35 Clustering - More Examples Human genetic clustering Sequence clustering Social network analysis Market research Grouping of shopping items COMP 3314 36 Dimensionality Reduction Often we are working with data of high dimensionality I.e.,", "mimetype": "text/plain", "start_char_idx": 4118, "end_char_idx": 6571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6b71c3a-3348-4eed-9980-80a7cc5f62b1": {"__data__": {"id_": "b6b71c3a-3348-4eed-9980-80a7cc5f62b1", "embedding": null, "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a", "node_type": "4", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "b95f2d304e422db36d0e0b5ced3626ba70fdff7411bff8f39e0e3c6449639a48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a65184b-61e3-40ea-8469-78edfce52058", "node_type": "1", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "14f79a8689c86e6813270b7eb2e14cb0c05e4c983fe0654ef2aa372af47978b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e99ba01-97b2-4d14-a087-34bac1484462", "node_type": "1", "metadata": {}, "hash": "4e4c9aff5d449f2d057d98ca54ab4bc4baee41a9d125eeb983ea7bd57052b520", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "e., each observation comes with a high number of measurements High dimensional data can present a challenge Computational performance Predictive performance Visualization Dimensionality reduction is a commonly used approach in feature preprocessing Compress data onto a smaller dimensional subspace Retaining most relevant information COMP 3314 37 Dimensionality Reduction - Example High-dimensional feature set can be projected onto 1D, 2D or 3D feature spaces 3D to 2D example COMP 3314 38 Types of machine learning In the following we will consider three types of machine learning COMP 3314 39 Terminology and Notations Iris flower data set contains measurements of 150 Iris flowers from three different species Setosa, Versicolor, and Virginica Introduced in Fishers 1936 paper The use of multiple measurements in taxonomic problems Row A single flower sample Column Flower features (measurements in centimeters) COMP 3314 40 Terminology and Notations We will use a matrix and vector notation to refer to our data Each sample is a separate row in a feature matrix X, where each feature is stored as a separate column Iris dataset example 150 samples and four features are written as a 150 x 4 matrix X COMP 3314 41 Terminology and Notations We will use the superscript i to refer to the ith training sample, and the subscript j to refer to the jth dimension of the dataset For example x (i) = x (150) refers to the first dimension of the flower j 1 and sample 150 We use lowercase, bold-face letters to refer to vectors and uppercase, bold-face letters to refer to matrices Note that each row in the iris dataset X can be written as a four- dimensional row vector and each feature dimension is a 150-dimensional column vector COMP 3314 42 Roadmap Typical workflow for using ML in predictive modeling COMP 3314 43 Preprocessing Preprocessing of the data is a crucial steps in any ML application Feature selection,", "mimetype": "text/plain", "start_char_idx": 6568, "end_char_idx": 8484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e99ba01-97b2-4d14-a087-34bac1484462": {"__data__": {"id_": "7e99ba01-97b2-4d14-a087-34bac1484462", "embedding": null, "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a", "node_type": "4", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "b95f2d304e422db36d0e0b5ced3626ba70fdff7411bff8f39e0e3c6449639a48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6b71c3a-3348-4eed-9980-80a7cc5f62b1", "node_type": "1", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "18f22db0fcf50440f0c0839546e5411c50d328555718e89274c6683b2c2aa115", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "000e238b-cdef-4f71-afc6-089fa50b5344", "node_type": "1", "metadata": {}, "hash": "f4387943941431a03b3053e511767a14d95b9f7e41d6c17db08280503dadd5f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "bold-face letters to refer to vectors and uppercase, bold-face letters to refer to matrices Note that each row in the iris dataset X can be written as a four- dimensional row vector and each feature dimension is a 150-dimensional column vector COMP 3314 42 Roadmap Typical workflow for using ML in predictive modeling COMP 3314 43 Preprocessing Preprocessing of the data is a crucial steps in any ML application Feature selection, extraction and scaling Select and extract useful features from raw data Many algorithms also require that the selected features are on the same scale Dimensionality reduction May improve Computational performance Predictive performance Sampling Randomly divide the dataset into a separate training and test set to determine whether our algorithm not only performs well on the training set but also generalizes well to new data Keep the test set until the very end to evaluate the final model COMP 3314 44 Learning Model selection Compare algorithms and select the best performing model Cross-validation How de we know which model performs well on the final test dataset if we dont use this test set for model selection?", "mimetype": "text/plain", "start_char_idx": 8054, "end_char_idx": 9204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "000e238b-cdef-4f71-afc6-089fa50b5344": {"__data__": {"id_": "000e238b-cdef-4f71-afc6-089fa50b5344", "embedding": null, "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a", "node_type": "4", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "b95f2d304e422db36d0e0b5ced3626ba70fdff7411bff8f39e0e3c6449639a48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e99ba01-97b2-4d14-a087-34bac1484462", "node_type": "1", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "4e413853428fc3b3679eaeec5a943cb8dc192a94d10c96ebe6df36203c4be4d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "752d5d12-a8a2-4903-b01c-58a9a497dfa9", "node_type": "1", "metadata": {}, "hash": "580a92e92316a31e46d6206233d413e7a26230cc0eb9e03523b16f10a8d6c6ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cross-validation splits the training dataset further into training and validation subsets Performance metric Decide upon a metric to measure performance Hyperparameter optimization Fine-tune parameters of the model based on performance on validation set COMP 3314 45 Evaluation and Prediction After model selection and training we use the test dataset to estimate how well it performs on unseen data Estimate the generalization error If we are satisfied with its performance, we can now use this model to predict new data Important Parameters for the previously mentioned procedures, such as feature scaling and dimensionality reduction, are solely obtained from the training dataset The same parameters are later reapplied to the test data and any new data samples COMP 3314 46 Python We assume that you are familiar with the basics of python Recommended textbook Free PDF here COMP 3314 47 Programming Environment: Local In this course we are going to use Python 3, NumPy, MatPlotLib, SciPy and Jupyter COMP 3314 48 Programming Environment: Cloud Google CoLab COMP 3314 49 Installation Code is here COMP 3314 50 Linear Algebra Review We will only use basic concepts from linear algebra However, if you need a quick refresher, please take a look at Zico Kolter's excellent videos COMP 3314 51 Python Review We assume that your are familiar with the libraries/tools, follow these links if you need a refresher NumPy Pandas Matplotlib Jupyter COMP 3314 52 References Materials in this chapter are based on Book Code COMP 3314 53 References Some materials in this chapter are based on Book Code COMP 3314 54 Exercise 1 How would you define Machine Learning? Can you name four types of problems where it shines? What is a labeled training set? What are the two most common supervised tasks? Can you name four common unsupervised tasks? What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains? What type of algorithm would you use to segment your customers into multiple groups? Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem? COMP 3314 55 Exercise 2 What is an online learning system? What is out-of-core learning?", "mimetype": "text/plain", "start_char_idx": 9205, "end_char_idx": 11435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "752d5d12-a8a2-4903-b01c-58a9a497dfa9": {"__data__": {"id_": "752d5d12-a8a2-4903-b01c-58a9a497dfa9", "embedding": null, "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0c8a4f5-faaa-42e1-8a90-be209966240a", "node_type": "4", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "b95f2d304e422db36d0e0b5ced3626ba70fdff7411bff8f39e0e3c6449639a48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "000e238b-cdef-4f71-afc6-089fa50b5344", "node_type": "1", "metadata": {"file_name": "1. Introduction_processed.txt", "source_file": "1. Introduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\1. Introduction_processed.txt"}, "hash": "996aed87cc28f0f52788e448a9a81c053928d81d77577835c3856bfaa20f96f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Can you name four common unsupervised tasks? What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains? What type of algorithm would you use to segment your customers into multiple groups? Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem? COMP 3314 55 Exercise 2 What is an online learning system? What is out-of-core learning? What type of learning algorithm relies on a similarity measure to make predictions? What is the difference between a model parameter and a learning algorithms hyperparameter? What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions? Can you name four of the main challenges in Machine Learning? COMP 3314 56 Exercise 3 If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions? What is a test set, and why would you want to use it? What is the purpose of a validation set? What can go wrong if you tune hyperparameters using the test set? COMP 3314 57 Boston Dynamics Innovative AI Projects", "mimetype": "text/plain", "start_char_idx": 10993, "end_char_idx": 12194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ebf05b2-2d84-458e-8dec-de18db07aa85": {"__data__": {"id_": "6ebf05b2-2d84-458e-8dec-de18db07aa85", "embedding": null, "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497", "node_type": "4", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "81581aea8eb4174aa8cff463b38ca37500af00ce72f59e468672f5ab1d6d5e79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "758a31d6-2e97-40c5-9f7e-49a8982d281e", "node_type": "1", "metadata": {}, "hash": "107f91759f3a53be31dc9ba2f2bc756e7e432717b9a51e150e8d6a029a5c00a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. Perceptron & Adaline COMP3314 Machine Learning COMP 3314 2 Outline In this chapter we will implement two of the first published machine learning algorithms for classification Perceptron Adaptive Linear Neurons (Adaline) This will lay the groundwork for using more powerful classifiers with the scikit-learn library We will Build an intuition for machine learning algorithms Use pandas, NumPy, and Matplotlib to read in, process, and visualize data Implement linear classification algorithms in Python COMP 3314 3 Outline Neuron Artificial Neuron History Definition Perceptron Perceptron Learning Rule Linearly Separable Implementation Adaptive Linear Neuron (Adaline) Implementation Feature Scaling Stochastic Gradient Descent / Mini-Batch Learning COMP 3314 4 Neuron Neurons are interconnected nerve cells in the brain that are involved in the processing and transmitting of chemical and electrical signals COMP 3314 5 Artificial Neuron - History McCulloch and Pitts described the first artificial neuron in 1943 (aka MCP neuron) as a simple logic gate with binary outputs Signals arrive at the dendrites, are integrated into the cell body, and, if the accumulated signal exceeds a certain threshold, an output signal is generated at the axon A few years later, in 1958, Rosenblatt published the first concept of the perceptron learning rule based on the MCP neuron Automatically learn the optimal weight coefficients that are then multiplied with the input features COMP 3314 6 Artificial Neuron - Definition Consider a binary classification task where we refer to our two classes as 1 (positive class) and -1 (negative class) We can define a decision function where is a threshold and z is the net input of the input values x and the corresponding weight vector w COMP 3314 7 Artificial Neuron - Definition For simplicity, we can bring the threshold to the left side of the equation and define and where w = - and x = 1 0 0 w is called the bias unit 0 COMP 3314 8 Artificial Neuron - Definition COMP 3314 9 Perceptron Learning Rule Rosenblatt's perceptron rule can be summarized by the following steps a. Initialize the weights b.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "758a31d6-2e97-40c5-9f7e-49a8982d281e": {"__data__": {"id_": "758a31d6-2e97-40c5-9f7e-49a8982d281e", "embedding": null, "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497", "node_type": "4", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "81581aea8eb4174aa8cff463b38ca37500af00ce72f59e468672f5ab1d6d5e79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ebf05b2-2d84-458e-8dec-de18db07aa85", "node_type": "1", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "c19bfa424484df30a66416382df4df523ac4955d0a9f06e6b1f684c8c479f9f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bc7672c-0368-4024-aa7f-118a7f2d2863", "node_type": "1", "metadata": {}, "hash": "94184d566efff4a7b4a6b3d2883f6099c687895c644d95c59d3371a345f4257c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can bring the threshold to the left side of the equation and define and where w = - and x = 1 0 0 w is called the bias unit 0 COMP 3314 8 Artificial Neuron - Definition COMP 3314 9 Perceptron Learning Rule Rosenblatt's perceptron rule can be summarized by the following steps a. Initialize the weights b. For each training sample x(i) i. Compute the output value , i.e. the class label predicted by unit step function i. Update the weights The value of w is calculated as follows j Where is the learning rate, y(i) is the true class label of the ith training sample, and (i) is the predicted class label COMP 3314 10 Weight Update All weights in the weight vector are updated simultaneously, which means that we don't recompute the (i) before all of the weights w are j updated I.e., for a two-dimensional dataset, we would write the update as COMP 3314 11 Weight Update In the two scenarios where the perceptron predicts the class label correctly, the weights remain unchanged However, in the case of a wrong prediction, the weights are being pushed towards the direction of the positive or negative target class COMP 3314 12 Weight Update Note that the weight update is proportional the value of x (i) j COMP 3314 13 Linearly Separable It was shown that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small If the two classes can't be separated by a linear decision boundary, we can set a maximum number of passes over the training dataset (epochs) and/or a threshold for the number of tolerated misclassifications The perceptron would never stop updating the weights otherwise COMP 3314 14 Linearly Separable vs.", "mimetype": "text/plain", "start_char_idx": 1829, "end_char_idx": 3533, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4bc7672c-0368-4024-aa7f-118a7f2d2863": {"__data__": {"id_": "4bc7672c-0368-4024-aa7f-118a7f2d2863", "embedding": null, "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497", "node_type": "4", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "81581aea8eb4174aa8cff463b38ca37500af00ce72f59e468672f5ab1d6d5e79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "758a31d6-2e97-40c5-9f7e-49a8982d281e", "node_type": "1", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "4a2c26532877a1911e63fa9268a4513db45b166c93419a29fc112901bbb31f2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7748a3cd-1d4f-4351-8f16-c5c00121d55e", "node_type": "1", "metadata": {}, "hash": "a13f8105e9e82b3370bdb33a5669408124457d3b416c375b4fcae89c981d3604", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Not Linearly Separable COMP 3314 15 Perceptron COMP 3314 16 Iris Dataset - Loading Code is here COMP 3314 17 Iris Dataset - Preprocessing and Plotting COMP 3314 18 Perceptron - Implementation COMP 3314 19 Perceptron - Training COMP 3314 20 Perceptron - Plotting Decision Region COMP 3314 21 Outline Neuron Artificial Neuron History Definition Perceptron Perceptron Learning Rule Linearly Separable Implementation Adaptive Linear Neuron (Adaline) Implementation Feature Scaling Stochastic Gradient Descent / Mini-Batch Learning COMP 3314 22 Adaptive Linear Neuron ADAptive LInear NEuron: Adaline Improvement on Perceptron algorithm Published in 1960 by Bernard Widrow and Ted Hoff In Adaline the weights are updated based on a linear activation function While the linear activation function is used for learning the weights, we still use a threshold function to make the final prediction COMP 3314 23 Adaptive Linear Neuron COMP 3314 24 Objective Function One of the key ingredients of supervised ML algorithms is the objective function that is to be optimized E.g., cost function that we want to minimize In Adaline the cost function J is defined as the sum of squared errors (SSE) between the calculated and true class label It can be shown that J is differentiable and convex It will be easy to minimize (using e.g.", "mimetype": "text/plain", "start_char_idx": 3534, "end_char_idx": 4851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7748a3cd-1d4f-4351-8f16-c5c00121d55e": {"__data__": {"id_": "7748a3cd-1d4f-4351-8f16-c5c00121d55e", "embedding": null, "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497", "node_type": "4", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "81581aea8eb4174aa8cff463b38ca37500af00ce72f59e468672f5ab1d6d5e79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bc7672c-0368-4024-aa7f-118a7f2d2863", "node_type": "1", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "14479273b4b3035586693bfc9211c0dfb35e74d25eb3f1a139844a0ed138e51d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b7e4051-b29c-4887-910c-ea04d7e76cc9", "node_type": "1", "metadata": {}, "hash": "760e0e868422d7d08553bc699b051e8912dc0161d5a4458aee696389b8446edb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "gradient descent) COMP 3314 25 Gradient Descent A generic optimization algorithm Capable of finding optimal solutions to a wide range of problems Main idea Tweak parameters iteratively in order to minimize a cost function Measures the local gradient of the error function with regard to the parameter vector and goes in the direction of descending gradient Once the gradient is zero, it has reached a minimum COMP 3314 26 Weight Update Update the weights by taking a step in the opposite direction of the gradient J(w) of our cost function J(w) The weight change w is defined as the negative gradient multiplied by the learning rate COMP 3314 27 COMP 3314 28 Weight Update To compute the gradient of the cost function, we compute the partial derivative of the cost function with respect to each weight w j The update of weight w can then be written as j Batch gradient descent Note that the weight update is calculated based on all samples in the training set (instead of updating the weights incrementally after each sample) COMP 3314 29 Learn from the Author The LMS algorithm and ADALINE. Part I - The LMS algorithm Bernard Widrow and Ted Hoff published a paper introducing Adaline in 1959 Bernard Widrow has a youtube channel He has a video talking about Adaline :-) COMP 3314 30 Adaline - Implementation Code is here COMP 3314 31 COMP 3314 32 Learning Rate Important hyperparameter of Gradient Descent If too small Requires excessive many iterations to converge Takes a long time If too large Might jump across the valley and end up on the other side Possibly even higher up than before Algorithm may diverge, with larger and larger values COMP 3314 33 COMP 3314 34 Gradient Descent Pitfalls J(w) w COMP 3314 35 Feature Scaling Many ML algorithms require feature scaling for w 2 optimal performance E.g., gradient descent converges more quickly if our data follows a standard distribution w 1 Standardization A feature scaling method w After standardization, feature have 2 a mean value of 0 a standard deviation of 1 w 1 COMP 3314 36 Standardization For instance, to standardize the jth feature,", "mimetype": "text/plain", "start_char_idx": 4852, "end_char_idx": 6953, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b7e4051-b29c-4887-910c-ea04d7e76cc9": {"__data__": {"id_": "4b7e4051-b29c-4887-910c-ea04d7e76cc9", "embedding": null, "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497", "node_type": "4", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "81581aea8eb4174aa8cff463b38ca37500af00ce72f59e468672f5ab1d6d5e79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7748a3cd-1d4f-4351-8f16-c5c00121d55e", "node_type": "1", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "3d74b54b916534a1ca0e788eb130a1a5cd089f7a770860cd562c65ab805cd765", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b1d711b-6070-407c-8a5b-7a8c29924ea1", "node_type": "1", "metadata": {}, "hash": "3bcaf02de72afaeebae589ef9b788fe283ae2282b99f6d7250fecba2283585a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "g., gradient descent converges more quickly if our data follows a standard distribution w 1 Standardization A feature scaling method w After standardization, feature have 2 a mean value of 0 a standard deviation of 1 w 1 COMP 3314 36 Standardization For instance, to standardize the jth feature, we can simply subtract the sample mean from every training sample and divide it by its standard j deviation j Here, x is a vector consisting of the jth feature values of all training j samples n This standardization technique is applied to each feature j in our dataset COMP 3314 37 COMP 3314 38 Stochastic Gradient Descent Imagine we have a very large dataset with millions of data points Not uncommon in ML applications Running batch gradient descent can be computationally costly in such scenarios since we need to reevaluate the whole training dataset each time we take one step Stochastic Gradient Descent is a popular alternative Instead of updating the weights based on the sum of the accumulated errors over all samples x(i) we update all the weights incremental for each training sample COMP 3314 39 Stochastic Gradient Descent Typically reaches convergences faster because of the more frequent weight updates Can escape shallow local minima more readily if we are working with nonlinear cost functions It is important to present it training data in a random order In addition shuffle the training set for every epoch to prevent cycles Another advantage of stochastic gradient descent is that we can use it for online learning In online learning, our model is trained on the fly as new training data arrives COMP 3314 40 w 2 w 1 COMP 3314 41 Mini-Batch Learning A compromise between batch gradient descent and stochastic gradient descent Apply batch gradient descent to smaller subsets of the training data E.g., 32 samples at a time The advantage over batch gradient descent is that convergence is reached faster via mini-batches because of the more frequent weight updates Furthermore,", "mimetype": "text/plain", "start_char_idx": 6658, "end_char_idx": 8650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b1d711b-6070-407c-8a5b-7a8c29924ea1": {"__data__": {"id_": "8b1d711b-6070-407c-8a5b-7a8c29924ea1", "embedding": null, "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e5a3d23-fdbe-46da-bc93-26420e7fd497", "node_type": "4", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "81581aea8eb4174aa8cff463b38ca37500af00ce72f59e468672f5ab1d6d5e79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b7e4051-b29c-4887-910c-ea04d7e76cc9", "node_type": "1", "metadata": {"file_name": "2. Perceptron  Adaline_processed.txt", "source_file": "2. Perceptron  Adaline", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\2. Perceptron  Adaline_processed.txt"}, "hash": "9f004ad9fca911528170245a5f5545a440cf785d6141bfe199eeacf9d8991227", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "our model is trained on the fly as new training data arrives COMP 3314 40 w 2 w 1 COMP 3314 41 Mini-Batch Learning A compromise between batch gradient descent and stochastic gradient descent Apply batch gradient descent to smaller subsets of the training data E.g., 32 samples at a time The advantage over batch gradient descent is that convergence is reached faster via mini-batches because of the more frequent weight updates Furthermore, mini-batch learning allows us to replace the for loop over the training samples in stochastic gradient descent with vectorized operations COMP 3314 42 COMP 3314 43 COMP 3314 44 COMP 3314 45 w 2 w 1 COMP 3314 46 Summary Gained a good understanding of the basic concepts of linear classifiers for supervised learning Implemented Perceptron Adaline Efficient training via a vectorized implementation of gradient descent and online learning via stochastic gradient descent COMP 3314 47 References Most materials in this chapter are based on Book Code COMP 3314 48 References Some materials in this chapter are based on Book Code COMP 3314 49 Exercise 1 Suppose the features in your training set have very different scales What can you do about it? Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model? Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough? Suppose you use Batch Gradient Descent and you plot the validation error at every epoch If you notice that the validation error consistently goes up, what is likely going on? How can you fix this? Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?", "mimetype": "text/plain", "start_char_idx": 8210, "end_char_idx": 9883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c82e40b-96d6-451a-9d56-237856160d46": {"__data__": {"id_": "5c82e40b-96d6-451a-9d56-237856160d46", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57df3278-0dc0-4316-a930-eb3cacbc305d", "node_type": "1", "metadata": {}, "hash": "a0bdb79b65755680778d18f50eba82bb53d74adf5eb3f3d7cb6123be1e4d6121", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. Logistic Regression, SVM, Decision Trees, KNN COMP3314 Machine Learning COMP 3314 2 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 3 Scikit-learn Library In the previous chapter we implement the perceptron rule in Python ourselves We will now use the scikit-learn library and train a perceptron model similar to the one we implemented in the previous chapter This will serve as an intro to the scikit-learn library COMP 3314 4 Code - PerceptronSkLearn.ipynb Available here on CoLab COMP 3314 5 Iris Dataset Conveniently, the Iris dataset is already available via scikit-learn We will only use two features (petal length and petal width) from the Iris dataset for visualization purposes Assign all flower samples to the feature matrix X and the corresponding class labels of the flower species to the vector y COMP 3314 6 Training and Testing To evaluate how well a trained model performs on unseen data, we will further split the dataset into separate training and test datasets Note that the train_test_split function shuffles the training sets internally and performs stratification before splitting Otherwise, all class 0 and class 1 samples would have ended up in the training set, and the test set would consist of 45 samples from class 2 COMP 3314 7 Stratification We took advantage of the built-in support for stratification In this context, stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset COMP 3314 8 Multiclass Classification n (number of classes) > 2 Some classification algorithms naturally permit n > 2 Others are by nature binary algorithms OvA or One-versus-Rest (OvR) Train one classifier per class, where the particular class is treated as the positive class Samples from all other classes are considered negative classes If we were to classify a new data sample, we would use our n classifiers,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57df3278-0dc0-4316-a930-eb3cacbc305d": {"__data__": {"id_": "57df3278-0dc0-4316-a930-eb3cacbc305d", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c82e40b-96d6-451a-9d56-237856160d46", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "d7aef4744004c1d460bee43cd49bdce8bab1947ecfae8817bb8a2b321e1efcea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d5e5554-f33d-45d3-91ec-5d3c8d9fde21", "node_type": "1", "metadata": {}, "hash": "455f1899658b46b306412ccb522d60c5919ecfe40e578ffabd15fc2219b2b473", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where the particular class is treated as the positive class Samples from all other classes are considered negative classes If we were to classify a new data sample, we would use our n classifiers, and assign the class label with the highest confidence to the particular sample COMP 3314 9 Feature Scaling Recall that many machine learning and optimization algorithms also require feature scaling for optimal performance Here, we will standardize the features using the StandardScaler class from scikit-learn's preprocessing module COMP 3314 10 Training We can now train a perceptron model Most algorithms in scikit-learn already support multiclass classification by default via the One-versus-Rest (OvR) method, which allows us to feed the three flower classes to the perceptron all at once COMP 3314 11 Scikit-Learn Help The Perceptron, as well as other scikit-learn functions and classes, often have additional parameters that we omit for clarity You can read more about those parameters using the help function COMP 3314 12 Scikit-Learn Help The Perceptron, as well as other scikit-learn functions and classes, often have additional parameters that we omit for clarity You can read more about those parameters using the help function COMP 3314 13 Testing Having trained a model in scikit-learn, we can make predictions via the predict method on the test data COMP 3314 14 Testing - Accuracy The scikit-learn library also implements a large variety of different performance metrics that are available via the metrics module We can calculate the classification accuracy as follows Alternatively, each classifier in scikit-learn has a score method COMP 3314 15 Decision Regions Plotting Finally, we can use our plot_decision_regions function to plot the decision regions of our newly trained perceptron model Visualize how well it separates the different flower samples However,", "mimetype": "text/plain", "start_char_idx": 1915, "end_char_idx": 3793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d5e5554-f33d-45d3-91ec-5d3c8d9fde21": {"__data__": {"id_": "2d5e5554-f33d-45d3-91ec-5d3c8d9fde21", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57df3278-0dc0-4316-a930-eb3cacbc305d", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "93f0f6605b36f46e72723acaeccecaaa0992489bce7e1916d7f3d0c87c2fec65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f03ce5e3-7352-46a6-91b4-f6605f93d2f2", "node_type": "1", "metadata": {}, "hash": "417b54167694b32244a59c379fc81eeb2e0113df265bc79a83fdea1294295315", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "each classifier in scikit-learn has a score method COMP 3314 15 Decision Regions Plotting Finally, we can use our plot_decision_regions function to plot the decision regions of our newly trained perceptron model Visualize how well it separates the different flower samples However, let's add a small modification to highlight the samples from the test dataset via small circles COMP 3314 16 Decision Regions Plotting COMP 3314 17 Decision Regions Plotting COMP 3314 18 Perceptron The three flower classes cannot be perfectly separated by a linear decision boundary Recall that the perceptron algorithm never converges on datasets that aren't perfectly linearly separable In the following sections, we will look at more powerful linear classifiers that converge to a cost minimum even if the classes are not perfectly linearly separable COMP 3314 19 Logistic Regression - Intuition Widely used algorithm for binary classification For classification, not regression The logit function is defined as the logarithm of odds, i.e., It takes as input values in the range 0 to 1 and transforms them to values over the entire real-number range COMP 3314 20 Logit Function COMP 3314 21 Inverse of Logit (Sigmoid) Function COMP 3314 22 Logistic Regression - Intuition The output of the logit function can be used to express a linear relationship between feature values and the log-odds p ( y = 1| x ) is the conditional probability that a particular sample belongs to class 1 given its features x COMP 3314 23 Adaline vs. Logistic Regression COMP 3314 24 Output of Sigmoid Function The output of the sigmoid function is interpreted as the probability of a particular sample belonging to class 1 (z) = P ( y = 1| x ; w ), given its features x parameterized by the weights w E.g, let ( z ) = 0.8 for a particular flower sample. I.e.,", "mimetype": "text/plain", "start_char_idx": 3512, "end_char_idx": 5332, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f03ce5e3-7352-46a6-91b4-f6605f93d2f2": {"__data__": {"id_": "f03ce5e3-7352-46a6-91b4-f6605f93d2f2", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d5e5554-f33d-45d3-91ec-5d3c8d9fde21", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "a4d7f689273e7c2550106bde6c3c92eff9e02ac1ada72ad63d62709d8261aa96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4f198b5-e43a-4b21-a880-969a86eff993", "node_type": "1", "metadata": {}, "hash": "f9c5855ba7a09e23e428675d0690a8719b4ab7c241b54b915276a14179a522d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Logistic Regression COMP 3314 24 Output of Sigmoid Function The output of the sigmoid function is interpreted as the probability of a particular sample belonging to class 1 (z) = P ( y = 1| x ; w ), given its features x parameterized by the weights w E.g, let ( z ) = 0.8 for a particular flower sample. I.e., the probability that this sample is an Iris-versicolor flower is 80 % The probability that this flower is an Iris-setosa flower can be calculated as P ( y = 0 | x ; w ) = 1 P ( y = 1| x ; w ) = 0.2 The predicted probability can then simply be converted into a binary outcome via a threshold function COMP 3314 25 Learning w Recall that we defined the sum-squared-error cost function as follows We minimized this function in order to learn the weights w for our Adaline classification model For logistic regression, we define the likelihood L that we want to maximize as COMP 3314 26 Learning w In practice, it is easier to maximize the (natural) log of this equation, which is called the log-likelihood function Let's rewrite the log-likelihood as a cost function that can be minimized using gradient descent COMP 3314 27 Learning w Let us take a look at the cost that we calculate for one single training sample We can see that the first term is zero if y = 0, and the second term is zero if y = 1 COMP 3314 28 Plotting our new J COMP 3314 29 From Adaline to Logistic Regression If we were to implement logistic regression ourselves, we could simply substitute the cost function J in our Adaline implementation from the previous chapter COMP 3314 30 Code - LogisticRegression.", "mimetype": "text/plain", "start_char_idx": 5023, "end_char_idx": 6610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d4f198b5-e43a-4b21-a880-969a86eff993": {"__data__": {"id_": "d4f198b5-e43a-4b21-a880-969a86eff993", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f03ce5e3-7352-46a6-91b4-f6605f93d2f2", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "6ba439d11837e6e4e1da844c1ff5338c5eae3ccf7027817520842a588a3fc6af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f469c08-5923-4b97-9183-b507e2147d68", "node_type": "1", "metadata": {}, "hash": "59225250272fd9a28330bad9b2c8b1bbc8c3f060c903fcccb9d429f0d9f9bea8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and the second term is zero if y = 1 COMP 3314 28 Plotting our new J COMP 3314 29 From Adaline to Logistic Regression If we were to implement logistic regression ourselves, we could simply substitute the cost function J in our Adaline implementation from the previous chapter COMP 3314 30 Code - LogisticRegression.ipynb Available here on CoLab COMP 3314 31 COMP 3314 32 COMP 3314 33 Logistic Regression with scikit-learn Scikit-learn's implementation of logistic regression also supports multi-class settings off the shelf (OvR by default) COMP 3314 34 Prediction The probability that training examples belong to a certain class can be computed using the predict_proba method For example, we can predict the probabilities of the first three samples in the test set as follows Notice that for each row the columns sum all up to one COMP 3314 35 Prediction We can get the predicted class labels by identifying the largest column in each row, for example, using NumPy's argmax function The returned class indices correspond to Iris-virginica, Iris-setosa, and Iris-setosa This is just a manual approach to calling the predict method directly COMP 3314 36 COMP 3314 37 Overfitting and Underfitting A model may perform well on training data but does not generalize well to unseen data (test data) Two main categories of things that can go wrong Overfitting (high variance) Could be caused by having too many parameters that lead to a model that is too complex Underfitting (high bias) Model is not complex enough to capture the pattern in the training data well COMP 3314 38 COMP 3314 39 Regularization One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization Useful method to handle collinearity (high correlation among features), filter out noise from data,", "mimetype": "text/plain", "start_char_idx": 6295, "end_char_idx": 8100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f469c08-5923-4b97-9183-b507e2147d68": {"__data__": {"id_": "6f469c08-5923-4b97-9183-b507e2147d68", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4f198b5-e43a-4b21-a880-969a86eff993", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "d92d38b3161f43ebf89ef43ef13418f0e1d723ec273a1e1bfb8529ca39feccad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb4f7723-5bab-48a7-a8a4-bc2d144ae98f", "node_type": "1", "metadata": {}, "hash": "44bcf4be95e742bbb5c3ab73e2c6a089c6829ae48fc61ea2d41ad2d56019e6ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "filter out noise from data, and eventually prevent overfitting Requires feature scaling such as standardization Need to ensure that all our features are on comparable scales COMP 3314 40 L2 Regularization The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values A common form of regularization is so-called L2 regularization (sometimes also called L2 shrinkage or weight decay), which can be written as follows is the so-called regularization parameter COMP 3314 41 Regularization - Updated Cost Function The cost function for logistic regression can be regularized by adding a simple regularization term, which will shrink the weights during model training Via the regularization parameter , we can then control how well we fit the training data while keeping the weights small By increasing the value of , we increase the regularization strength COMP 3314 42 C The parameter C that is implemented for the LogisticRegression class in scikit-learn is directly related to the regularization parameter , which is its inverse Decreasing the value of the inverse regularization parameter C means that we are increasing the regularization strength COMP 3314 43 Regularization Lets plot the L2-regularization path for the two weight coefficients We fit ten logistic regression models with different values for C For the purposes of illustration, consider only class 1 COMP 3314 44 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 45 Support Vector Machine (SVM) Powerful and widely used learning algorithm Can be considered an extension of the perceptron Perceptron: Minimized misclassification errors SVM: Maximize margin The margin is the distance between the separating hyperplane (decision boundary) and the training samples that are closest to this hyperplane, which are the so-called support vectors Original idea based on paper from 1963 by Vladimir Vapnik Extended by Vladimir Vapnik in 1992 and 1995 SVMs are used to solve various real-world problems Text categorization, classification of images, handwritten character recognition, Protein classification, ... COMP 3314 46 Which Hyperplane ?", "mimetype": "text/plain", "start_char_idx": 8073, "end_char_idx": 10424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb4f7723-5bab-48a7-a8a4-bc2d144ae98f": {"__data__": {"id_": "cb4f7723-5bab-48a7-a8a4-bc2d144ae98f", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f469c08-5923-4b97-9183-b507e2147d68", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "425828ac2f922efcd3609d1a9e2058675433bc1f7bc71bda0a72ecd7fa124a49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98331acb-520d-4c88-bf33-ba12ed2d1c05", "node_type": "1", "metadata": {}, "hash": "15496140ad9a77dc2a6ec6e04a6b11e9073704eb3e4ecb793d7ec7fcf9e37cea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "which are the so-called support vectors Original idea based on paper from 1963 by Vladimir Vapnik Extended by Vladimir Vapnik in 1992 and 1995 SVMs are used to solve various real-world problems Text categorization, classification of images, handwritten character recognition, Protein classification, ... COMP 3314 46 Which Hyperplane ? Sample from positive class Sample from negative class X 2 Potential decision boundary X 1 COMP 3314 47 SVM: Maximize Margin Sample from positive class Sample from negative class X 2 Decision boundary X 1 COMP 3314 48 Large Margin Classification Fitting the widest possible street is called large margin classification Notice that adding more training instances off the street will not affect the decision boundary at all It is fully determined (or supported) by samples located on the edge of the street These instances are called the support vectors COMP 3314 49 SVM: Decision Rule Sample from positive class X Sample from negative class 2 Use scalar projection: w x c Decision Rule: wTx + w 0 then positive class 0 x w X 1 COMP 3314 50 SVM: More Constraints Sample from positive class x pos X Sample from negative class x 2 neg wTx + w 1 pos 0 wTx + w -1 neg 0 y(i) (wTx + w ) 1 pos 0 y(i) (wTx + w ) 1 x neg 0 w y(i) (wTx + w ) -1 0 0 X 1 COMP 3314 51 SVM: More Constraints Sample from positive class x pos X Sample from negative class x 2 neg Constraint from previous slide: y(i) (wTx + w ) -1 0 0 Additional constraint: y(i) (wTx + w ) -1 = 0 0 at the gutter i.e.", "mimetype": "text/plain", "start_char_idx": 10089, "end_char_idx": 11593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98331acb-520d-4c88-bf33-ba12ed2d1c05": {"__data__": {"id_": "98331acb-520d-4c88-bf33-ba12ed2d1c05", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb4f7723-5bab-48a7-a8a4-bc2d144ae98f", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "140781b6b9189523d59364baa050705b0eefe5c026e4610774ff7559e55d7052", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c733f247-b28b-4d88-872c-4448c08c915d", "node_type": "1", "metadata": {}, "hash": "90d451896468bbe9cf6b174234e8bf9a01a87d4e3f6fd37d524df74f9ac85c86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "y(i) (wTx + w ) -1 = 0 + 0 y(i) (wTx + w ) -1 = 0 - 0 X 1 COMP 3314 52 SVM: Margin X 2 x - x What is the width of the street, + - x + i.e., the margin? x - X 1 COMP 3314 53 SVM: Margin X 2 We can take the scalar projection with the unit vector w / ||w|| Width of the street: x - x w (x - x )T w / ||w|| + - + - X 1 COMP 3314 54 SVM: Margin Width of the street: (x - x )T w / ||w|| X + - 2 = (wTx - wTx ) / ||w|| + - = (1-w + 1+w ) / ||w|| 0 0 = 2 / ||w|| x - x w + - Recall that we had the constraints: y(i) (wTx + w ) -1 = 0 + 0 y(i) (wTx + w ) -1 = 0 - 0 i.e., wTx = 1 - w + 0 X 1 wTx = -1 - w - 0 COMP 3314 55 SVM: Optimization The objective function of the SVM is the maximization of the margin 2 / ||w|| Equivalently we can minimize the reciprocal term ||w|| COMP 3314 56 SVM: Optimization For mathematical convenience we solve the following arg min w subject to the constraints This can be minimized efficiently by quadratic programming More details can be found here The Nature of Statistical Learning Theory Vladimir Vapnik, 2000 A Tutorial on Support Vector Machines for Pattern Recognition Data Mining and Knowledge Discovery, 1998 It can be shown that the search space is convex The optimization will not get stuck in a local minima COMP 3314 57 Scaling SVMs are sensitive to the feature scales COMP 3314 58 Hard vs.", "mimetype": "text/plain", "start_char_idx": 11594, "end_char_idx": 12921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c733f247-b28b-4d88-872c-4448c08c915d": {"__data__": {"id_": "c733f247-b28b-4d88-872c-4448c08c915d", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98331acb-520d-4c88-bf33-ba12ed2d1c05", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "f0da2e8d0d2e2ece43cb540e81d4f036b3190d3b066e01ad62fe75762b83d1e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a644b85b-2c37-4a87-9732-ef9c73655f74", "node_type": "1", "metadata": {}, "hash": "e7a15ddc5a2e83f902aa52c163cec121efcdc1d191df538d4305bf92252bfb11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Soft If we strictly impose that all instances must be off the street and on the right side, this is called hard margin classification There are two main issues with hard margin classification First, it only works if the data is linearly separable Second, it is sensitive to outliers COMP 3314 59 Hard vs. Soft To avoid these issues, use a more flexible model The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations I.e., instances that end up in the middle of the street or even on the wrong side This is called soft margin classification. COMP 3314 60 Slack Variables A slack variable can be introduced Positive (or zero) Allows for convergence in the presence of misclassifications For nonlinearly separable data Soft margin COMP 3314 61 If Data is not Linearly Separable Introduce Penalty How to penalize? arg min Idea: ||w||2 + C (# mistakes) w Not all mistakes are equally bad. Use margin to penalize the mistakes.", "mimetype": "text/plain", "start_char_idx": 12922, "end_char_idx": 13908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a644b85b-2c37-4a87-9732-ef9c73655f74": {"__data__": {"id_": "a644b85b-2c37-4a87-9732-ef9c73655f74", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c733f247-b28b-4d88-872c-4448c08c915d", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "e077c9ead13a01292a26ec6c38e146965d5d7563355e0fdb49c5921c1ef2da18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40428c82-f883-4bc5-8b8e-d1da59b86d88", "node_type": "1", "metadata": {}, "hash": "9b3463fcd13173fb868e38b66047e8bc24ba4304cfb73965b6f3f2eb15140862", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "COMP 3314 60 Slack Variables A slack variable can be introduced Positive (or zero) Allows for convergence in the presence of misclassifications For nonlinearly separable data Soft margin COMP 3314 61 If Data is not Linearly Separable Introduce Penalty How to penalize? arg min Idea: ||w||2 + C (# mistakes) w Not all mistakes are equally bad. Use margin to penalize the mistakes. COMP 3314 62 If Data is not Linearly Separable Introduce Penalty Introduce slack variable (i) If point x(i) is on the wrong side then get penalty (i) (i) arg min Idea v2: (j) w, (i) Under the following constraints y(i) (wTx(i) + w ) 1- (i) 0 COMP 3314 63 Slack Penalty C New optimization problem: arg min w, (i) Via the variable C, we can control the penalty for misclassification Large values of C correspond to large error penalties, whereas we are less strict about misclassification errors if we choose smaller values for C This concept is related to regularization Decreasing the value of C increases the bias and lowers the variance of the model COMP 3314 64 Hinge Loss Function The preceding formulation of the SVM cost function was in the so called QP form An equivalent formulation uses the hinge loss function max( 0, 1 - y(i) ( wTx(i) + w ) ) 0 The hinge loss function returns 0 if the point is correctly classified If the point is on the wrong side of the margin, the function's value is proportional to the distance from the margin COMP 3314 65 SVM Natural Form SVM in the natural form: arg min w,w 0 Hinge Loss vs. 0\\1 Loss y t l a n e p z = y( wTx + w ) 0 1 0 COMP 3314 66 Code - SVM.ipynb Available here on CoLab COMP 3314 67 COMP 3314 68 Logistic Regression vs.", "mimetype": "text/plain", "start_char_idx": 13529, "end_char_idx": 15187, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40428c82-f883-4bc5-8b8e-d1da59b86d88": {"__data__": {"id_": "40428c82-f883-4bc5-8b8e-d1da59b86d88", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a644b85b-2c37-4a87-9732-ef9c73655f74", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "16a3edb6a7792165351543878be0f7baf8496fbe14a171d7c614915710e7c6e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a47bc34-2be0-4c6e-bf1f-5c510fe57264", "node_type": "1", "metadata": {}, "hash": "10d8212416c6ec1047d758bbd5271bbe4022f5958c90f68bac44f4e32a3d1d88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0\\1 Loss y t l a n e p z = y( wTx + w ) 0 1 0 COMP 3314 66 Code - SVM.ipynb Available here on CoLab COMP 3314 67 COMP 3314 68 Logistic Regression vs. SVM Linear logistic regression and linear SVMs often yield similar results Logistic regression tries to maximize the conditional likelihoods of the training data, which makes it more prone to outliers than SVMs, which mostly care about the points that are closest to the decision boundary (support vectors) On the other hand, logistic regression has the advantage that it is a simpler model and can be implemented more easily Furthermore, logistic regression models can be easily updated, which is attractive when working with streaming data COMP 3314 69 Alternative Scikit-learn Implementations We used the LIBLINEAR library (via scikit-learn library's Perceptron and LogisticRegression classes) in the previous section A highly optimized C/C++ library developed at NTU Similarly, the SVC makes use of LIBSVM, which is an equivalent C/C++ library specialized for SVMs The above libraries are very fast, however, if your dataset cannot fit into memory you may use an alternative implementation available in SGDClassifier Supports online learning via the partial_fit method COMP 3314 70 Kernel SVM SVM can be kernelized to solve nonlinear classification problems Lets create a small data set of linearly inseparable data COMP 3314 71 Kernel SVM - Mapping Function Basic idea Deal with linearly inseparable data by creating nonlinear combinations of the original features by projecting them onto a higher-dimensional space where it becomes linearly separable Example We can transform a 2D dataset onto a new 3D feature space where the classes become separable via the following projection COMP 3314 72 Kernel SVM One problem with this mapping approach is that the construction of the new features is computationally very expensive, especially if we are dealing with high-dimensional data This is where the so-called kernel trick comes into play In practice all we need is to replace the dot product In order to save the expensive step of calculating the dot product between two points explicitly,", "mimetype": "text/plain", "start_char_idx": 15038, "end_char_idx": 17182, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a47bc34-2be0-4c6e-bf1f-5c510fe57264": {"__data__": {"id_": "0a47bc34-2be0-4c6e-bf1f-5c510fe57264", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40428c82-f883-4bc5-8b8e-d1da59b86d88", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "b48d3c65319ca4a394eb0348e87ef4fb5f4a8ee8665c6cbe4b9b728b75d44641", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c23c418-30ba-4b6d-944c-668c5cf67b3a", "node_type": "1", "metadata": {}, "hash": "39bc2c34069ac88dc60503978fb92a4ef6eda41b8f0b5b5fea8de9c1a28329e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "especially if we are dealing with high-dimensional data This is where the so-called kernel trick comes into play In practice all we need is to replace the dot product In order to save the expensive step of calculating the dot product between two points explicitly, we define a so-called kernel function On of the most widely used kernels is the Gaussian kernel (aka radial basis function) where is a free parameter COMP 3314 73 COMP 3314 74 Gamma The gamma parameter can be understood as a cut-off parameter for the Gaussian If we increase the value for gamma, we increase the influence or reach of the training samples, which leads to a tighter and bumpier decision boundary To get a better intuition for gamma, let us apply an RBF kernel SVM to our Iris flower dataset COMP 3314 75 COMP 3314 76 COMP 3314 77 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 78 Decision Tree Learning Decision tree classifiers are attractive models if we care about interpretability This model breaks down our data by making a decision based on asking a series of questions Example of decision tree COMP 3314 79 Decision Tree Learning Model learns to ask a series of questions E.g.: Is sepal width 2.8 cm? Prediction is based on answers to questions What questions to ask? Split data on feature that results in largest Information Gain (IG) Iterative repeat splitting until leaves are pure I.e., samples all belong to same class This can result in a very deep tree with many nodes, which can easily lead to overfitting Thus, we typically want to prune the tree by setting a limit for the maximal depth of the tree COMP 3314 80 Decision Tree Learning - Intuition Training data: Color Diameter Label Green 3 Apple Yellow 3 Apple Red 1 Grape Red 1 Grape Yellow 3 Lemon features COMP 3314 81 Decision Tree Learning - Intuition Color Diam Label Green 3 Apple Yellow 3 Apple Is diameter >= 3 ?", "mimetype": "text/plain", "start_char_idx": 16918, "end_char_idx": 18976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c23c418-30ba-4b6d-944c-668c5cf67b3a": {"__data__": {"id_": "6c23c418-30ba-4b6d-944c-668c5cf67b3a", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a47bc34-2be0-4c6e-bf1f-5c510fe57264", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "2d662bd9fedee0c7a889828d96fb778fb35c1d0b3301f47bd4bf1abe8f954332", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8cd049f-1543-4814-b063-820f58eb10dd", "node_type": "1", "metadata": {}, "hash": "f58e3d8f328d5013b2e0fe602dd5d6d2c5a2e2c8bc39d2edd16fed426672032e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Red 1 Grape Color Diam Label Red 1 Grape Green 3 Apple mixed Yellow 3 Lemon F T Yellow 3 Apple Yellow 3 Lemon Color Diam Label unmixed Is color == Yellow ? Red 1 Grape Red 1 Grape Predict grape 100% F T Color Diam Label Color Diam Label Yellow 3 Apple Green 3 Apple Yellow 3 Lemon Predict apple 50% Predict apple 100% Predict lemon 50% COMP 3314 82 Which questions to ask and when? Quantify how much a question helps to unmix the labels 1. Quantify the amount of uncertainty at a single node E.g. Gini Impurity 2. How much does a question reduce the uncertainty We aim to maximize Information Gain COMP 3314 83 1. Gini Impurity Chance of being incorrect if you randomly assign a label to an example in the same set p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 84 1. Gini Impurity Color Diam Label Green 3 Apple Gini Impurity: Yellow 3 Apple Is diameter >= 3 ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Color Diam Label Gini Impurity: Green 3 Apple I G = 0 R R e e d d 1 1 G G r r a a p p e e Yellow 3 Apple I G = 0.44 Yellow 3 Lemon p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 85 1. Gini Impurity Color Diam Label Green 3 Apple Gini Impurity: Yellow 3 Apple Is color == green ?", "mimetype": "text/plain", "start_char_idx": 18977, "end_char_idx": 20311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8cd049f-1543-4814-b063-820f58eb10dd": {"__data__": {"id_": "a8cd049f-1543-4814-b063-820f58eb10dd", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c23c418-30ba-4b6d-944c-668c5cf67b3a", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "dd6a44c243a4bfaa8623101f3a1b8ceef509b7b386b13c472e75724b3f4e8b3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e38f3751-920f-426a-8cc3-d78c98dd4c6c", "node_type": "1", "metadata": {}, "hash": "b388aa7011b818f0599197d8a9a906d777c67b6a9d599923742d1ee3fab597e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Gini Impurity Color Diam Label Green 3 Apple Gini Impurity: Yellow 3 Apple Is color == green ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Color Diam Label Gini Impurity: Yellow 3 Apple I = 0 Green 3 Apple G Red 1 Grape I = 0.625 G Red 1 Grape Yellow 3 Lemon p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 86 1. Gini Impurity Color Diam Label Green 3 Apple Gini Impurity: Yellow 3 Apple Is color == yellow ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Green 3 Apple Color Diam Label Gini Impurity: I = 0.44 Red 1 Grape Yellow 3 Apple I = 0.5 G Yellow 3 Lemon G Red 1 Grape p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 87 Which questions to ask and when? Quantify how much a question helps to unmix the labels 1. Quantify the amount of uncertainty at a single node E.g. Gini Impurity 2. How much does a question reduce the uncertainty We aim to maximize Information Gain COMP 3314 88 2. Information Gain Information Gain: Color Diam Label IG = 0.376 Green 3 Apple Gini Impurity: Yellow 3 Apple Is diameter >= 3 ?", "mimetype": "text/plain", "start_char_idx": 20217, "end_char_idx": 21422, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e38f3751-920f-426a-8cc3-d78c98dd4c6c": {"__data__": {"id_": "e38f3751-920f-426a-8cc3-d78c98dd4c6c", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8cd049f-1543-4814-b063-820f58eb10dd", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "4ac7bb83e7279077619bbd481be315f0404277bd5239762794f568b93405527a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a29d838-d68b-43f6-8366-91d0710aa648", "node_type": "1", "metadata": {}, "hash": "93bda6a2e703751d101a7a4f504f6e0bec85128abbd88a8c94647e00d6aa8512", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Quantify how much a question helps to unmix the labels 1. Quantify the amount of uncertainty at a single node E.g. Gini Impurity 2. How much does a question reduce the uncertainty We aim to maximize Information Gain COMP 3314 88 2. Information Gain Information Gain: Color Diam Label IG = 0.376 Green 3 Apple Gini Impurity: Yellow 3 Apple Is diameter >= 3 ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Color Diam Label Gini Impurity: Green 3 Apple I G = 0 R R e e d d 1 1 G G r r a a p p e e Yellow 3 Apple I G = 0.44 Yellow 3 Lemon COMP 3314 89 2. Information Gain - Task Information Gain: Color Diam Label IG = ? Green 3 Apple Gini Impurity: Yellow 3 Apple Color == green ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Gini Impurity: Gini Impurity: I = ? I = ? G G COMP 3314 90 2. Information Gain Information Gain: Color Diam Label IG = 0.176 Green 3 Apple Gini Impurity: Yellow 3 Apple Is color == yellow ? I = 0.64 Red 1 Grape G Red 1 Grape Yellow 3 Lemon F T Color Diam Label Gini Impurity: Green 3 Apple Color Diam Label Gini Impurity: I = 0.44 Red 1 Grape Yellow 3 Apple I = 0.5 G Yellow 3 Lemon G Red 1 Grape COMP 3314 91 Example Information Gain: Color Diam Label IG = 0.376 Green 3 Apple Gini Impurity: Yellow 3 Apple Is diameter >= 3 ?", "mimetype": "text/plain", "start_char_idx": 21065, "end_char_idx": 22363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a29d838-d68b-43f6-8366-91d0710aa648": {"__data__": {"id_": "8a29d838-d68b-43f6-8366-91d0710aa648", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e38f3751-920f-426a-8cc3-d78c98dd4c6c", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "9f95c8df11611acad04426138e9f16373874861af89576b835036885ff82c907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5df967b8-ef3b-42f8-8c37-3754c86d6820", "node_type": "1", "metadata": {}, "hash": "bc23cfdc51996b7ed4d5655fec6fc6cb70c90154d2cd16ef71bd930df74eea2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I = 0.64 Red 1 Grape Color Diam Label G Gini Impurity: Red 1 Grape Green 3 Apple Yellow 3 Lemon F T Yellow 3 Apple I = 0.44 G Yellow 3 Lemon Gini Impurity: Color Diam Label Information Gain: Is color == Yellow ? I = 0 Red 1 Grape G IG = 0.11 Red 1 Grape F T Color Diam Label Color Diam Label Yellow 3 Apple Green 3 Apple Yellow 3 Lemon Gini Impurity: Gini Impurity: I = 0 I = 0.5 G G COMP 3314 92 Maximizing Information Gain Split nodes at most informative features In our tree learning algorithm we optimize an objective function E.g., maximize information gain at each split f is the feature to perform the split D and D are the dataset of the parent and jth child node p j I is our impurity measure N is the total number of samples at the parent node, and p N is the number of samples in the jth child node. j COMP 3314 93 Binary Decision Tree For binary decision trees each parent node is split into two child nodes D and D left right The following impurity measures or splitting criteria are commonly used in binary decision trees Gini impurity ( I ), G Entropy ( I ), and H Classification error ( I ) E COMP 3314 94 Entropy p( i | t ) is the proportion of the samples that belong to class i for a particular node t The entropy is therefore 0 if all samples at a node belong to the same class, and the entropy is maximal if we have a uniform class distribution The entropy criterion attempts to maximize the mutual information in the tree p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 95 Entropy For example,", "mimetype": "text/plain", "start_char_idx": 22364, "end_char_idx": 23932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5df967b8-ef3b-42f8-8c37-3754c86d6820": {"__data__": {"id_": "5df967b8-ef3b-42f8-8c37-3754c86d6820", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a29d838-d68b-43f6-8366-91d0710aa648", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "414e66a54f56b95428d882f90b70b221ea18e56381461159d510678db2658943", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84e2322f-2668-4411-ba38-df18b0bf3e3a", "node_type": "1", "metadata": {}, "hash": "d364b7d05dca9dffd78d88379a7373e14d44ecc1f5bf0c80ab7c22cfff79f15c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and the entropy is maximal if we have a uniform class distribution The entropy criterion attempts to maximize the mutual information in the tree p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 95 Entropy For example, in a binary class setting ( c = 2 ) The entropy is 0 if p ( i = 1| t ) = 1 or p ( i = 0 | t ) = 0 If the classes are distributed uniformly with p ( i = 1| t ) = 0.5 and p ( i = 0 | t ) = 0.5 , the entropy is 1 p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 96 Gini The Gini impurity can be understood as a criterion to minimize the probability of misclassification Similar to entropy, the Gini impurity is maximal if the classes are perfectly mixed, for example, in a binary class setting ( c = 2 ) p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 97 Classification Error Another impurity measure is the classification error This is a useful criterion for pruning but not recommended for growing a decision tree, since it is less sensitive to changes in the class probabilities of the nodes p( i | t ) is the proportion of the samples that belong to class i for a particular node t COMP 3314 98 Example IG E p( i | t ) is the proportion of the samples that belong to class i for a particular node t Consider the two possible splitting scenarios shown in the figure Lets calculate the information gain using the classification error I E as a splitting criterion I (D ) = 0.5 E p Case A: Case B: I (D ) = 0.25 I (D ) = 0.33 E left E left I (D ) = 0.25 I (D ) = 0 E right E right IG = 0.25 IG = 0.", "mimetype": "text/plain", "start_char_idx": 23663, "end_char_idx": 25333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84e2322f-2668-4411-ba38-df18b0bf3e3a": {"__data__": {"id_": "84e2322f-2668-4411-ba38-df18b0bf3e3a", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5df967b8-ef3b-42f8-8c37-3754c86d6820", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "02fab0b8a059f8572c1744c82bca180a73c4aaeb8d5f03bdb88730d686b6e38a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ae28f98-82a7-46fa-a829-ce3bffbfd8f3", "node_type": "1", "metadata": {}, "hash": "aadbf3769c4a485200629e09aa5caa23597e8ae7c1388518adc019c7faf230df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 E p Case A: Case B: I (D ) = 0.25 I (D ) = 0.33 E left E left I (D ) = 0.25 I (D ) = 0 E right E right IG = 0.25 IG = 0.25 E E COMP 3314 99 Example IG G p( i | t ) is the proportion of the samples that belong to class i for a particular node t Consider the two possible splitting scenarios shown in the figure Lets calculate the information gain using the gini impurity I as a G splitting criterion I (D ) = 0.5 G p Case A: Case B: I (D ) = 0.375 I (D ) = 0.44 G left G left I (D ) = 0.375 I (D ) = 0 G right G right IG = 0.125 IG = 0.17 G G COMP 3314 100 Example IG H p( i | t ) is the proportion of the samples that belong to class i for a particular node t Consider the two possible splitting scenarios shown in the figure Lets calculate the information gain using the entropy I as a H splitting criterion I (D ) = 1 H p Case A: Case B: I (D ) = 0.81 I (D ) = 0.92 H left H left I (D ) = 0.81 I (D ) = 0 H right H right IG = 0.19 IG = 0.31 H H COMP 3314 101 Code - DecisionTrees.ipynb Available here on CoLab COMP 3314 102 COMP 3314 103 Building a Decision Tree Decision trees can build complex decision boundaries by dividing the feature space into rectangles However, we have to be careful since the deeper the decision tree, the more complex the decision boundary becomes, which can easily result in overfitting Using scikit-learn, we will now train a decision tree with a maximum depth of 4, using Gini Impurity as a criterion for impurity Although feature scaling may be desired for visualization purposes,", "mimetype": "text/plain", "start_char_idx": 25211, "end_char_idx": 26727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ae28f98-82a7-46fa-a829-ce3bffbfd8f3": {"__data__": {"id_": "3ae28f98-82a7-46fa-a829-ce3bffbfd8f3", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84e2322f-2668-4411-ba38-df18b0bf3e3a", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "8bee269efeb7a29fa6ce8a678470e5985d626427f431096a95595e3f58e989f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3085e786-0751-4570-b0ca-670512804737", "node_type": "1", "metadata": {}, "hash": "45ed5ee48d348e154ca0b44f02b464459f08ee494ca3b4769d9eb898c7f47998", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we have to be careful since the deeper the decision tree, the more complex the decision boundary becomes, which can easily result in overfitting Using scikit-learn, we will now train a decision tree with a maximum depth of 4, using Gini Impurity as a criterion for impurity Although feature scaling may be desired for visualization purposes, note that feature scaling is not a requirement for decision tree algorithms COMP 3314 104 COMP 3314 105 Visualizing Decision Tree The following code will create an image of our decision tree in PNG format For this to work you may have to install sudo apt-get install graphviz pip install pydotplus COMP 3314 106 Random Forests A random forest can be considered as an ensemble of decision trees The idea behind a random forest is to average multiple (deep) decision trees that individually suffer from high variance, to build a more robust model that has a better generalization performance and is less susceptible to overfitting COMP 3314 107 Random Forests Creation 1. Draw a random bootstrap sample of size n (i.e., randomly choose n samples from the training set with replacement) 2. Grow a decision tree from the bootstrap sample. At each node: a. Randomly select d features without replacement b. Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain 3. Repeat steps 1. and 2. k times 4. Aggregate the prediction by each tree to assign the class label by majority vote COMP 3314 108 Random Forests Intuition COMP 3314 109 Bootstrap Sample Size Decreasing the size of the bootstrap samples (i.e., decreasing n) increases the diversity among the individual trees The probability that a particular training sample is included in the bootstrap sample is lower This increases the randomness of the random forest Helps to reduce the effect of overfitting However, smaller bootstrap samples typically result in a lower overall performance of the random forest, a small gap between training and test performance, but a low test performance overall Conversely,", "mimetype": "text/plain", "start_char_idx": 26386, "end_char_idx": 28475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3085e786-0751-4570-b0ca-670512804737": {"__data__": {"id_": "3085e786-0751-4570-b0ca-670512804737", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ae28f98-82a7-46fa-a829-ce3bffbfd8f3", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "665fba9e1f0d8d34d0776c6e108012bb90d4c49d8120615078d9ea42339a32a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2eb7d8ee-aef3-490e-bc0a-c46f1aced4c2", "node_type": "1", "metadata": {}, "hash": "84fb8438e07fdd3b4acdb50dc5e5190e2ec810368f056ed55ece677b9fc09883", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "e., decreasing n) increases the diversity among the individual trees The probability that a particular training sample is included in the bootstrap sample is lower This increases the randomness of the random forest Helps to reduce the effect of overfitting However, smaller bootstrap samples typically result in a lower overall performance of the random forest, a small gap between training and test performance, but a low test performance overall Conversely, increasing the size of the bootstrap sample may increase the degree of overfitting The bootstrap samples, and consequently the individual decision trees, become more similar to each other, they learn to fit the original training dataset more closely COMP 3314 110 COMP 3314 111 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 112 KNN Algorithm The k-nearest neighbor (KNN) classifier is fairly straightforward and can be summarized by the following steps Choose the number of k and a distance metric Find the k-nearest neighbors of the sample that we want to classify Assign the class label by majority vote COMP 3314 113 KNN The main advantage of such a memory-based approach is that the classifier immediately adapts as we collect new training data However, the downside is that the computational complexity for classifying new samples grows linearly with the number of samples in the training dataset in the worst-case scenario Unless the dataset has very few dimensions (features) and the algorithm has been implemented using efficient data structures such as KD-trees Furthermore, we can't discard training samples since no training step is involved Storage space can become a challenge if we are working with large datasets COMP 3314 114 Code - KNN.ipynb Available here on CoLab COMP 3314 115 COMP 3314 116 KNN The right choice of k is crucial to find a good balance between overfitting and underfitting We also have to make sure that we choose a distance metric that is appropriate for the features in the dataset Often, a simple Euclidean distance measure is used for real- value samples, for example, the flowers in our Iris dataset,", "mimetype": "text/plain", "start_char_idx": 28016, "end_char_idx": 30306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2eb7d8ee-aef3-490e-bc0a-c46f1aced4c2": {"__data__": {"id_": "2eb7d8ee-aef3-490e-bc0a-c46f1aced4c2", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3085e786-0751-4570-b0ca-670512804737", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "d0b19d7fd8c46dc25d5788da1c651604b846ffff11da83884ac7ce91fddf6f74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3048a5e6-d0e0-41e5-8d39-193e2047a0aa", "node_type": "1", "metadata": {}, "hash": "3942b4c2e324b18ec42a74872ad9c3f1508aa1042127366cfbca086bdd307012", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ipynb Available here on CoLab COMP 3314 115 COMP 3314 116 KNN The right choice of k is crucial to find a good balance between overfitting and underfitting We also have to make sure that we choose a distance metric that is appropriate for the features in the dataset Often, a simple Euclidean distance measure is used for real- value samples, for example, the flowers in our Iris dataset, which have features measured in centimeters. However, if we are using a Euclidean distance measure, it is also important to standardize the data so that each feature contributes equally to the distance COMP 3314 117 KNN The Minkowski distance that we used in the previous code is just a generalization of the Euclidean and Manhattan distance, which can be written as follows It becomes the Euclidean distance if we set the parameter p=2 or the Manhattan distance at p=1 Many other distance metrics are available in scikit-learn and can be provided to the metric parameter COMP 3314 118 Curse of Dimensionality KNN is susceptible to overfitting due to the curse of dimensionality A phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed- size training dataset Even the closest neighbors being too far away in a high- dimensional space to give a good estimate We can use feature selection and dimensionality reduction techniques to help us avoid the curse of dimensionality COMP 3314 119 Outline Introduction to more robust algorithms for classification Logistic Regression Support Vector Machines Decision Trees KNN Examples using the scikit-learn machine learning library Strengths and weaknesses of classifiers COMP 3314 120 References Most materials in this chapter are based on Book Code COMP 3314 121 References Some materials in this chapter are based on Book Code COMP 3314 122 References 16. Learning: Support Vector Machines MIT 6.034 Artificial Intelligence, Fall 2010 View the complete course: http://ocw.mit.edu/6-034F10 Instructor: Patrick Winston In this lecture, we explore support vector machines in some mathematical detail.", "mimetype": "text/plain", "start_char_idx": 29919, "end_char_idx": 32005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3048a5e6-d0e0-41e5-8d39-193e2047a0aa": {"__data__": {"id_": "3048a5e6-d0e0-41e5-8d39-193e2047a0aa", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2eb7d8ee-aef3-490e-bc0a-c46f1aced4c2", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "b8fd0a6d42d33cbea017532f73eff259a877628306d0ea16583859a8f951b4e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbf2f6fb-61ed-4de5-a565-37ec30a96995", "node_type": "1", "metadata": {}, "hash": "c98e77cbabe265c15ff9367aa530bec3fe4f9700f5b182a8695d5df1fba86a37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learning: Support Vector Machines MIT 6.034 Artificial Intelligence, Fall 2010 View the complete course: http://ocw.mit.edu/6-034F10 Instructor: Patrick Winston In this lecture, we explore support vector machines in some mathematical detail. We use Lagrange multipliers to maximize the width of the street given certain constraints. If needed, we transform vectors into another space, using a kernel function. License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu COMP 3314 123 References 7 4 Soft Margin SVMs 9 46 COMP 3314 124 References CS229 Lecture notes COMP 3314 125 References Introduction to Machine Learning, Third Edition Ethem Alpaydin COMP 3314 126 References Logistic Regression: From Introductory to Advanced Concepts and Applications Scott Menard, 2010 COMP 3314 127 References Lets Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8 Hey everyone! Glad to be back! Decision Tree classifiers are intuitive, interpretable, and one of my favorite supervised learning algorithms. In this episode, Ill walk you through writing a Decision Tree classifier from scratch, in pure Python. Ill introduce concepts including Decision Tree Learning, Gini Impurity, and Information Gain. Then, well code it all up. Understanding how to accomplish this was helpful to me when I studied Machine Learning for the first time, and I hope it will prove useful to you as well. You can find the code from this video here: https://goo.gl/UdZoNr https://goo.gl/ZpWYzt Books!", "mimetype": "text/plain", "start_char_idx": 31764, "end_char_idx": 33315, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bbf2f6fb-61ed-4de5-a565-37ec30a96995": {"__data__": {"id_": "bbf2f6fb-61ed-4de5-a565-37ec30a96995", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3048a5e6-d0e0-41e5-8d39-193e2047a0aa", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "39b1a3b7d3a13a079b560ec4eeb86ae014463e91aa3250384de938bb4226ce20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f560cf60-9321-4303-9ae6-4081cd5e245b", "node_type": "1", "metadata": {}, "hash": "fab385c7dd47d78f79887448e90d054dc54544b4d2bf4ec08fd7d967f4c7588e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ill introduce concepts including Decision Tree Learning, Gini Impurity, and Information Gain. Then, well code it all up. Understanding how to accomplish this was helpful to me when I studied Machine Learning for the first time, and I hope it will prove useful to you as well. You can find the code from this video here: https://goo.gl/UdZoNr https://goo.gl/ZpWYzt Books! Hands-On Machine Learning with Scikit-Learn and TensorFlow https://goo.gl/kM0anQ Follow Josh on Twitter: https://twitter.com/random_forests Check out more Machine Learning Recipes here: https://goo.gl/KewA03 Subscribe to the Google Developers channel: http://goo.gl/mQyv5L COMP 3314 128 References Random Forest Algorithm - Random Forest Explained | Random Forest in Machine Learning | Simplilearn This Random Forest Algorithm tutorial will explain how Random Forest algorithm works in Machine Learning. By the end of this video, you will be able to understand what is Machine Learning, what is Classification problem, applications of Random Forest, why we need Random Forest, how it works with simple examples and how to implement Random Forest algorithm in Python. Below are the topics covered in this Machine Learning tutorial: 1. What is Machine Learning? 2. Applications of Random Forest 3. What is Classification? 4. Why Random Forest? 5. Random Forest and Decision Tree 6.", "mimetype": "text/plain", "start_char_idx": 32945, "end_char_idx": 34295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f560cf60-9321-4303-9ae6-4081cd5e245b": {"__data__": {"id_": "f560cf60-9321-4303-9ae6-4081cd5e245b", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbf2f6fb-61ed-4de5-a565-37ec30a96995", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "2415dcbdbae9b46d3dcf8ffab8fc3ec467e470f96aec4e3f49bd8ac8630d2add", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8238366b-4349-4ab0-a06f-f75b88fc9f58", "node_type": "1", "metadata": {}, "hash": "3390aa2f7154b91be297136e4305ebf12d088b8e809bb3ffa1975988563a687f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Below are the topics covered in this Machine Learning tutorial: 1. What is Machine Learning? 2. Applications of Random Forest 3. What is Classification? 4. Why Random Forest? 5. Random Forest and Decision Tree 6. Use case - Iris Flower Analysis Subscribe to our channel for more Machine Learning Tutorials: https://www.youtube.com/user/Simplilearn?sub_confirmation=1 You can also go through the Slides here: https://goo.gl/K8T4tW Machine Learning Articles: https://www.simplilearn.com/what-is-artificial-intelligence-and-why-ai-certification-article?utm_campaign=Random-Forest-Tutorial-eM4uJ6XGnSMutm_medium=Tutorialsutm_source=youtube To gain in-depth knowledge of Machine Learning, check our Machine Learning certification training course: https://www.simplilearn.com/big-data-and-analytics/machine-learning-certification-training-course?utm_campaign=Random-Forest-Tutorial-eM4uJ6XGnSMutm_medium=Tutorialsutm_source=youtube #MachineLearningAlgorithms #Datasciencecourse #DataScience #SimplilearnMachineLearning #MachineLearningCourse - - - - - - - - About Simplilearn Machine Learning course: A form of artificial intelligence, Machine Learning is revolutionizing the world of computing as well as all peoples digital interactions. COMP 3314 129 Exercise 1 What is the fundamental idea behind Support Vector Machines? What is a support vector? Why is it important to scale the inputs when using SVMs? Can an SVM classifier output a confidence score when it classifies an instance? What about a probability? Say youve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease (gamma)? What about C? COMP 3314 130 Exercise 2 Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.", "mimetype": "text/plain", "start_char_idx": 34083, "end_char_idx": 35939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8238366b-4349-4ab0-a06f-f75b88fc9f58": {"__data__": {"id_": "8238366b-4349-4ab0-a06f-f75b88fc9f58", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f560cf60-9321-4303-9ae6-4081cd5e245b", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "b299fde2434136c4d5e8a02f4c4d0c521e79954b4e08d6c2c02caf11f76a8d1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69f7e520-1fcf-4a66-9cf1-d858caf4bea4", "node_type": "1", "metadata": {}, "hash": "0b997def0116270355cf95afa5d2825668c5d26d0b0b8a0f5f3bcb9b6e4d04c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What about a probability? Say youve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease (gamma)? What about C? COMP 3314 130 Exercise 2 Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus- the-rest to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach? COMP 3314 131 Exercise 3 What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances? Is a nodes Gini impurity generally lower or greater than its parents? Is it generally lower/greater, or always lower/greater? If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth? If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features? If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances? COMP 3314 132 Exercise 4 Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set Hint: the KNeighborsClassifier works quite well for this task; you just need to find good hyperparameter values (try a grid search on the weights and n_neighbors hyperparameters) COMP 3314 133 Exercise 5 Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel.", "mimetype": "text/plain", "start_char_idx": 35566, "end_char_idx": 37348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69f7e520-1fcf-4a66-9cf1-d858caf4bea4": {"__data__": {"id_": "69f7e520-1fcf-4a66-9cf1-d858caf4bea4", "embedding": null, "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09dba5a8-fbbc-4862-a3eb-f62ded418ca6", "node_type": "4", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "0bde72ec6c2d1b5fb9c136441beb0aedb6b427de2858992614a02027fd207baf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8238366b-4349-4ab0-a06f-f75b88fc9f58", "node_type": "1", "metadata": {"file_name": "3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt", "source_file": "3. Logistic Regression, SVM, Decision Trees, KNN", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\3. Logistic Regression, SVM, Decision Trees, KNN_processed.txt"}, "hash": "d08b8fed299b193a69d0187087e815981a40675dad9fc05b5c9f48b9bc0ba061", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set Finally, train your best model on this expanded training set and measure its accuracy on the test set You should observe that your model performs even better now This technique of artificially growing the training set is called data augmentation or training set expansion COMP 3314 134 Exercise 6 Tackle the Titanic dataset A great place to start is on Kaggle", "mimetype": "text/plain", "start_char_idx": 37349, "end_char_idx": 37830, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e21fdec2-4b97-46b1-955b-fe51e72cbe6a": {"__data__": {"id_": "e21fdec2-4b97-46b1-955b-fe51e72cbe6a", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfeb49b9-09a2-4016-9be8-33b2c9f93861", "node_type": "1", "metadata": {}, "hash": "c5a906644947cf001b81f6b444158a3005880867bac628d153deeb97a79b8bc5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. Data Preprocessing COMP3314 Machine Learning COMP 3314 2 Introduction Preprocessing a dataset is a crucial step Garbage in, garbage out Quality of data and amount of useful information it contains are key factors Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: 100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Preprocessing is often the most important phase of a machine learning project COMP 3314 3 Outline In this chapter you will learn how to Remove and impute missing values from the dataset Get categorical data into shape Select relevant features Specifically, we will looking at the following topics Dealing with missing data Nominal and ordinal features Partitioning a dataset into training and testing sets Bringing features onto the same scale Selecting meaningful features Sequential feature selection algorithms Random forests COMP 3314 4 Dealing with Missing Data Missing data is common in real-world applications Samples might be missing one or more values ML models are unable to handle this Two ways to handle this Remove entries Imputing missing values from other samples and features COMP 3314 5 Code - DataPreprocessing.ipynb Available here on CoLab COMP 3314 6 Identifying Missing Values Consider the following simple example generated from CSV COMP 3314 7 Identifying Missing Values For larger data, it can be tedious to look for missing values Use the isnull method to return a DataFrame with Boolean values that indicate whether a cell contains a numeric value (False),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfeb49b9-09a2-4016-9be8-33b2c9f93861": {"__data__": {"id_": "bfeb49b9-09a2-4016-9be8-33b2c9f93861", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e21fdec2-4b97-46b1-955b-fe51e72cbe6a", "node_type": "1", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "3169819d409daa69b30403e8c0bafc837704ff8b24ce727885b328b500dd51e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6aea7b9-1e40-4ef6-bd5e-e3d843535173", "node_type": "1", "metadata": {}, "hash": "c2661efe0e1d38d372747b3b9e6802a51828645490bd91cc48f4c621afecb277", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ipynb Available here on CoLab COMP 3314 6 Identifying Missing Values Consider the following simple example generated from CSV COMP 3314 7 Identifying Missing Values For larger data, it can be tedious to look for missing values Use the isnull method to return a DataFrame with Boolean values that indicate whether a cell contains a numeric value (False), or if data is missing (True) Use sum() to count the number of missing values per column COMP 3314 8 Remove Missing Data One option is to simply remove the corresponding features (columns) or samples (rows) Rows with missing values can be dropped via the dropna method with argument axis=0 Columns with missing values can be dropped via the dropna method with argument axis=1 COMP 3314 9 Dropna The dropna method supports several additional parameters that can come in handy only drop rows only drop rows where NaN appear where all drop rows that in specific columns columns are have less than 4 (here: 'C') NaN real values COMP 3314 10 Remove Missing Data Convenient approach Disadvantage May remove too many samples Risk losing valuable information Our classifier may need them to discriminate between classes Could make a reliable analysis impossible Alternative approach: Interpolation COMP 3314 11 Interpolation Estimate missing values from the other training samples in our dataset Example: Mean imputation Replace missing value with the mean value of the entire feature column mean and median are for numerical data only, Try to change to: most_frequent and constant can - median be used for numerical data or - most_frequent strings - constant,", "mimetype": "text/plain", "start_char_idx": 1236, "end_char_idx": 2841, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6aea7b9-1e40-4ef6-bd5e-e3d843535173": {"__data__": {"id_": "a6aea7b9-1e40-4ef6-bd5e-e3d843535173", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfeb49b9-09a2-4016-9be8-33b2c9f93861", "node_type": "1", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "5071f53eb10487498581720989bbf0071b523d0772d55f2c47898f85b56d34f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5517e0a7-37ad-44ed-98f4-ff0144280a1c", "node_type": "1", "metadata": {}, "hash": "7b94e32ed7fe6317dca42b1f6aa7a71318c691eb5c143dfefef4090871d32a66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Try to change to: most_frequent and constant can - median be used for numerical data or - most_frequent strings - constant, fill_value=42 COMP 3314 12 Scikit-Learn Estimator API SimpleImputer is a Transformer class Used for data transformation Two essential methods fit transform Estimator class Very similar to transformer class Two essential methods fit predict Transform (optional) COMP 3314 13 Transformer - Fit and Transform fit method Used to learn the parameters from the training data transform method Uses those parameters to transform the data Note: Number of features need to be identical COMP 3314 14 Estimator - Fit and Predict Use fit method to learn parameters Additionally provide class labels Use predict method to make predictions about unlabeled data COMP 3314 15 Handling Categorical Data We have been exclusively working with numerical data How to handle categorical data? A categorical feature can take on one of a limited, and usually Example of categorical data fixed, number of possible values XL L M COMP 3314 16 Categorical Data It is common that real-world datasets contain categorical features How to deal with this type of data? Nominal features vs ordinal features Ordinal features can be sorted / ordered E.g., t-shirt size, because we can define an order XL>L>M Nominal features don't imply any order E.g., t-shirt color COMP 3314 17 Example Dataset nominal ordinal numerical COMP 3314 18 Mapping Ordinal Features To ensure correct interpretation of ordinal features, convert string values to integers Reverse-mapping to go back COMP 3314 19 Encoding Class Labels Most models require integer encoding for class labels Note: class labels are not ordinal, and it doesn't matter which integer number we assign to a particular string label COMP 3314 20 LabelEncoder Alternatively, there is a convenient LabelEncoder class directly implemented in scikit-learn to achieve this Shortcut of calling fit and transform separately COMP 3314 21 One-Hot Encoding We could use a similar approach to transform the nominal color column of our dataset, as follows Problem: Model may assume that green > blue, and red > green This could result in suboptimal model Workaround: Use one-hot encoding Create a dummy feature for each unique value of nominal features E.g.,", "mimetype": "text/plain", "start_char_idx": 2718, "end_char_idx": 5000, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5517e0a7-37ad-44ed-98f4-ff0144280a1c": {"__data__": {"id_": "5517e0a7-37ad-44ed-98f4-ff0144280a1c", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6aea7b9-1e40-4ef6-bd5e-e3d843535173", "node_type": "1", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "2575a985da25c46451da29469028da526867f4745cb5ce8de7e58fca888365bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b289722-756e-4aca-aa0e-af027f5ee4df", "node_type": "1", "metadata": {}, "hash": "9c618e34ee0e39c51cbaba35c0547aea7b758854888126b7205cb67f1d77e69e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "there is a convenient LabelEncoder class directly implemented in scikit-learn to achieve this Shortcut of calling fit and transform separately COMP 3314 21 One-Hot Encoding We could use a similar approach to transform the nominal color column of our dataset, as follows Problem: Model may assume that green > blue, and red > green This could result in suboptimal model Workaround: Use one-hot encoding Create a dummy feature for each unique value of nominal features E.g., a blue sample is encoded as blue = 1 , green = 0 , red = 0 COMP 3314 22 One-Hot Encoding Use the OneHotEncoder available in scikit-learns preprocessing module -1 means unknown dimension and we want numpy to figure it out Apply to only a single column COMP 3314 23 One-Hot Encoding via ColumnTransformer To selectively transform columns in a multi-feature array, use ColumnTransformer Accepts a list of (name, transformer, column(s)) tuple Only modify the first column COMP 3314 24 One-Hot Encoding - Via Pandas An even more convenient way to create those dummy features via one-hot encoding is to use the get_dummies method implemented in pandas get_dummies will only convert string columns COMP 3314 25 One-Hot Encoding - Dropping First Feature Note that we do not lose any information by removing one dummy column E.g., if we remove the column color_blue, the feature information is still preserved since if we observe color_green=0 and color_red=0, it implies that the observation must be blue COMP 3314 26 UCI Wine Dataset The UCI wine dataset consists of 178 wine samples with 13 features describing their different chemical properties COMP 3314 27 UCI Wine Dataset: Training-Testing Lets first divide the dataset into separate training and testing sets COMP 3314 28 UCI Wine Dataset: Training-Testing It is important to balance the trade-off between inaccurate estimation of generalization error and withholding too much information from the learning algorithm In practice, the most commonly used splits are 60:40, 70:30, or 80:20, depending on the size of the initial dataset For large datasets,", "mimetype": "text/plain", "start_char_idx": 4528, "end_char_idx": 6603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b289722-756e-4aca-aa0e-af027f5ee4df": {"__data__": {"id_": "1b289722-756e-4aca-aa0e-af027f5ee4df", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5517e0a7-37ad-44ed-98f4-ff0144280a1c", "node_type": "1", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "f528432bdc9bfeab41da394af4202236da6e094d7726283f43e6df253f2d06e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a275f3c3-b6b9-4b37-9c23-83fa65d9612e", "node_type": "1", "metadata": {}, "hash": "65bbd27fcf907a21e70f0de01e5cce8cbf33b5712d1c43d3507b2cdbcc4ae316", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the most commonly used splits are 60:40, 70:30, or 80:20, depending on the size of the initial dataset For large datasets, 90:10 or 99:1 splits are also common and appropriate Instead of discarding the allocated test data after model training and evaluation, we can retrain a classifier on the entire dataset as it can improve the predictive performance of the model While this approach is generally recommended, it could lead to worse generalization performance COMP 3314 29 Feature Scaling The majority of ML algorithms require feature scaling Decision trees and random forests are two of few ML algorithms that dont require feature scaling Importance Consider the squared error function in Adaline for two dimensional features where one feature is measured on a scale from 1 to 10 and the second feature is measured on a scale from 1 to 100,000 The second feature would contribute to the error with a much higher significance Two common approaches to bring different features onto the same scale Normalization E.g., rescaling features to a range of [0, 1] Standardization E.g., center features at mean 0 with standard deviation 1 COMP 3314 30 Feature Scaling - Normalization Most often, normalization refers to the rescaling of features to a range of [0, 1] To normalize our data, we can simply apply a min-max scaling to each feature column A new value x(i) of a sample x(i) is calculated as follows norm Here x is the smallest value in a feature column and x the largest min max COMP 3314 31 Feature Scaling - Standardization Standardization is more practical for various reasons including retaining useful information about outliers A new value x(i) of a sample x(i) is calculated as follows std Here is the sample mean of feature column and the corresponding standard deviation x x Similar to the MinMaxScaler class, scikit-learn also implements a class for standardization COMP 3314 32 Normalization vs.", "mimetype": "text/plain", "start_char_idx": 6481, "end_char_idx": 8392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a275f3c3-b6b9-4b37-9c23-83fa65d9612e": {"__data__": {"id_": "a275f3c3-b6b9-4b37-9c23-83fa65d9612e", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b289722-756e-4aca-aa0e-af027f5ee4df", "node_type": "1", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "ab7089e4f1859b478a1dacfa3cfd4a743c841aa9607f86ed31009bfe5aba46c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25f52715-070a-4579-8a33-9edfd7aed672", "node_type": "1", "metadata": {}, "hash": "49d1c2d796ca8e4f57ee3725b1795af7273a15f633cc230526643400f82d4ee1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "scikit-learn also implements a class for standardization COMP 3314 32 Normalization vs. Standardization The following example illustrates the difference between standardization and normalization COMP 3314 33 Robust Scaler More advanced methods for feature scaling are available in sklearn The RobustScaler is especially helpful and recommended if working with small datasets that contain many outliers COMP 3314 34 Feature Selection Selects a subset of relevant features Simplify model for easier interpretation Shorten training time Avoid curse of dimensionality Reduce overfitting Feature selection feature extraction (covered in next chapter) Selecting subset of the features creating new features We are going to look a two techniques for feature selection L1 Regularization Sequential Backward Selection (SBS) COMP 3314 35 L1 vs. L2 Regularization L2 regularization (penalty) used in chapter 3 Another approach: L1 regularization (penalty) COMP 3314 36 L1 Regularization Why is L1 regularization a technique for feature selection? COMP 3314 37 The two axis represent the model parameters, previously we used w and w 1 2 COMP 3314 38 Points on the contour have equal cost The background contour represents the L2 regularization term COMP 3314 39 Why are the contours circular? Make sure you can answer this question before you continue COMP 3314 40 and run gradient 0 cost descent Lets initialize The cost decreases our model with linearly with the (2.0, 0.5) ... distance to the origin COMP 3314 41 The background contour represents Adalines cost functions + L2 regularization term (i.e., a combination of both) COMP 3314 42 Initialization This is the path gradient descent takes COMP 3314 43 Points on the contour have equal cost The background contour represents the L1 regularization term COMP 3314 44 Why are the contours diamond shaped ?", "mimetype": "text/plain", "start_char_idx": 8305, "end_char_idx": 10152, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25f52715-070a-4579-8a33-9edfd7aed672": {"__data__": {"id_": "25f52715-070a-4579-8a33-9edfd7aed672", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a275f3c3-b6b9-4b37-9c23-83fa65d9612e", "node_type": "1", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "64977fe58c29df6272e5274ec872179cb5b6821fc997178ad081cff70a6645b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffbab969-e6b3-495a-8490-62113458bede", "node_type": "1", "metadata": {}, "hash": "8a3f37e5f88046d96a0cbc4241d59b3cbf9ec2e74668bc16dc33d41303602d5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Make sure you can answer this question before you continue COMP 3314 45 First the cost and run decreases equally for gradient both parameters descent 0 cost Lets initialize our model with (2.0, 0.5) When one of them is 0 it will then decrease the other one only COMP 3314 46 The background contour represents Adalines cost functions + L1 regularization term (i.e., a combination of both) COMP 3314 47 Notice how the path quickly reaches 0 for one of the parameters Initialization This is the path gradient descent takes COMP 3314 48 To avoid the bouncing around you should gradually reduce the learning rate COMP 3314 49 Sparse Solution We can simply set the penalty parameter to l1 for models in scikit-learn that support L1 regularization In scikit-learn, w corresponds to intercept_ and w (for j > 0) corresponds to the 0 j values in coef_ COMP 3314 50 Sparse Solution - Regularization Strength COMP 3314 51 Sparse Solution - Regularization Strength COMP 3314 52 Sequential Backward Selection (SBS) Reduces an initial d-dimensional space to a k-dimensional subspace (k < d) by automatically selecting features that are most relevant Idea: Sequentially remove features until desired feature number is reached Define a criterion function J to be maximized E.g., performance of the classifier after removal Eliminate the feature that causes the least performance loss COMP 3314 53 SBS Steps: 1. Initialize the algorithm with k = d d is the dimensionality of the full feature space X d 2. Determine the feature x- = argmax J (X - x) that maximizes the criterion k function J 3. Remove the feature x- from the feature set X = X - x- k-1 k k = k - 1 4. Terminate if k equals the number of desired features; otherwise,", "mimetype": "text/plain", "start_char_idx": 10153, "end_char_idx": 11867, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffbab969-e6b3-495a-8490-62113458bede": {"__data__": {"id_": "ffbab969-e6b3-495a-8490-62113458bede", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25f52715-070a-4579-8a33-9edfd7aed672", "node_type": "1", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "16d616f20d7162300bc09c32fea7ae59d1ce4797996bc548866c3dcfd2bd4885", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fab40c41-ceb5-4e01-97a7-75e7b5c86466", "node_type": "1", "metadata": {}, "hash": "28f6828c3fbb44dfbce9bfe20aab0bbac81e15df3bd426b421d92e4108bf027e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Initialize the algorithm with k = d d is the dimensionality of the full feature space X d 2. Determine the feature x- = argmax J (X - x) that maximizes the criterion k function J 3. Remove the feature x- from the feature set X = X - x- k-1 k k = k - 1 4. Terminate if k equals the number of desired features; otherwise, go to step 2 In the following we will implement SBS in Python from scratch COMP 3314 54 COMP 3314 55 COMP 3314 56 SBS - Analyzing the Result The smallest feature subset (k = 3) that yielded such a good performance on the validation dataset has the following features The accuracy of the KNN classifier on the original test set is as follows The three-feature subset has the following accuracy COMP 3314 57 Feature Selection Algorithms in scikit-learn There are many more feature selection algorithms available via scikit-learn A comprehensive discussion of the different feature selection methods is beyond the scope of this lecture A good summary with illustrative examples can be found here COMP 3314 58 Assessing Feature Importance We can determine relevant features using random forest Measure the feature importance as the averaged impurity decrease The random forest implementation in scikit-learn already collects the feature importance values for us Access them via the feature_importances_ attribute after fitting a RandomForestClassifier In the following we will train a forest of 500 trees on the Wine dataset and rank the 13 features by their respective importance measures COMP 3314 59 COMP 3314 60 SelectFromModel scikit-learn implements a SelectFromModel object that selects features based on a user-specified threshold after model fitting Use the RandomForestClassifier as a feature selector and intermediate step in a scikit-learn Pipeline object,", "mimetype": "text/plain", "start_char_idx": 11548, "end_char_idx": 13332, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fab40c41-ceb5-4e01-97a7-75e7b5c86466": {"__data__": {"id_": "fab40c41-ceb5-4e01-97a7-75e7b5c86466", "embedding": null, "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4124fd20-c144-428a-8738-868f0ef176f5", "node_type": "4", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "65a55452a93dff968e8eedcfc57d7ebfc0bede697da1f5f93a89aa3ff5de1796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffbab969-e6b3-495a-8490-62113458bede", "node_type": "1", "metadata": {"file_name": "4. Data Preprocessing_processed.txt", "source_file": "4. Data Preprocessing", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\4. Data Preprocessing_processed.txt"}, "hash": "683ee20af32b0e3822fa2f34ad524d7c1108e4bec7eec05ffde5e15b5e2080e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "which allows us to connect different preprocessing steps with an estimator COMP 3314 61 Feature Extraction Alternative way to reduce the model complexity Feature selection Select a subset of original features Feature extraction Technique to compress a dataset onto a lower-dimensional feature space (dimensionality reduction) Covered in the next chapter COMP 3314 62 Conclusion Handle missing data correctly Encode categorical variables correctly Map ordinal and nominal feature values to integer representations L1 regularization can help us to avoid overfitting by reducing the complexity of a model Used a sequential feature selection algorithm to select meaningful features from a dataset COMP 3314 63 References Most materials in this chapter are based on Book Code COMP 3314 64 References Some materials in this chapter are based on Book Code COMP 3314 65 References The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition Trevor Hastie, Robert Tibshirani, Jerome Friedman https://web.stanford.edu/~hastie/ElemStatLearn/ Pandas User Guide: Working with missing data", "mimetype": "text/plain", "start_char_idx": 13333, "end_char_idx": 14441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6e27028-bf1f-41ed-83d4-fc9ab35ae216": {"__data__": {"id_": "e6e27028-bf1f-41ed-83d4-fc9ab35ae216", "embedding": null, "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98", "node_type": "4", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c2617c9-d271-4bf3-84fc-0ff48804c94f", "node_type": "1", "metadata": {}, "hash": "216d829ec1a3cb1acf6aab3b93f69cb8da265d847e14511a8d16a64b6137611e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. Dimensionality Reduction COMP3314 Machine Learning COMP 3314 2 Motivation Many ML problems have thousands or even millions of features As a result we have an intractable problem Training is slow Finding a solution is difficult (Curse of Dimensionality) Data visualization is impossible Solution Dimensionality Reduction using feature extraction Often possible without losing much relevant information E.g., Merge neighboring pixels of the MNIST dataset COMP 3314 3 The Curse of Dimensionality COMP 3314 4 High Dimensional Weirdness 2D Pick a random point in a unit square will have <0.4% chance of being located <0.001 from a border 10,000D Pick a random point in a unit hypercube will have >99.99999% chance of being located <0.001 from a border I.e., the high-dimensional unit hypercube can be said to consist almost entirely of borders with almost no middle COMP 3314 5 High Dimensional Weirdness 2D Pick two random points in a unit square Distance between them will be 0.52 on average 1,000,000D Pick two random points in a unit hypercube Distance between them will be 408.25 on average How can two points be so far apart when they both lie within the same unit hypercube?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c2617c9-d271-4bf3-84fc-0ff48804c94f": {"__data__": {"id_": "1c2617c9-d271-4bf3-84fc-0ff48804c94f", "embedding": null, "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98", "node_type": "4", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6e27028-bf1f-41ed-83d4-fc9ab35ae216", "node_type": "1", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "501cab8e30e6aaeeac96edd3bf2900c977f6a930cd4786764b18ea1e32faad71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eaeb3e6d-1c89-47ba-9f2f-7eab8c725ebe", "node_type": "1", "metadata": {}, "hash": "a1fe29e70846b13b3ad93e08e96e595588b402711401df25390bb16fbf056960", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a result, new test samples will likely be far away from training samples in high dimensional space Overfitting risk is much higher in high-dimensional space COMP 3314 6 Idea: Projection In most problems, training instances are not spread out uniformly across all dimensions Many features are almost constant, while others are highly correlated As a result, all training instances lie within (or close to) a much lower-dimensional subspace of the high-dimensional space COMP 3314 7 When Projection Fails Projection is not always the best approach Consider the following toy dataset to illustrate this problem The Swiss roll Simply projecting onto a plane (e.g., dropping x3) would squash different layers COMP 3314 8 Solution: Manifold Learning The Swiss roll is an example of 2D manifold A 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space It is possible to learn the manifold on which the training instances lie and then to unroll the swiss roll COMP 3314 9 Manifold Learning Note The decision boundary may not always be simpler in lower dimensions COMP 3314 10 Outline PCA Principal Component Analysis Projects data points onto (few) principal components LLE Locally Linear Embedding Powerful nonlinear dimensionality reduction technique Manifold Learning technique that does not rely on projections COMP 3314 11 PCA - Principal Component Analysis By far the most popular dimensionality reduction algorithm Identifies a hyperplane and then projects data onto it How to choose the hyperplane?", "mimetype": "text/plain", "start_char_idx": 1180, "end_char_idx": 2711, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eaeb3e6d-1c89-47ba-9f2f-7eab8c725ebe": {"__data__": {"id_": "eaeb3e6d-1c89-47ba-9f2f-7eab8c725ebe", "embedding": null, "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98", "node_type": "4", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c2617c9-d271-4bf3-84fc-0ff48804c94f", "node_type": "1", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "690408c14ce633bab9374b894051bc52b6c7a20525bbdffa38b53b272c30adda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0af44f70-2875-4b9c-af12-2b1700a58d73", "node_type": "1", "metadata": {}, "hash": "c5508d4b55ffb44e03b6167d6e37749e3e5ab3f44bb4365955a227044a299342", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "COMP 3314 13 Preserving the Variance Select axis that preserves the maximum amount of variance I.e., loses less information than other projections Example: 1D Hyperplanes Preserves max variance Lets try to project this data onto three different axis Preserves intermediate amount of variance Preserves very little variance COMP 3314 14 Principal Components (PC) The first PC is the axis that accounts for the largest amount of variance E.g., PC1 in the figure The second PC is orthogonal to the first one and accounts for the largest amount of remaining variance E.g., PC2 in the figure In this 2D example there is no choice If it were in a higher-dimensional dataset the third PC would be orthogonal to both previous axes, and a fourth, a fifth, and so onas many axes as the number of dimensions in the dataset COMP 3314 15 How to find PCs? There is a standard matrix factorization technique called Singular Value Decomposition (SVD) It decomposes the training set matrix X into the matrix multiplication of three matrices X = U V, where V contains the unit vectors that define all the principal components that we are looking for Note that PCs are highly sensitive to data scaling We need to standardize the features prior to PCA if the features were measured on different scales COMP 3314 16 PCA - Principal Component Analysis An unsupervised linear transformation technique Finds PCs Using e.g., SVD Projects data onto a subspace with fewer (or equal) dimensions using some (or all) of the found PCs Multiply original data with a transformation matrix that consists of PCs, some (or all) COMP 3314 17 Projecting Down to k Dimensions Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to k dimensions by projecting it onto the hyperplane defined by the first k principal components To project the training set onto the hyperplane and obtain a reduced dataset of dimensionality k, compute the matrix multiplication of the training set vector (or matrix) x (or X) by the matrix W, defined as the matrix containing the first k columns of V W is a d k transformation matrix Maps a d-dimensional vector x to a k-dimensional vector z COMP 3314 18 Code - PCA.", "mimetype": "text/plain", "start_char_idx": 2712, "end_char_idx": 4925, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0af44f70-2875-4b9c-af12-2b1700a58d73": {"__data__": {"id_": "0af44f70-2875-4b9c-af12-2b1700a58d73", "embedding": null, "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98", "node_type": "4", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eaeb3e6d-1c89-47ba-9f2f-7eab8c725ebe", "node_type": "1", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "3535c57d4526adcf2662bd0897697c03c8b7aab2b94c5f3e5504e8a2fa377cce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a026ba11-6d60-41c1-9768-38f26eb85285", "node_type": "1", "metadata": {}, "hash": "7bdd17c2808f03c4b156efb9d17e3bfe0078c73c5f6e38c484b71a431430a990", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "compute the matrix multiplication of the training set vector (or matrix) x (or X) by the matrix W, defined as the matrix containing the first k columns of V W is a d k transformation matrix Maps a d-dimensional vector x to a k-dimensional vector z COMP 3314 18 Code - PCA.ipynb Available here on CoLab COMP 3314 19 Load and Standardize Data Lets apply PCA on the wine dataset Load the wine dataset and split it into separate train and test sets Standardize the (d=13)-dimensional dataset COMP 3314 20 Projecting Down COMP 3314 21 COMP 3314 22 Using Scikit-Learns PCS Scikit-Learns PCA class uses SVD decomposition to implement PCA Just like we did manually The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions COMP 3314 23 COMP 3314 24 Explained Variance Ratio Another useful piece of information is the explained variance ratio of each principal component Available via the explained_variance_ratio_ variable The ratio indicates the proportion of the datasets variance that lies along each principal component This output tells us that 36.9% of the datasets variance lies along the first PC, and 18.4% lies along the second PC, etc COMP 3314 25 Choosing the Right Number of Dimensions Choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%) Unless, of course, you are reducing dimensionality for data visualizationin that case you will want to reduce the dimensionality down to 2 or 3 The following code performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 90% of the training sets variance COMP 3314 26 Choosing the Right Number of Dimensions You could then set n_components=k and run PCA again But there is a much better option: instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0,", "mimetype": "text/plain", "start_char_idx": 4653, "end_char_idx": 6577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a026ba11-6d60-41c1-9768-38f26eb85285": {"__data__": {"id_": "a026ba11-6d60-41c1-9768-38f26eb85285", "embedding": null, "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98", "node_type": "4", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0af44f70-2875-4b9c-af12-2b1700a58d73", "node_type": "1", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8eecb617943ff4e61e07dba0ae9aec7dd29d531da59a7a16feaed354f180e297", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e75dbde7-bf8e-410f-b68f-b48e7e60ef4e", "node_type": "1", "metadata": {}, "hash": "e8d798684bfc2c66d509cee41d871dbbe3f9e5ba95faffb29d1374cec7447299", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "then computes the minimum number of dimensions required to preserve 90% of the training sets variance COMP 3314 26 Choosing the Right Number of Dimensions You could then set n_components=k and run PCA again But there is a much better option: instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve COMP 3314 27 Choosing the Right Number of Dimensions Yet another option is to plot the explained variance as a function of the number of dimensions COMP 3314 28 COMP 3314 29 COMP 3314 30 PCA for Compression Lets apply PCA to the MNIST dataset while preserving 90% of its variance 87 features instead of the original 784 features This size reduction can speed up a classification algorithm (such as an SVM classifier) tremendously It is also possible to decompress the reduced dataset back to 784 dimensions This wont give you back the original data, since the projection lost a bit of information (within the 10% variance that was dropped) The following code compresses the MNIST dataset down to 87 dimensions, then uses the inverse_transform() method to decompress it back to 784 dimensions COMP 3314 31 PCA for Compression COMP 3314 32 Randomized PCA If you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a stochastic algorithm called Randomized PCA that quickly finds an approximation of the first d principal components It is dramatically faster than full SVD when k is much smaller than d By default, svd_solver is actually set to \"auto\" Scikit-Learn automatically uses the randomized PCA algorithm if d is greater than 500 and k is less than 80% d, or else it uses the full SVD approach If you want to force Scikit-Learn to use full SVD, you can set the svd_solver hyperparameter to \"full\" COMP 3314 33 Incremental PCA The previous PCA implementations require the whole training set to fit in memory Incremental PCA (IPCA) allows you to feed an IPCA algorithm one mini-batch at a time Useful for large training sets and online training (i.e.,", "mimetype": "text/plain", "start_char_idx": 6197, "end_char_idx": 8302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e75dbde7-bf8e-410f-b68f-b48e7e60ef4e": {"__data__": {"id_": "e75dbde7-bf8e-410f-b68f-b48e7e60ef4e", "embedding": null, "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98", "node_type": "4", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a026ba11-6d60-41c1-9768-38f26eb85285", "node_type": "1", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "828c07594b568b62809aa1b446b786513e53e852138c8dabc1d392dbb5e4d183", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c18d836-3d89-4c63-9b02-7a19e60a10cd", "node_type": "1", "metadata": {}, "hash": "4bc023de2473a0183380180f71d94a0cbea27c6638ed3bc8026475ed21419ab5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "or else it uses the full SVD approach If you want to force Scikit-Learn to use full SVD, you can set the svd_solver hyperparameter to \"full\" COMP 3314 33 Incremental PCA The previous PCA implementations require the whole training set to fit in memory Incremental PCA (IPCA) allows you to feed an IPCA algorithm one mini-batch at a time Useful for large training sets and online training (i.e., on the fly, as new data arrive) The following code splits the MNIST dataset into 100 mini-batches (using NumPys array_split() function) and feeds them to Scikit-Learns IncrementalPCA class Note that you must call the partial_fit() method with each mini-batch, rather than the fit() method with the whole training set COMP 3314 34 Outline PCA Principal Component Analysis Projects data points onto (few) principal components LLE Locally Linear Embedding Powerful nonlinear dimensionality reduction technique Manifold Learning technique that does not rely on projections COMP 3314 35 Code - LLE.ipynb Available here on CoLab COMP 3314 36 LLE How it works Measures how each training instance linearly relates to its closest neighbors Then looks for a low-dimensional representation of the training set where these local relationships are best preserved This approach makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise COMP 3314 37 Example: Unrolling the Swiss roll COMP 3314 38 LLE - Details For each training sample x(i), the algorithm identifies its n_neighbors closest neighbors E.g., n_neighbors = 10 Then it tries to reconstruct x(i) as a linear function of these neighbors More specifically, it finds the weights w such that the squared i,j distance between x(i) and is as small as possible, assuming w = 0 if x(j) is not one of the k i,j closest neighbors of x(i) COMP 3314 39 LLE - Details Thus the first step of LLE is the constrained optimization problem below, where W is the weight matrix containing all the weights w i,", "mimetype": "text/plain", "start_char_idx": 7909, "end_char_idx": 9884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c18d836-3d89-4c63-9b02-7a19e60a10cd": {"__data__": {"id_": "9c18d836-3d89-4c63-9b02-7a19e60a10cd", "embedding": null, "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98", "node_type": "4", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e75dbde7-bf8e-410f-b68f-b48e7e60ef4e", "node_type": "1", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "04ab43faf83a7c77a03cb7c1a41c9945b5d357e180cf96437069d2ec85da7afc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9eb35d2-36b5-4b39-aab3-7c1ee47e3dcf", "node_type": "1", "metadata": {}, "hash": "943e40b47eb296d9146a01385dfa3c34d3135f13def8071c5fe3d415b8f5dfcf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "it finds the weights w such that the squared i,j distance between x(i) and is as small as possible, assuming w = 0 if x(j) is not one of the k i,j closest neighbors of x(i) COMP 3314 39 LLE - Details Thus the first step of LLE is the constrained optimization problem below, where W is the weight matrix containing all the weights w i,j The second constraint simply normalizes the weights for each training instance x(i) COMP 3314 40 LLE - Details After this step, the weight matrix W (containing the weights w ) i,j encodes the local linear relationships between the training instances The second step is to map the training instances into a k- dimensional space (where k < d) while preserving these local relationships as much as possible If z(i) is the image of x(i) in this k-dimensional space, then we want the squared distance between z(i) and to be as small as possible COMP 3314 41 LLE - Details This idea leads to the following unconstrained optimization problem It looks very similar to the first step, but instead of keeping the instances fixed and finding the optimal weights, we are doing the reverse Keeping the weights fixed and finding the optimal position of the instances images in the low-dimensional space Note that Z is the matrix containing all z(i) COMP 3314 42 Other Dimensionality Reduction Techniques There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn Here are some of the most popular ones Random Projections Multidimensional Scaling (MDS) Isomap t-Distributed Stochastic Neighbor Embedding (t-SNE) Linear Discriminant Analysis (LDA) COMP 3314 43 References Most materials in this chapter are based on Book Code COMP 3314 44 References Some materials in this chapter are based on Book Code COMP 3314 45 Exercise 1 What are the main motivations for reducing a datasets dimensionality? What are the main drawbacks? What is the curse of dimensionality? Once a datasets dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?", "mimetype": "text/plain", "start_char_idx": 9550, "end_char_idx": 11594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9eb35d2-36b5-4b39-aab3-7c1ee47e3dcf": {"__data__": {"id_": "c9eb35d2-36b5-4b39-aab3-7c1ee47e3dcf", "embedding": null, "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8fcdc48-9a9f-4d26-8a24-d4a6c57bcf98", "node_type": "4", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8cfbf0a203b710d2dcb081472bf6e41a2bcaf4a2114a60cdd89cfa6cdf098871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c18d836-3d89-4c63-9b02-7a19e60a10cd", "node_type": "1", "metadata": {"file_name": "5. Dimensionality Reduction_processed.txt", "source_file": "5. Dimensionality Reduction", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\5. Dimensionality Reduction_processed.txt"}, "hash": "8212ea5f58144cc8181a1c1a9e20a261c222d0052c8be6ecbf7dc64a57b04435", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What are the main drawbacks? What is the curse of dimensionality? Once a datasets dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why? Can PCA be used to reduce the dimensionality of a highly nonlinear dataset? Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95% How many dimensions will the resulting dataset have? COMP 3314 46 Exercise 2 In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA? How can you evaluate the performance of a dimensionality reduction algorithm on your dataset? Does it make any sense to chain two different dimensionality reduction algorithms? COMP 3314 47 Exercise 3 Load the MNIST dataset and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing) Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set Next, use PCA to reduce the datasets dimensionality, with an explained variance ratio of 95% Train a new Random Forest classifier on the reduced dataset and see how long it takes Was training much faster? Next, evaluate the classifier on the test set How does it compare to the previous classifier?", "mimetype": "text/plain", "start_char_idx": 11414, "end_char_idx": 12703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9619dc67-af98-4be6-8edf-e51e4f176eec": {"__data__": {"id_": "9619dc67-af98-4be6-8edf-e51e4f176eec", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cff0234c-2352-4748-addd-12807a80182c", "node_type": "1", "metadata": {}, "hash": "8e91b119602d9c37c28af73cd6d4f51c2993094df922a2c82377f9d5969d5352", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. Evaluation & Tuning COMP3314 Machine Learning COMP 3314 2 Motivation Evaluation Tuning Learn about the best practices of building models by fine-tuning the model and evaluating its performance Obtain unbiased estimates of a model's performance Diagnose the common problems of machine learning algorithms Fine-tune machine learning models Evaluate predictive models using different performance metrics COMP 3314 3 Outline Pipelining Transformers Validation Holdout Cross-Validation Learning and Validation Curve Hyperparameter Search Performance Evaluation Metrics Precision Recall F1-score Receiver Operating Characteristic Scoring Metrics for Multiclass Classification Class Imbalance COMP 3314 4 Code - EvaluationAndTuning.ipynb Available here on CoLab COMP 3314 5 Breast Cancer Wisconsin Dataset (BCWD) The BCWD contains 569 samples of malignant and benign tumor cells The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnoses (M = malignant, B = benign) Columns 3-32 contain 30 real-valued features that have been computed from digitized images of the cell nuclei The BCWD has been deposited in the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) Lets read in the dataset COMP 3314 6 BCWD - Preprocessing Next, we assign the 30 features to a NumPy array X Using a LabelEncoder object, we transform the class labels from their original string representation ( 'M' and 'B' ) into integers Then we divide the dataset into a separate training dataset (80%) and a separate test dataset (20%) COMP 3314 7 Pipelining Transformers Lets standardize and compress our data from the initial 30 dimensions onto a lower two-dimensional subspace via PCA before feeding it into a logistic regression classifier Instead of going through the fitting and transformation steps for the training and test datasets separately, we can chain the StandardScaler, PCA,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cff0234c-2352-4748-addd-12807a80182c": {"__data__": {"id_": "cff0234c-2352-4748-addd-12807a80182c", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9619dc67-af98-4be6-8edf-e51e4f176eec", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "e5e404e03a616713386b159558f52756eb6499f8d4c917d53edb0886721f021c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7caa5dc-66e2-4d56-bb5a-f385774974df", "node_type": "1", "metadata": {}, "hash": "8d3de9b1401416246c916d414427fe979fe4690499239332cc8840298f755c01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can chain the StandardScaler, PCA, and LogisticRegression objects in a pipeline The Pipeline class in scikit-learn allows us to fit a model including an arbitrary number of transformation steps and apply it to make predictions about new data COMP 3314 8 COMP 3314 9 Validation One of the key steps in building a machine learning model is to estimate its performance on data that the model hasn't seen before To find an acceptable bias-variance trade-off, we need to evaluate our model carefully Validation can help us obtain reliable estimates of the model's generalization performance, that is, how well the model performs on unseen data We will learn about the common cross-validation techniques Holdout cross-validation K-fold cross-validation COMP 3314 10 The Holdout Method A classic and popular approach Split our initial dataset into a separate training and test dataset The former is used for model training, and the latter is used to estimate its generalization performance To tune and compare different parameter settings, i.e. to figure out optimal values of hyperparameters, we further split the training set into training and validation Advantage of test set that the model hasn't seen before during the training and model selection step: We can obtain a less biased estimate of its ability to generalize to new data COMP 3314 11 The Holdout Method COMP 3314 12 The Holdout Method A disadvantage of the holdout method is that the performance estimate may be very sensitive to how we partition the training set into the training and validation subsets The estimate may vary for different samples of the data COMP 3314 13 K-Fold Cross-Validation Randomly split the training dataset into k folds without replacement Use k - 1 folds for training, and one fold for evaluation Repeated k times This will give us k models and performance estimates Calculate the average performance of the models based on the different, independent folds Obtain a performance estimate that is less sensitive to the sub-partitioning of the training data compared to the holdout method Once we have found satisfactory hyperparameter values,", "mimetype": "text/plain", "start_char_idx": 1926, "end_char_idx": 4056, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7caa5dc-66e2-4d56-bb5a-f385774974df": {"__data__": {"id_": "d7caa5dc-66e2-4d56-bb5a-f385774974df", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cff0234c-2352-4748-addd-12807a80182c", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "208e76ac219169817975c115b6f2c24f767685e3d30f6e9f220d63104a268a3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51373707-bf06-41cc-8adc-321ed0c1d8ed", "node_type": "1", "metadata": {}, "hash": "811086d9abd70228fe5afa90dd1a1bdfa90b1fb993761454795434dfeaa5da29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and one fold for evaluation Repeated k times This will give us k models and performance estimates Calculate the average performance of the models based on the different, independent folds Obtain a performance estimate that is less sensitive to the sub-partitioning of the training data compared to the holdout method Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training set and obtain a final performance estimate using the independent test set COMP 3314 14 K-Fold Cross-Validation Performance Estimate E i COMP 3314 15 K-Fold Cross-Validation A good value for k in is 10, as empirical evidence shows For relatively small training sets, it can be useful to increase k More training data will be used in each iteration Results in a lower bias towards estimating the generalization performance by averaging the individual model estimates Runtime will increase Yields estimates with higher variance, since the training folds will be more similar to each other For larger datasets, we can choose a smaller value for k Still obtain an accurate estimate of the average performance of the model while reducing the computational cost of refitting and evaluating the model on the different folds COMP 3314 16 Leave-One-Out Cross-Validation A special case of k-fold cross-validation is the Leave-one-out cross-validation (LOOCV) method Set the number of folds equal to the number of training samples (k = n) so that only one training sample is used for testing during each iteration Recommended approach for working with very small datasets COMP 3314 17 Stratified K-Fold Cross-Validation In stratified cross-validation, the class proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset Can yield better bias and variance estimates,", "mimetype": "text/plain", "start_char_idx": 3685, "end_char_idx": 5534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51373707-bf06-41cc-8adc-321ed0c1d8ed": {"__data__": {"id_": "51373707-bf06-41cc-8adc-321ed0c1d8ed", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7caa5dc-66e2-4d56-bb5a-f385774974df", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "144251449ef4571725eb6d87ff81a0144a6def9a02e79edd12667cfe283e27a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77ec4bcf-444a-4b5c-a11f-5a33c4a044d9", "node_type": "1", "metadata": {}, "hash": "b5371939fe09850d60e3afe6c38233c56eddf071d57203f3479f34eb850df114", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the class proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset Can yield better bias and variance estimates, especially in cases of unequal class proportions Scikit-learn provides the StratifiedKFold iterator class for this COMP 3314 18 Stratified K-Fold Cross-Validation COMP 3314 19 Stratified K-Fold Cross-Validation The previous code example was useful to illustrate how k-fold cross-validation works Scikit-learn also implements a k-fold cross-validation scorer Allows us to evaluate our model less verbosely COMP 3314 20 Outline Pipelining Transformers Validation Holdout Cross-Validation Learning and Validation Curve Hyperparameter Search Performance Evaluation Metrics Precision Recall F1-score Receiver Operating Characteristic Scoring Metrics for Multiclass Classification Class Imbalance COMP 3314 21 Learning and Validation Curve Simple yet powerful diagnostic tools Learning curve Can help diagnose overfitting (high variance) or underfitting (high bias) Validation curve Can help us address some common issues of a learning algorithm COMP 3314 22 Learning Curve Plot the model training and validation accuracies as functions of the training set size Can detect if model suffers from high variance or high bias more data could help address this problem COMP 3314 23 COMP 3314 24 Validation Curve Validation curves are very similar to learning curves Instead of plotting the train and test accuracies as functions of the sample size, we vary the values of the model parameters E.g., the regularization parameter C doc COMP 3314 25 Hyperparameter Grid Search Finds the optimal combination of hyperparameter values Approach Brute-force exhaustive search Specify list of values for all hyperparameters Evaluate the model performance for all combinations COMP 3314 26 Hyperparameter Grid Search Grid search is a powerful approach for finding the optimal set of parameters Evaluation of all possible parameter combinations is however computationally very expensive An alternative approach to sampling different parameter combinations using scikit-learn is randomized search Using the RandomizedSearchCV class in scikit-learn, we can draw random parameter combinations from sampling distributions with a specified budget COMP 3314 27 Nested Cross-Validation Previously we used k-fold cross-validation in combination with grid search We optimized the hyperparameters based on a validation score,", "mimetype": "text/plain", "start_char_idx": 5348, "end_char_idx": 7827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77ec4bcf-444a-4b5c-a11f-5a33c4a044d9": {"__data__": {"id_": "77ec4bcf-444a-4b5c-a11f-5a33c4a044d9", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51373707-bf06-41cc-8adc-321ed0c1d8ed", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "b0e98838bed827be6b4c28020f3d3fed3689562e2198ac7d025717afd05c5b2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35caf5fe-203b-437f-91bd-948c412a4acc", "node_type": "1", "metadata": {}, "hash": "faf4975e617d84588b454f8c9822bd86c7b6758adaba97e4448ec294cad714e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we can draw random parameter combinations from sampling distributions with a specified budget COMP 3314 27 Nested Cross-Validation Previously we used k-fold cross-validation in combination with grid search We optimized the hyperparameters based on a validation score, the validation score is biased and not a good estimate of the generalization any longer To get a proper estimate of the generalization we should compute the score on another validation set The recommended approach is nested cross-validation In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds An inner loop is then used to select the model using k-fold cross-validation on the training fold After model selection, the test fold is then used to evaluate the model performance COMP 3314 28 COMP 3314 29 Nested Cross-Validation The returned average cross-validation accuracy gives us a better estimate of what to expect if we tune the hyperparameters of a model and use it on unseen data COMP 3314 30 Nested Cross-Validation We could use the nested cross-validation approach to compare an SVM model to a simple decision tree classifier; for simplicity, we will only tune its depth parameter COMP 3314 31 Outline Pipelining Transformers Validation Holdout Cross-Validation Learning and Validation Curve Hyperparameter Search Performance Evaluation Metrics Precision Recall F1-score Receiver Operating Characteristic Scoring Metrics for Multiclass Classification Class Imbalance COMP 3314 32 Different Performance Evaluation Metrics In general, accuracy is a useful metric to quantify the performance of a model However, there are several other performance metrics that can be used to measure a model's relevance, such as Precision Recall F1-score COMP 3314 33 Confusion Matrix A matrix that lays out the performance of a learning algorithm The confusion matrix is simply a square matrix that reports the counts of the True Positive (TP) True Negative (TN) False Positive (FP) False Negative (FN) predictions of a classifier COMP 3314 34 Quiz Calculate the confusion matrix for the following example x 2 Actual class Predicted class x 1 COMP 3314 35 Confusion Matrix COMP 3314 36 Confusion Matrix Note that we previously encoded the class labels so that malignant samples are the \"positive\" class (1),", "mimetype": "text/plain", "start_char_idx": 7560, "end_char_idx": 9893, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35caf5fe-203b-437f-91bd-948c412a4acc": {"__data__": {"id_": "35caf5fe-203b-437f-91bd-948c412a4acc", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77ec4bcf-444a-4b5c-a11f-5a33c4a044d9", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "64fcd3a6afe38775b78602ca78aac73770deb4a9d00b88194ed5d2242b0697d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "970af8ef-055f-49d6-bd23-7a99d93beaa0", "node_type": "1", "metadata": {}, "hash": "346b4a392e2c7ae41c07e4a832d659516f671f069dfd43ca051f5b4eda4ccf90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and benign samples are the \"negative\" class (0): Note that the (true) class 0 samples that are correctly predicted as class 0 (true negatives) are in the upper left corner of the matrix on the previous slide To change the ordering so that the true negatives are in the lower right corner (index 1,1) and the true positives are in the upper left, we can use the labels argument COMP 3314 37 Error and Accuracy Both the prediction error (ERR) and accuracy (ACC) provide general information about how many samples are misclassified Error can be understood as the sum of all false predictions divided by the number of total predictions Accuracy is calculated as the sum of correct predictions divided by the total number of prediction COMP 3314 38 Quiz Calculate the error and accuracy for the following example x 2 Actual class Predicted class x 1 COMP 3314 39 True/False Positive Rate The True positive rate (TPR) and False positive rate (FPR) are performance metrics that are especially useful for imbalanced class problems COMP 3314 40 Quiz Calculate the TPR and FPR for the following example x 2 Actual class Predicted class x 1 COMP 3314 41 Precision and Recall The performance metrics precision (PRE) and recall (REC) are related to those true positive and negative rates, and in fact, REC is synonymous with TPR In practice, often a combination of PRE and REC is used, the so-called F1-score COMP 3314 42 Quiz Calculate the precision, recall and F1-score for the following example x 2 Actual class Predicted class x 1 COMP 3314 43 Scoring Metrics in scikit-learn Those scoring metrics are all implemented in scikit-learn and can be imported from the sklearn.", "mimetype": "text/plain", "start_char_idx": 9894, "end_char_idx": 11556, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "970af8ef-055f-49d6-bd23-7a99d93beaa0": {"__data__": {"id_": "970af8ef-055f-49d6-bd23-7a99d93beaa0", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35caf5fe-203b-437f-91bd-948c412a4acc", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "5802b884d9d8ae4f98a14eb724185bed9f834ff966872bd62dbbf590c17667f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25a8efbd-8d2f-47bc-b787-e0bf9dab0dde", "node_type": "1", "metadata": {}, "hash": "b269950ee06748c99d69c0d240746bd1f60cb074e285bcdfc4fb2b3f4d6be65b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and in fact, REC is synonymous with TPR In practice, often a combination of PRE and REC is used, the so-called F1-score COMP 3314 42 Quiz Calculate the precision, recall and F1-score for the following example x 2 Actual class Predicted class x 1 COMP 3314 43 Scoring Metrics in scikit-learn Those scoring metrics are all implemented in scikit-learn and can be imported from the sklearn.metrics module COMP 3314 44 Scoring Metric in GridSearchCV We can use a different scoring metric than accuracy in the GridSearchCV Via the scoring parameter Remember that the positive class in scikit-learn is the class that is labeled as class 1 Specify a different positive label by construct scorer via the make_scorer function COMP 3314 45 Receiver Operating Characteristic ROC ROC graphs are useful tools to select models for classification based on their performance with respect to the FPR and TPR Computed by shifting the decision threshold of the classifier The diagonal of an ROC graph can be interpreted as random guessing, and classification models that fall below the diagonal are considered as worse than random guessing A perfect classifier would fall into the top left corner of the graph with a TPR of 1 and an FPR of 0 Based on the ROC curve, we can then compute the so-called ROC Area Under the Curve (ROC AUC) to characterize the performance of a classification model COMP 3314 46 COMP 3314 47 Scoring Metrics for Multiclass Classification The scoring metrics that we discussed in this section are specific to binary classification systems Macro and micro averaging methods exist to extend those scoring metrics to multiclass problems via One-versus-All (OvA) classification The micro-average is calculated from the individual TPs, TNs, FPs, and FNs of the system For example, the micro-average of the precision score in a k-class system can be calculated as The macro-average is simply calculated as the average scores of the different systems COMP 3314 48 Micro-averaging vs.", "mimetype": "text/plain", "start_char_idx": 11170, "end_char_idx": 13152, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25a8efbd-8d2f-47bc-b787-e0bf9dab0dde": {"__data__": {"id_": "25a8efbd-8d2f-47bc-b787-e0bf9dab0dde", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "970af8ef-055f-49d6-bd23-7a99d93beaa0", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "3f30c0af025922dd151579c0ed81441ba13314593955c649bd3ea7588883eae6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c26c21e4-a57e-4805-95b6-fdffe23146d9", "node_type": "1", "metadata": {}, "hash": "bd0755d884a13dd4277624999c521464ed36ce819fa3b3341b358cdc5f0792c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "TNs, FPs, and FNs of the system For example, the micro-average of the precision score in a k-class system can be calculated as The macro-average is simply calculated as the average scores of the different systems COMP 3314 48 Micro-averaging vs. Macro-averaging Micro-averaging is useful if we want to weight each instance or prediction equally Macro-averaging weights all classes equally to evaluate the overall performance of a classifier with regard to the most frequent class labels COMP 3314 49 Micro/Macro-averaging in scikit-learn If we are using binary performance metrics to evaluate multiclass classification models in scikit-learn, a normalized or weighted variant of the macro-average is used by default The weighted macro-average is calculated by weighting the score of each class label by the number of true instances when calculating the average The weighted macro-average is useful if we are dealing with class imbalances, that is, different numbers of instances for each label While the weighted macro-average is the default for multiclass problems in scikit-learn, we can specify the averaging method via the average parameter inside the different scoring functions that we import from the sklearn.metrics module, for example, the precision_score or make_scorer functions COMP 3314 50 Class Imbalance Common problem when working with real-world Samples from one class or multiple classes are over-represented in a dataset COMP 3314 51 Dealing with Class Imbalance Let's create an imbalanced dataset from our breast cancer dataset, which originally consisted of 357 benign tumors (class 0) and 212 malignant tumors (class 1) If we were to compute the accuracy of a model that always predicts the majority class (benign, class 0), we would achieve a prediction accuracy of approximately 90 percent COMP 3314 52 Dealing with Class Imbalance Thus, when we fit classifiers on such datasets, it would make sense to focus on other metrics than accuracy when comparing different models, such as precision, recall, the ROC curve Whatever we care most about in our application Our priority might be to identify the majority of patients with malignant cancer patients to recommend an additional screening, then recall should be our metric of choice In spam filtering,", "mimetype": "text/plain", "start_char_idx": 12907, "end_char_idx": 15181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c26c21e4-a57e-4805-95b6-fdffe23146d9": {"__data__": {"id_": "c26c21e4-a57e-4805-95b6-fdffe23146d9", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25a8efbd-8d2f-47bc-b787-e0bf9dab0dde", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "6650a1001b23e019d8aafb54da8973e4150d1d21380143bc54c100e27986d999", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "934451c0-1236-4018-870b-9da6a31f1ff2", "node_type": "1", "metadata": {}, "hash": "b56fa3e1f72538d66d41cc874f97fc81baa63e7e53229b37c28706c56c6a8ee6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "we would achieve a prediction accuracy of approximately 90 percent COMP 3314 52 Dealing with Class Imbalance Thus, when we fit classifiers on such datasets, it would make sense to focus on other metrics than accuracy when comparing different models, such as precision, recall, the ROC curve Whatever we care most about in our application Our priority might be to identify the majority of patients with malignant cancer patients to recommend an additional screening, then recall should be our metric of choice In spam filtering, where we don't want to label emails as spam if the system is not very certain, precision might be a more appropriate metric COMP 3314 53 Dealing with Class Imbalance Aside from evaluating machine learning models, class imbalance influences a learning algorithm during model fitting itself Since machine learning algorithms typically optimize a reward or cost function that is computed as a sum over the training examples that it sees during fitting, the decision rule is likely going to be biased towards the majority class I.e., the algorithm implicitly learns a model that optimizes the predictions based on the most abundant class in the dataset, in order to minimize the cost or maximize the reward during training One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class. Via scikit- learn, adjusting such a penalty is as convenient as setting the class_weight parameter to class_weight='balanced', which is implemented for most classifiers.", "mimetype": "text/plain", "start_char_idx": 14654, "end_char_idx": 16215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "934451c0-1236-4018-870b-9da6a31f1ff2": {"__data__": {"id_": "934451c0-1236-4018-870b-9da6a31f1ff2", "embedding": null, "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31962845-0db9-4121-bb9f-207b3124a377", "node_type": "4", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "2312261c9ca00f2b60c6816fd955160a91020a9d50390ac3ca058a68ae896fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c26c21e4-a57e-4805-95b6-fdffe23146d9", "node_type": "1", "metadata": {"file_name": "6. Evaluation  Tuning_processed.txt", "source_file": "6. Evaluation  Tuning", "file_path": "uploads\\rag_upload_20250731_231204\\processed_texts\\6. Evaluation  Tuning_processed.txt"}, "hash": "8da6eaa42c6c5daff32d11694b1620b96348577b904e390e4d65817c63b0f48a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "e., the algorithm implicitly learns a model that optimizes the predictions based on the most abundant class in the dataset, in order to minimize the cost or maximize the reward during training One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class. Via scikit- learn, adjusting such a penalty is as convenient as setting the class_weight parameter to class_weight='balanced', which is implemented for most classifiers. COMP 3314 54 Dealing with Class Imbalance Other popular strategies for dealing with class imbalance include upsampling the minority class, downsampling the majority class, and the generation of synthetic training samples Unfortunately, there's no universally best solution, no technique that works best across different problem domains In practice, it is recommended to try out different strategies on a given problem, evaluate the results, and choose the technique that seems most appropriate The scikit-learn library implements a simple resample function that can help with the upsampling of the minority class by drawing new samples from the dataset with replacement Similarly, we could downsample the majority class by removing training examples from the dataset To perform downsampling using the resample function, we could simply swap the class 1 label with class 0 COMP 3314 55 Dealing with Class Imbalance After resampling, we can then stack the original class 0 samples with the upsampled class 1 subset to obtain a balanced dataset as follows Consequently, a majority vote prediction rule would only achieve 50 percent accuracy COMP 3314 56 Dealing with Class Imbalance Another technique for dealing with class imbalance is the generation of synthetic training samples, which is beyond the scope of this course A widely used algorithm for synthetic training sample generation is Synthetic Minority Over-sampling Technique (SMOTE) More techniques to deal with the curse of imbalance can be found here COMP 3314 57 References Most materials in this chapter are based on Book Code COMP 3314 58 References A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, International Joint Conference on Artificial Intelligence (IJCAI), 14 (12): 1137-43, 1995 Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, Sebastian Raschka Analysis of Variance of Cross-validation Estimators of the Generalization Error, M. Markatou, H. Tian, S. Biswas, and G. M. Hripcsak, Journal of Machine Learning Research, 6: 1127-1168, 2005 Bias in Error Estimation When Using Cross-validation for Model Selection, BMC Bioinformatics, S. Varma and R. Simon, 7(1): 91, 2006", "mimetype": "text/plain", "start_char_idx": 15708, "end_char_idx": 18425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}