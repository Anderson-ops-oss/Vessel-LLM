6. Evaluation & Tuning COMP3314 Machine Learning COMP 3314 2 Motivation Evaluation Tuning Learn about the best practices of building models by fine-tuning the model and evaluating its performance Obtain unbiased estimates of a model's performance Diagnose the common problems of machine learning algorithms Fine-tune machine learning models Evaluate predictive models using different performance metrics COMP 3314 3 Outline Pipelining Transformers Validation Holdout Cross-Validation Learning and Validation Curve Hyperparameter Search Performance Evaluation Metrics Precision Recall F1-score Receiver Operating Characteristic Scoring Metrics for Multiclass Classification Class Imbalance COMP 3314 4 Code - EvaluationAndTuning.ipynb Available here on CoLab COMP 3314 5 Breast Cancer Wisconsin Dataset (BCWD) The BCWD contains 569 samples of malignant and benign tumor cells The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnoses (M = malignant, B = benign) Columns 3-32 contain 30 real-valued features that have been computed from digitized images of the cell nuclei The BCWD has been deposited in the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) Lets read in the dataset COMP 3314 6 BCWD - Preprocessing Next, we assign the 30 features to a NumPy array X Using a LabelEncoder object, we transform the class labels from their original string representation ( 'M' and 'B' ) into integers Then we divide the dataset into a separate training dataset (80%) and a separate test dataset (20%) COMP 3314 7 Pipelining Transformers Lets standardize and compress our data from the initial 30 dimensions onto a lower two-dimensional subspace via PCA before feeding it into a logistic regression classifier Instead of going through the fitting and transformation steps for the training and test datasets separately, we can chain the StandardScaler, PCA, and LogisticRegression objects in a pipeline The Pipeline class in scikit-learn allows us to fit a model including an arbitrary number of transformation steps and apply it to make predictions about new data COMP 3314 8 COMP 3314 9 Validation One of the key steps in building a machine learning model is to estimate its performance on data that the model hasn't seen before To find an acceptable bias-variance trade-off, we need to evaluate our model carefully Validation can help us obtain reliable estimates of the model's generalization performance, that is, how well the model performs on unseen data We will learn about the common cross-validation techniques Holdout cross-validation K-fold cross-validation COMP 3314 10 The Holdout Method A classic and popular approach Split our initial dataset into a separate training and test dataset The former is used for model training, and the latter is used to estimate its generalization performance To tune and compare different parameter settings, i.e. to figure out optimal values of hyperparameters, we further split the training set into training and validation Advantage of test set that the model hasn't seen before during the training and model selection step: We can obtain a less biased estimate of its ability to generalize to new data COMP 3314 11 The Holdout Method COMP 3314 12 The Holdout Method A disadvantage of the holdout method is that the performance estimate may be very sensitive to how we partition the training set into the training and validation subsets The estimate may vary for different samples of the data COMP 3314 13 K-Fold Cross-Validation Randomly split the training dataset into k folds without replacement Use k - 1 folds for training, and one fold for evaluation Repeated k times This will give us k models and performance estimates Calculate the average performance of the models based on the different, independent folds Obtain a performance estimate that is less sensitive to the sub-partitioning of the training data compared to the holdout method Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training set and obtain a final performance estimate using the independent test set COMP 3314 14 K-Fold Cross-Validation Performance Estimate E i COMP 3314 15 K-Fold Cross-Validation A good value for k in is 10, as empirical evidence shows For relatively small training sets, it can be useful to increase k More training data will be used in each iteration Results in a lower bias towards estimating the generalization performance by averaging the individual model estimates Runtime will increase Yields estimates with higher variance, since the training folds will be more similar to each other For larger datasets, we can choose a smaller value for k Still obtain an accurate estimate of the average performance of the model while reducing the computational cost of refitting and evaluating the model on the different folds COMP 3314 16 Leave-One-Out Cross-Validation A special case of k-fold cross-validation is the Leave-one-out cross-validation (LOOCV) method Set the number of folds equal to the number of training samples (k = n) so that only one training sample is used for testing during each iteration Recommended approach for working with very small datasets COMP 3314 17 Stratified K-Fold Cross-Validation In stratified cross-validation, the class proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset Can yield better bias and variance estimates, especially in cases of unequal class proportions Scikit-learn provides the StratifiedKFold iterator class for this COMP 3314 18 Stratified K-Fold Cross-Validation COMP 3314 19 Stratified K-Fold Cross-Validation The previous code example was useful to illustrate how k-fold cross-validation works Scikit-learn also implements a k-fold cross-validation scorer Allows us to evaluate our model less verbosely COMP 3314 20 Outline Pipelining Transformers Validation Holdout Cross-Validation Learning and Validation Curve Hyperparameter Search Performance Evaluation Metrics Precision Recall F1-score Receiver Operating Characteristic Scoring Metrics for Multiclass Classification Class Imbalance COMP 3314 21 Learning and Validation Curve Simple yet powerful diagnostic tools Learning curve Can help diagnose overfitting (high variance) or underfitting (high bias) Validation curve Can help us address some common issues of a learning algorithm COMP 3314 22 Learning Curve Plot the model training and validation accuracies as functions of the training set size Can detect if model suffers from high variance or high bias more data could help address this problem COMP 3314 23 COMP 3314 24 Validation Curve Validation curves are very similar to learning curves Instead of plotting the train and test accuracies as functions of the sample size, we vary the values of the model parameters E.g., the regularization parameter C doc COMP 3314 25 Hyperparameter Grid Search Finds the optimal combination of hyperparameter values Approach Brute-force exhaustive search Specify list of values for all hyperparameters Evaluate the model performance for all combinations COMP 3314 26 Hyperparameter Grid Search Grid search is a powerful approach for finding the optimal set of parameters Evaluation of all possible parameter combinations is however computationally very expensive An alternative approach to sampling different parameter combinations using scikit-learn is randomized search Using the RandomizedSearchCV class in scikit-learn, we can draw random parameter combinations from sampling distributions with a specified budget COMP 3314 27 Nested Cross-Validation Previously we used k-fold cross-validation in combination with grid search We optimized the hyperparameters based on a validation score, the validation score is biased and not a good estimate of the generalization any longer To get a proper estimate of the generalization we should compute the score on another validation set The recommended approach is nested cross-validation In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds An inner loop is then used to select the model using k-fold cross-validation on the training fold After model selection, the test fold is then used to evaluate the model performance COMP 3314 28 COMP 3314 29 Nested Cross-Validation The returned average cross-validation accuracy gives us a better estimate of what to expect if we tune the hyperparameters of a model and use it on unseen data COMP 3314 30 Nested Cross-Validation We could use the nested cross-validation approach to compare an SVM model to a simple decision tree classifier; for simplicity, we will only tune its depth parameter COMP 3314 31 Outline Pipelining Transformers Validation Holdout Cross-Validation Learning and Validation Curve Hyperparameter Search Performance Evaluation Metrics Precision Recall F1-score Receiver Operating Characteristic Scoring Metrics for Multiclass Classification Class Imbalance COMP 3314 32 Different Performance Evaluation Metrics In general, accuracy is a useful metric to quantify the performance of a model However, there are several other performance metrics that can be used to measure a model's relevance, such as Precision Recall F1-score COMP 3314 33 Confusion Matrix A matrix that lays out the performance of a learning algorithm The confusion matrix is simply a square matrix that reports the counts of the True Positive (TP) True Negative (TN) False Positive (FP) False Negative (FN) predictions of a classifier COMP 3314 34 Quiz Calculate the confusion matrix for the following example x 2 Actual class Predicted class x 1 COMP 3314 35 Confusion Matrix COMP 3314 36 Confusion Matrix Note that we previously encoded the class labels so that malignant samples are the "positive" class (1), and benign samples are the "negative" class (0): Note that the (true) class 0 samples that are correctly predicted as class 0 (true negatives) are in the upper left corner of the matrix on the previous slide To change the ordering so that the true negatives are in the lower right corner (index 1,1) and the true positives are in the upper left, we can use the labels argument COMP 3314 37 Error and Accuracy Both the prediction error (ERR) and accuracy (ACC) provide general information about how many samples are misclassified Error can be understood as the sum of all false predictions divided by the number of total predictions Accuracy is calculated as the sum of correct predictions divided by the total number of prediction COMP 3314 38 Quiz Calculate the error and accuracy for the following example x 2 Actual class Predicted class x 1 COMP 3314 39 True/False Positive Rate The True positive rate (TPR) and False positive rate (FPR) are performance metrics that are especially useful for imbalanced class problems COMP 3314 40 Quiz Calculate the TPR and FPR for the following example x 2 Actual class Predicted class x 1 COMP 3314 41 Precision and Recall The performance metrics precision (PRE) and recall (REC) are related to those true positive and negative rates, and in fact, REC is synonymous with TPR In practice, often a combination of PRE and REC is used, the so-called F1-score COMP 3314 42 Quiz Calculate the precision, recall and F1-score for the following example x 2 Actual class Predicted class x 1 COMP 3314 43 Scoring Metrics in scikit-learn Those scoring metrics are all implemented in scikit-learn and can be imported from the sklearn.metrics module COMP 3314 44 Scoring Metric in GridSearchCV We can use a different scoring metric than accuracy in the GridSearchCV Via the scoring parameter Remember that the positive class in scikit-learn is the class that is labeled as class 1 Specify a different positive label by construct scorer via the make_scorer function COMP 3314 45 Receiver Operating Characteristic ROC ROC graphs are useful tools to select models for classification based on their performance with respect to the FPR and TPR Computed by shifting the decision threshold of the classifier The diagonal of an ROC graph can be interpreted as random guessing, and classification models that fall below the diagonal are considered as worse than random guessing A perfect classifier would fall into the top left corner of the graph with a TPR of 1 and an FPR of 0 Based on the ROC curve, we can then compute the so-called ROC Area Under the Curve (ROC AUC) to characterize the performance of a classification model COMP 3314 46 COMP 3314 47 Scoring Metrics for Multiclass Classification The scoring metrics that we discussed in this section are specific to binary classification systems Macro and micro averaging methods exist to extend those scoring metrics to multiclass problems via One-versus-All (OvA) classification The micro-average is calculated from the individual TPs, TNs, FPs, and FNs of the system For example, the micro-average of the precision score in a k-class system can be calculated as The macro-average is simply calculated as the average scores of the different systems COMP 3314 48 Micro-averaging vs. Macro-averaging Micro-averaging is useful if we want to weight each instance or prediction equally Macro-averaging weights all classes equally to evaluate the overall performance of a classifier with regard to the most frequent class labels COMP 3314 49 Micro/Macro-averaging in scikit-learn If we are using binary performance metrics to evaluate multiclass classification models in scikit-learn, a normalized or weighted variant of the macro-average is used by default The weighted macro-average is calculated by weighting the score of each class label by the number of true instances when calculating the average The weighted macro-average is useful if we are dealing with class imbalances, that is, different numbers of instances for each label While the weighted macro-average is the default for multiclass problems in scikit-learn, we can specify the averaging method via the average parameter inside the different scoring functions that we import from the sklearn.metrics module, for example, the precision_score or make_scorer functions COMP 3314 50 Class Imbalance Common problem when working with real-world Samples from one class or multiple classes are over-represented in a dataset COMP 3314 51 Dealing with Class Imbalance Let's create an imbalanced dataset from our breast cancer dataset, which originally consisted of 357 benign tumors (class 0) and 212 malignant tumors (class 1) If we were to compute the accuracy of a model that always predicts the majority class (benign, class 0), we would achieve a prediction accuracy of approximately 90 percent COMP 3314 52 Dealing with Class Imbalance Thus, when we fit classifiers on such datasets, it would make sense to focus on other metrics than accuracy when comparing different models, such as precision, recall, the ROC curve Whatever we care most about in our application Our priority might be to identify the majority of patients with malignant cancer patients to recommend an additional screening, then recall should be our metric of choice In spam filtering, where we don't want to label emails as spam if the system is not very certain, precision might be a more appropriate metric COMP 3314 53 Dealing with Class Imbalance Aside from evaluating machine learning models, class imbalance influences a learning algorithm during model fitting itself Since machine learning algorithms typically optimize a reward or cost function that is computed as a sum over the training examples that it sees during fitting, the decision rule is likely going to be biased towards the majority class I.e., the algorithm implicitly learns a model that optimizes the predictions based on the most abundant class in the dataset, in order to minimize the cost or maximize the reward during training One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class. Via scikit- learn, adjusting such a penalty is as convenient as setting the class_weight parameter to class_weight='balanced', which is implemented for most classifiers. COMP 3314 54 Dealing with Class Imbalance Other popular strategies for dealing with class imbalance include upsampling the minority class, downsampling the majority class, and the generation of synthetic training samples Unfortunately, there's no universally best solution, no technique that works best across different problem domains In practice, it is recommended to try out different strategies on a given problem, evaluate the results, and choose the technique that seems most appropriate The scikit-learn library implements a simple resample function that can help with the upsampling of the minority class by drawing new samples from the dataset with replacement Similarly, we could downsample the majority class by removing training examples from the dataset To perform downsampling using the resample function, we could simply swap the class 1 label with class 0 COMP 3314 55 Dealing with Class Imbalance After resampling, we can then stack the original class 0 samples with the upsampled class 1 subset to obtain a balanced dataset as follows Consequently, a majority vote prediction rule would only achieve 50 percent accuracy COMP 3314 56 Dealing with Class Imbalance Another technique for dealing with class imbalance is the generation of synthetic training samples, which is beyond the scope of this course A widely used algorithm for synthetic training sample generation is Synthetic Minority Over-sampling Technique (SMOTE) More techniques to deal with the curse of imbalance can be found here COMP 3314 57 References Most materials in this chapter are based on Book Code COMP 3314 58 References A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, International Joint Conference on Artificial Intelligence (IJCAI), 14 (12): 1137-43, 1995 Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, Sebastian Raschka Analysis of Variance of Cross-validation Estimators of the Generalization Error, M. Markatou, H. Tian, S. Biswas, and G. M. Hripcsak, Journal of Machine Learning Research, 6: 1127-1168, 2005 Bias in Error Estimation When Using Cross-validation for Model Selection, BMC Bioinformatics, S. Varma and R. Simon, 7(1): 91, 2006